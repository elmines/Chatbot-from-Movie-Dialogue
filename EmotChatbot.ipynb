{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a chatbot using conversations from Cornell University's [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). The main features of our model are LSTM cells, a bidirectional dynamic RNN, and decoders with attention. \n",
    "\n",
    "The conversations will be cleaned rather extensively to help the model to produce better responses. As part of the cleaning process, punctuation will be removed, rare words will be replaced with \"UNK\" (our \"unknown\" token), longer sentences will not be used, and all letters will be in the lowercase. \n",
    "\n",
    "With a larger amount of data, it would be more practical to keep features, such as punctuation. However, I am using FloydHub's GPU services and I don't want to get carried away with too training for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#Local libraries\n",
    "from corpus import Corpus\n",
    "import metrics\n",
    "import loss_functions\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code to load the data is courtesy of https://github.com/suriyadeepan/practical_seq2seq/blob/master/datasets/cornell_corpus/data.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_corpus = Corpus(\"movie_lines.txt\", \"movie_conversations.txt\", max_vocab=8100, max_line_length=30)\n",
    "\n",
    "questions_text = cornell_corpus.prompts\n",
    "answers_text = cornell_corpus.answers\n",
    "questions_int = cornell_corpus.prompts_int\n",
    "answers_int = cornell_corpus.answers_int\n",
    "\n",
    "UNK = cornell_corpus.unk\n",
    "vocab2int = cornell_corpus.vocab2int\n",
    "int2vocab = cornell_corpus.int2vocab\n",
    "\n",
    "METATOKEN_INDEX = len(vocab2int)\n",
    "META = \"<META>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "GO = \"<GO>\"\n",
    "codes = [META, EOS, PAD, GO]\n",
    "    \n",
    "\n",
    "source_vocab_size = len(vocab2int)\n",
    "dest_vocab_size = len(vocab2int)\n",
    "\n",
    "vocab_dicts = (vocab2int, int2vocab)\n",
    "(questions_vocab_to_int, questions_int_to_vocab) = vocab_dicts\n",
    "(answers_vocab_to_int, answers_int_to_vocab) = vocab_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', '<UNK>', '<UNK>', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', '<UNK>', 'public', 'break', 'up', 'on', 'the', '<UNK>', 'again']\n",
      "['well', 'i', 'thought', 'we', 'would', 'start', 'with', '<UNK>', 'if', 'that', 'okay', 'with', 'you']\n",
      "\n",
      "['well', 'i', 'thought', 'we', 'would', 'start', 'with', '<UNK>', 'if', 'that', 'okay', 'with', 'you']\n",
      "['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please']\n",
      "\n",
      "['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please']\n",
      "['okay', 'then', 'how', 'about', 'we', 'try', 'out', 'some', 'french', '<UNK>', 'saturday', 'night']\n",
      "\n",
      "['you', 'are', 'asking', 'me', 'out', 'that', 'so', 'cute', 'what', 'your', 'name', 'again']\n",
      "['forget', 'it']\n",
      "\n",
      "['no', 'no', 'it', 'my', 'fault', 'we', 'did', 'not', 'have', 'a', 'proper', 'introduction']\n",
      "['cameron']\n",
      "\n",
      "['cameron']\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "['seems', 'like', 'she', 'could', 'get', 'a', 'date', 'easy', 'enough']\n",
      "\n",
      "['why']\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "['that', 'a', 'shame']\n",
      "\n",
      "['gosh', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend']\n",
      "['let', 'me', 'see', 'what', 'i', 'can', 'do']\n",
      "\n",
      "['<UNK>', 'ma', '<UNK>', 'this', 'is', 'my', 'head']\n",
      "['right', 'see', 'you', 'are', 'ready', 'for', 'the', '<UNK>']\n",
      "\n",
      "['that', 'because', 'it', 'such', 'a', 'nice', 'one']\n",
      "['forget', 'french']\n",
      "\n",
      "['how', 'is', 'our', 'little', 'find', 'the', '<UNK>', 'a', 'date', 'plan', '<UNK>']\n",
      "['well', 'there', 'someone', 'i', 'think', 'might', 'be']\n",
      "\n",
      "['there']\n",
      "['where']\n",
      "\n",
      "['you', 'got', 'something', 'on', 'your', 'mind']\n",
      "['i', 'counted', 'on', 'you', 'to', 'help', 'my', 'cause', 'you', 'and', 'that', '<UNK>', 'are', 'obviously', 'failing', 'are', 'not', 'we', 'ever', 'going', 'on', 'our', 'date']\n",
      "\n",
      "['you', 'have', 'my', 'word', 'as', 'a', 'gentleman']\n",
      "['you', 'are', 'sweet']\n",
      "\n",
      "['how', 'do', 'you', 'get', 'your', 'hair', 'to', 'look', 'like', 'that']\n",
      "['<UNK>', 'deep', '<UNK>', 'every', 'two', 'days', 'and', 'i', 'never', 'ever', 'use', 'a', '<UNK>', 'without', 'the', '<UNK>', '<UNK>']\n",
      "\n",
      "['sure', 'have']\n",
      "['i', 'really', 'really', 'really', 'wan', 'na', 'go', 'but', 'i', 'ca', 'not', 'not', 'unless', 'my', 'sister', 'goes']\n",
      "\n",
      "['i', 'really', 'really', 'really', 'wan', 'na', 'go', 'but', 'i', 'ca', 'not', 'not', 'unless', 'my', 'sister', 'goes']\n",
      "['i', 'workin', 'on', 'it', 'but', 'she', 'does', 'not', 'seem', 'to', 'be', 'goin', 'for', 'him']\n",
      "\n",
      "['she', 'not', 'a']\n",
      "['lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<UNK>', '<UNK>', 'in', 'one', 'of', 'her', '<UNK>', 'so', 'i', 'pretty', 'sure', 'she', 'not', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "['lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<UNK>', '<UNK>', 'in', 'one', 'of', 'her', '<UNK>', 'so', 'i', 'pretty', 'sure', 'she', 'not', '<UNK>', '<UNK>', '<UNK>']\n",
      "['so', 'that', 'the', 'kind', 'of', 'guy', 'she', 'likes', 'pretty', 'ones']\n",
      "\n",
      "['so', 'that', 'the', 'kind', 'of', 'guy', 'she', 'likes', 'pretty', 'ones']\n",
      "['who', 'knows', 'all', 'i', 'have', 'ever', 'heard', 'her', 'say', 'is', 'that', 'she', 'would', 'dip', 'before', 'dating', 'a', 'guy', 'that', 'smokes']\n",
      "\n",
      "['hi']\n",
      "['looks', 'like', 'things', 'worked', 'out', 'tonight', 'huh']\n",
      "\n",
      "['you', 'know', '<UNK>']\n",
      "['i', 'believe', 'we', 'share', 'an', 'art', '<UNK>']\n",
      "\n",
      "['have', 'fun', 'tonight']\n",
      "['tons']\n",
      "\n",
      "['i', 'looked', 'for', 'you', 'back', 'at', 'the', 'party', 'but', 'you', 'always', 'seemed', 'to', 'be', 'occupied']\n",
      "['i', 'was']\n",
      "\n",
      "['i', 'was']\n",
      "['you', 'never', 'wanted', 'to', 'go', 'out', 'with', 'did', 'you']\n",
      "\n",
      "['well', 'no']\n",
      "['then', 'that', 'all', 'you', 'had', 'to', 'say']\n",
      "\n",
      "['then', 'that', 'all', 'you', 'had', 'to', 'say']\n",
      "['but']\n",
      "\n",
      "['but']\n",
      "['you', 'always', 'been', 'this', 'selfish']\n",
      "\n",
      "['then', '<UNK>', 'says', 'if', 'you', 'go', 'any', 'lighter', 'you', 'are', 'gon', 'na', 'look', 'like', 'an', 'extra', 'on']\n",
      "['no']\n",
      "\n",
      "['do', 'you', 'listen', 'to', 'this', 'crap']\n",
      "['what', 'crap']\n",
      "\n",
      "['what', 'crap']\n",
      "['me', 'this', '<UNK>', 'blonde', '<UNK>', 'i', 'like', 'boring', 'myself']\n",
      "\n",
      "['me', 'this', '<UNK>', 'blonde', '<UNK>', 'i', 'like', 'boring', 'myself']\n",
      "['thank', 'god', 'if', 'i', 'had', 'to', 'hear', 'one', 'more', 'story', 'about', 'your', '<UNK>']\n",
      "\n",
      "['i', 'figured', 'you', 'would', 'get', 'to', 'the', 'good', 'stuff', 'eventually']\n",
      "['what', 'good', 'stuff']\n",
      "\n",
      "['what', 'good', 'stuff']\n",
      "['the', 'real', 'you']\n",
      "\n",
      "['the', 'real', 'you']\n",
      "['like', 'my', 'fear', 'of', 'wearing', '<UNK>']\n",
      "\n",
      "['i', 'kidding', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '<UNK>', 'and', 'you', 'do', 'not', 'know', 'how', 'to', 'quit']\n",
      "['no']\n",
      "\n",
      "['no']\n",
      "['okay', 'you', 'are', 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie']\n",
      "\n",
      "['wow']\n",
      "['let', 'go']\n",
      "\n",
      "['she', 'okay']\n",
      "['i', 'hope', 'so']\n",
      "\n",
      "['they', 'do', 'to']\n",
      "['they', 'do', 'not']\n",
      "\n",
      "['did', 'you', 'change', 'your', 'hair']\n",
      "['no']\n",
      "\n",
      "['no']\n",
      "['you', 'might', 'wan', 'na', 'think', 'about', 'it']\n",
      "\n",
      "['where', 'did', 'he', 'go', 'he', 'was', 'just', 'here']\n",
      "['who']\n",
      "\n",
      "['who']\n",
      "['joey']\n",
      "\n",
      "['great']\n",
      "['would', 'you', 'mind', 'getting', 'me', 'a', 'drink', 'cameron']\n",
      "\n",
      "['he', 'practically', 'proposed', 'when', 'he', 'found', 'out', 'we', 'had', 'the', 'same', '<UNK>', 'i', 'mean', 'dr', '<UNK>', 'is', 'great', 'an', 'all', 'but', 'he', 'not', 'exactly', 'relevant', 'party', 'conversation']\n",
      "['is', 'he', '<UNK>', 'or', 'dry']\n",
      "\n",
      "['is', 'he', '<UNK>', 'or', 'dry']\n",
      "['combination', 'i', 'do', 'not', 'know', 'i', 'thought', 'he', 'would', 'be', 'different', 'more', 'of', 'a', 'gentleman']\n",
      "\n",
      "['bianca', 'i', 'do', 'not', 'think', 'the', '<UNK>', 'of', 'dating', 'joey', '<UNK>', 'are', 'going', 'to', 'include', '<UNK>', 'and', '<UNK>']\n",
      "['sometimes', 'i', 'wonder', 'if', 'the', 'guys', 'we', 'are', 'supposed', 'to', 'want', 'to', 'go', 'out', 'with', 'are', 'the', 'ones', 'we', 'actually', 'want', 'to', 'go', 'out', 'with', 'you', 'know']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(questions_text[i])\n",
    "    print(answers_text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(sequence, int2vocab):\n",
    "    return [int2vocab[index] for index in sequence if index != METATOKEN_INDEX]\n",
    "\n",
    "def text_to_int(sequence, vocab2int):\n",
    "    return [vocab2int.get(token, vocab2int[UNK]) for token in sequence if token not in codes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_corpus=[]\n",
    "combined_corpus.extend(questions_text)\n",
    "combined_corpus.extend(answers_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393982"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can',\n",
       "  'we',\n",
       "  'make',\n",
       "  'this',\n",
       "  'quick',\n",
       "  '<UNK>',\n",
       "  '<UNK>',\n",
       "  'and',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'are',\n",
       "  'having',\n",
       "  'an',\n",
       "  'incredibly',\n",
       "  '<UNK>',\n",
       "  'public',\n",
       "  'break',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  '<UNK>',\n",
       "  'again'],\n",
       " ['well',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'we',\n",
       "  'would',\n",
       "  'start',\n",
       "  'with',\n",
       "  '<UNK>',\n",
       "  'if',\n",
       "  'that',\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you'],\n",
       " ['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'asking',\n",
       "  'me',\n",
       "  'out',\n",
       "  'that',\n",
       "  'so',\n",
       "  'cute',\n",
       "  'what',\n",
       "  'your',\n",
       "  'name',\n",
       "  'again'],\n",
       " ['no',\n",
       "  'no',\n",
       "  'it',\n",
       "  'my',\n",
       "  'fault',\n",
       "  'we',\n",
       "  'did',\n",
       "  'not',\n",
       "  'have',\n",
       "  'a',\n",
       "  'proper',\n",
       "  'introduction']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 1024\n",
    "model = Word2Vec(sentences=combined_corpus, size=embedding_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84987336,  0.36433262, -0.5068913 , ...,  0.17207386,\n",
       "       -0.11774277, -0.48065165], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['well']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.zeros((len(model.wv.vocab),1024))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "        word_vecs[i] = model[word]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary lengths\n",
      "8101\n",
      "8101\n",
      "8101\n",
      "8101\n",
      "8101\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary lengths\")\n",
    "print(len(word_vecs))\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs.npy',word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Word2Affect Vector - VAD </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_vad=pd.read_excel('Warriner, Kuperman, Brysbaert - 2013 BRM-ANEW expanded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>D.Rat.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.64</td>\n",
       "      <td>21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>11</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  A.SD.Sum  \\\n",
       "1     aardvark        6.26      2.21         19        2.41      1.40   \n",
       "2      abalone        5.30      1.59         20        2.65      1.90   \n",
       "3      abandon        2.84      1.54         19        3.73      2.43   \n",
       "4  abandonment        2.63      1.74         19        4.95      2.64   \n",
       "5        abbey        5.85      1.69         20        2.20      1.70   \n",
       "\n",
       "   A.Rat.Sum  D.Mean.Sum  D.SD.Sum  D.Rat.Sum   ...     A.Rat.L  A.Mean.H  \\\n",
       "1         22        4.27      1.75         15   ...          11      2.55   \n",
       "2         20        4.95      1.79         22   ...          12      2.38   \n",
       "3         22        3.32      2.50         22   ...          11      3.82   \n",
       "4         21        2.64      1.81         28   ...          14      5.29   \n",
       "5         20        5.00      2.02         25   ...           9      2.55   \n",
       "\n",
       "   A.SD.H  A.Rat.H  D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "1    1.29       11      4.12    1.64        8      4.43    1.99        7  \n",
       "2    1.92        8      5.55    2.21       11      4.36    1.03       11  \n",
       "3    2.14       11      2.77    2.09       13      4.11    2.93        9  \n",
       "4    2.63        7      2.31    1.45       16      3.08    2.19       12  \n",
       "5    1.92       11      4.83    2.18       18      5.43    1.62        7  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vad.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "word_vecs_vad = np.zeros((len(model.wv.vocab),1027))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "       for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    match=list((set(synonyms) & set(df_vad[\"Word\"])))\n",
    "    #print(match)\n",
    "    if match:  \n",
    "        #print(\"in\")\n",
    "        word_vecs_vad[i][0:1024] = model[word]\n",
    "        word = match[0]\n",
    "        word_vecs_vad[i][1024]=df_vad[df_vad['Word'] == word][\"V.Mean.Sum\"]\n",
    "        word_vecs_vad[i][1025]=df_vad[df_vad['Word'] == word][\"A.Mean.Sum\"]\n",
    "        word_vecs_vad[i][1026]=df_vad[df_vad['Word'] == word][\"D.Mean.Sum\"]\n",
    "    else:\n",
    "        #print(\"out\")\n",
    "        word_vecs_vad[i][0:1024] = model[word]\n",
    "        word_vecs_vad[i][1024]=5\n",
    "        word_vecs_vad[i][1025]=1\n",
    "        word_vecs_vad[i][1026]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5.85\n",
       "Name: V.Mean.Sum, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vad[df_vad['Word'] == \"abbey\"][\"V.Mean.Sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_VAD.npy',word_vecs_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add EOS tokens to target data now that the embeddings have been trained\n",
    "for i in range(len(answers_int)):\n",
    "    answers_text[i] += \" \" + EOS\n",
    "    answers_int[i].append(METATOKEN_INDEX)\n",
    "    \n",
    "    #answers_int[i].append(answers_vocab_to_int[EOS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196991\n",
      "196991\n",
      "\n",
      "['cameron']\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "\n",
      "['why']\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "\n",
      "['there']\n",
      "['where']\n",
      "\n",
      "['yes', 'i', 'see', 'you', 'have', 'issued', 'each', 'of', 'them', 'with', 'a', 'martini', 'henry', '<UNK>', 'our', '<UNK>', 'for', 'native', '<UNK>', 'one', 'rifle', 'to', 'ten', 'men', 'and', 'only', 'five', 'rounds', 'per', 'rifle']\n",
      "['but', 'will', 'they', 'make', 'good', 'use', 'of', 'them']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort questions and answers by the length of questions.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "max_source_line_length = max( [len(sentence) for sentence in questions_int])\n",
    "max_targ_line_length = max([len(sentence) for sentence in answers_int])\n",
    "max_line_length = max(max_source_line_length, max_targ_line_length)\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for index, sequence in enumerate(questions_int):\n",
    "        if len(sequence) == length:\n",
    "            sorted_questions.append(questions_int[index])\n",
    "            sorted_answers.append(answers_int[index])\n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "indices = [0, 1, 2, len(sorted_questions) - 1]\n",
    "for i in indices:\n",
    "    print(int_to_text(sorted_questions[i], questions_int_to_vocab))\n",
    "    print(int_to_text(sorted_answers[i], answers_int_to_vocab))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: This really should be something like \"preprocess_targets\"\n",
    "def process_decoding_input(target_data, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat( [tf.fill([batch_size, 1], METATOKEN_INDEX), ending], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_cell(rnn_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "def multi_dropout_cell(rnn_size, keep_prob, num_layers):    \n",
    "    return tf.contrib.rnn.MultiRNNCell( [dropout_cell(rnn_size, keep_prob) for _ in range(num_layers)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Create the encoding layer\n",
    "    \n",
    "    Returns a tuple `(outputs, output_states)` where\n",
    "      outputs is a 2-tuple of vectors of dimensions [sequence_length, rnn_size] for the forward and backward passes\n",
    "      output_states is a 2-tupe of the final hidden states of the forward and backward passes\n",
    "    \n",
    "    \"\"\"\n",
    "    forward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    backward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw = forward_cell,\n",
    "                                                   cell_bw = backward_cell,\n",
    "                                                   sequence_length = sequence_lengths,\n",
    "                                                   inputs = rnn_inputs,\n",
    "                                                    dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(enc_state, enc_outputs, dec_embed_input, dec_embeddings, #Inputs\n",
    "                        attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                        keep_prob, beam_width, #Hypeparameters\n",
    "                        source_lengths, target_lengths, batch_size): \n",
    "   \n",
    "    with tf.variable_scope(\"decoding\", reuse=tf.AUTO_REUSE) as decoding_scope:\n",
    "        dec_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "        init_dec_state_size = batch_size\n",
    "        #TRAINING\n",
    "        train_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size, memory=enc_outputs,\n",
    "                                                         memory_sequence_length=source_lengths)\n",
    "        \n",
    "        train_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, train_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_lengths, time_major=False)\n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(train_cell, helper,\n",
    "                            train_cell.zero_state(init_dec_state_size, tf.float32)\n",
    "                                                        .clone(cell_state=enc_state),\n",
    "                            output_layer = output_layer)\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, scope=decoding_scope)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        #INFERENCE\n",
    "        #Tile inputs\n",
    "        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, beam_width)\n",
    "        enc_outputs = tf.contrib.seq2seq.tile_batch(enc_outputs, beam_width)\n",
    "        tiled_source_lengths = tf.contrib.seq2seq.tile_batch(source_lengths, beam_width)\n",
    "        init_dec_state_size *= beam_width\n",
    "        \n",
    "        infer_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size,memory=enc_outputs,\n",
    "                                                          memory_sequence_length=tiled_source_lengths)\n",
    "        infer_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, infer_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        start_tokens = tf.tile( [METATOKEN_INDEX], [batch_size]) #Not by batch_size*beam_width, strangely\n",
    "        end_token = METATOKEN_INDEX\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = infer_cell,\n",
    "            embedding = dec_embeddings,\n",
    "            start_tokens = start_tokens, \n",
    "            end_token = end_token,\n",
    "            beam_width = beam_width,\n",
    "            initial_state = infer_cell.zero_state(init_dec_state_size, tf.float32).clone(cell_state=enc_state),\n",
    "            output_layer = output_layer\n",
    "        )  \n",
    "        final_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, scope=decoding_scope,\n",
    "                                                                      maximum_iterations=50)\n",
    "        \n",
    "        ids = final_decoder_output.predicted_ids\n",
    "        beams = ids\n",
    "                \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(wordVecs,input_data, target_data, keep_prob, batch_size,\n",
    "                  source_lengths, target_lengths,\n",
    "                  vocab_size, enc_embedding_size, dec_embedding_size,\n",
    "                  attn_size, rnn_size, num_layers, beam_width):\n",
    "    \n",
    "\n",
    "    W = tf.Variable(wordVecs,trainable=False,name=\"W\")\n",
    "    enc_embed_input = tf.nn.embedding_lookup(W, input_data)\n",
    "    enc_outputs, enc_states = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, source_lengths)    \n",
    "    concatenated_enc_output = tf.concat(enc_outputs, -1)\n",
    "    init_dec_state = enc_states[0]    \n",
    "    \n",
    "    \n",
    "    dec_input = process_decoding_input(target_data, batch_size)\n",
    "    dec_embeddings = W \n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    print(dec_embed_input.shape)\n",
    "    output_layer = tf.layers.Dense(vocab_size,bias_initializer=tf.zeros_initializer(),activation=tf.nn.relu)\n",
    "    logits, beams = decoding_layer(init_dec_state,\n",
    "                            concatenated_enc_output,\n",
    "                            dec_embed_input,\n",
    "                            dec_embeddings,\n",
    "                            attn_size,\n",
    "                            rnn_size, \n",
    "                            num_layers,\n",
    "                            output_layer,\n",
    "                            keep_prob,\n",
    "                            beam_width,\n",
    "                            source_lengths,\n",
    "                            target_lengths, \n",
    "                            batch_size\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_with_meta = 8102\n",
      "METATOKEN_INDEX = 8101\n",
      "wordVecsWithMeta.shape = (8102, 1027)\n",
      "wordVecsWithMeta[METATOKEN_INDEX] = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Settings used by Asghar et al.\n",
    "rnn_size = 1024\n",
    "num_layers = 1\n",
    "embedding_size = 1027\n",
    "encoding_embedding_size = 1027 ## if word2vec.  1027 if word2Affect vect - VAD.  .... 1028 if VADC\n",
    "decoding_embedding_size = 1027 ## if word2vec.  1027 if word2Affect vect - VAD.  .... 1028 if VADC\n",
    "\n",
    "flag_affect_functions = True # change this flag to false if affect functions are not used\n",
    "attention_size = 256\n",
    "\n",
    "#Training\n",
    "epochs = 50\n",
    "train_batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.00001\n",
    "\n",
    "keep_probability = 0.75\n",
    "vocab_size = len(answers_vocab_to_int)\n",
    "#Decoding\n",
    "beam_width = 10\n",
    "\n",
    "#Validation\n",
    "valid_batch_size = 128\n",
    "\n",
    "wordVecs = np.load('word_Vecs_VAD.npy').astype(np.float32)\n",
    "metatoken_embedding = np.zeros((1, embedding_size), dtype=wordVecs.dtype)\n",
    "wordVecsWithMeta = np.concatenate( (wordVecs, metatoken_embedding), axis=0 )\n",
    "vocab_size_with_meta = wordVecsWithMeta.shape[0]\n",
    "\n",
    "print(\"vocab_size_with_meta =\", vocab_size_with_meta)\n",
    "print(\"METATOKEN_INDEX =\", METATOKEN_INDEX)\n",
    "print(\"wordVecsWithMeta.shape =\", wordVecsWithMeta.shape)\n",
    "print(\"wordVecsWithMeta[METATOKEN_INDEX] =\", wordVecsWithMeta[METATOKEN_INDEX])\n",
    "\n",
    "#print(wordVecsWithMeta.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "(?, ?, 1027)\n",
      "(?, ?, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#                                      batch_size, time\n",
    "input_data = tf.placeholder(tf.int32, [None,       None], name='input')\n",
    "targets = tf.placeholder(tf.int32,    [None,       None], name='targets')\n",
    "lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "#                                          batch_size\n",
    "source_lengths = tf.placeholder(tf.int32, [None], name=\"source_lengths\")\n",
    "target_lengths = tf.placeholder(tf.int32, [None], name=\"target_lengths\")\n",
    "batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, beams = \\\n",
    "seq2seq_model(wordVecsWithMeta,input_data, targets, keep_prob, batch_size,\n",
    "        source_lengths, target_lengths, \n",
    "        vocab_size_with_meta, encoding_embedding_size, decoding_embedding_size, attention_size, rnn_size, num_layers,\n",
    "        beam_width)\n",
    "\n",
    "# Find the shape of the input data for sequence_loss\n",
    "with tf.name_scope(\"optimization\"): \n",
    "    \n",
    "    if(flag_affect_functions):\n",
    "        #Embeddings\n",
    "        Weight = tf.Variable(wordVecsWithMeta,trainable=False,name=\"Weight\")\n",
    "        embed_vec  = tf.nn.embedding_lookup(Weight, input_data)\n",
    "        input_embed =embed_vec[:,:,1024:1027]\n",
    "\n",
    "\n",
    "\n",
    "        dec_res = process_decoding_input(targets, batch_size)\n",
    "        dec_embed = tf.nn.embedding_lookup(Weight, dec_res)\n",
    "        target_embed = dec_embed[:,:,1024:1027]\n",
    "        print(target_embed.shape)\n",
    "    xent = loss_functions.cross_entropy(train_logits, targets, target_lengths)\n",
    "    '''--param values are 0.5, 0.4 and 0.5 as per the hyper parameter tuning mentioned in paper for LDMIN,LDMAX,LDAC functions--\n",
    "    '''\n",
    "    cost = loss_functions.min_affective_dissonance(0.5,train_logits,targets,target_lengths,input_embed,target_embed)\n",
    "    #cost = loss_functions.max_affective_dissonance(0.4,train_logits,targets,target_lengths,input_embed,target_embed)\n",
    "    #cost = loss_functions.max_affective_content(0.5,train_logits,targets,target_lengths,target_embed)\n",
    "    \n",
    "    eval_mask = tf.sequence_mask(target_lengths, dtype=tf.float32)\n",
    "    perplexity = tf.contrib.seq2seq.sequence_loss(train_logits, targets, eval_mask,\n",
    "                                                softmax_loss_function=metrics.perplexity)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    pad_int = METATOKEN_INDEX\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_length - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        source_lengths = np.array( [len(sentence) for sentence in questions_batch] )\n",
    "        target_lengths = np.array( [len(sentence) for sentence in answers_batch])\n",
    "        \n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield source_lengths, target_lengths, pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shuffle(source_sequences, target_sequences):\n",
    "    if len(source_sequences) != len(target_sequences):\n",
    "        raise ValueError(\"Cannot shuffle parallel sets with different numbers of sequences\")\n",
    "    indices = np.random.permutation(len(source_sequences))\n",
    "    shuffled_source = [source_sequences[indices[i]] for i in range(len(indices))]\n",
    "    shuffled_target = [target_sequences[indices[i]] for i in range(len(indices))]\n",
    "    \n",
    "    return (shuffled_source, shuffled_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  show_response(question_int, beams, answer_int = None, best_only=True):\n",
    "    pad_q = METATOKEN_INDEX\n",
    "    print(\"Prompt\")\n",
    "    print(\"  Word Ids: {}\".format([i for i in question_int if i != pad_q]))\n",
    "    print(\"      Text: {}\".format(int_to_text(question_int, questions_int_to_vocab)))\n",
    "    \n",
    "    pad_a = METATOKEN_INDEX\n",
    "    if answer_int is not None:\n",
    "        print(\"Actual Answer\")\n",
    "        print(\"  Word Ids: {}\".format([i for i in answer_int if i != pad_a]))\n",
    "        print(\"      Text: {}\".format(int_to_text(answer_int, answers_int_to_vocab)))\n",
    "\n",
    "    limit = 1 if best_only else beam_width\n",
    "    for i in range(limit):\n",
    "        beam = beams[:, i]\n",
    "        print(\"\\nBeam Answer\", i)\n",
    "        print('  Word Ids: {}'.format([i for i in beam if i != pad_a]))\n",
    "        print('      Text: {}'.format(int_to_text(beam, answers_int_to_vocab)))\n",
    "        \n",
    "def check_response(session, question_int, answer_int=None, best_only=True):\n",
    "    \"\"\"\n",
    "    session - the TensorFlow session\n",
    "    question_int - a list of integers\n",
    "    answer - the actual, correct response (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    two_d_question_int = [question_int]\n",
    "    q_lengths = [len(question_int)]\n",
    "    \n",
    "    [beam_output] = session.run([beams], feed_dict = {input_data: np.array(two_d_question_int, dtype=np.float32),\n",
    "                                                      source_lengths: q_lengths,\n",
    "                                                      keep_prob: 1})\n",
    "    \n",
    "    show_response(question_int, beam_output[0], answer_int, best_only=best_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics_batch(xent, perp, bleu, wer, num_batches=None):\n",
    "    \"\"\"\n",
    "    xent - cross-entropy error summed across batches (unless num_batches is None)\n",
    "    perp - perplexity          summed across batches (unless num_batches is None)\n",
    "    bleu - BLEU                summed across batches (unless num_batches is None)\n",
    "    wer -  Word-Error Rate     summed across batches (unless num_batches is None)\n",
    "    num_batches - the number of batches (used to average each of the metrics)\n",
    "    \"\"\"\n",
    "    if num_batches:\n",
    "        xent /= num_batches\n",
    "        perp /= num_batches\n",
    "        bleu /= num_batches\n",
    "        wer /= num_batches\n",
    "\n",
    "    print(\"Metrics averaged by sentence\")\n",
    "    print(\"\\t  Cross-Entropy: {:>9.6f}\".format(xent))\n",
    "    print(\"\\t     Perplexity: {}\".format(perp))\n",
    "    print(\"\\t           BLEU: {}\".format(bleu))\n",
    "    print(\"\\tWord-Error Rate: {}\".format(wer))\n",
    "    \n",
    "def calc_metrics_beams(beams, prompt_int, answer_int ):\n",
    "    print(\"Sample output\")\n",
    "    show_response(prompt_int, beams, answer_int, best_only=False)\n",
    "    targ_text = [int_to_text(answer_int, answers_int_to_vocab)]\n",
    "    pred_text = [int_to_text(beams[:, 0], answers_int_to_vocab)]\n",
    "    sing_bleu = metrics.bleu(targ_text, pred_text)\n",
    "    sing_wer = metrics.batch_word_error_rate(targ_text, pred_text)\n",
    "    print(\"Metrics for best beam\")\n",
    "    print(\"\\tBLEU: {}\".format(sing_bleu))\n",
    "    print(\"\\t WER: {}\".format(sing_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167443\n",
      "29548\n"
     ]
    }
   ],
   "source": [
    "# Validate the training with 10% of the data\n",
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "\n",
    "# Split the questions and answers into training and validating data\n",
    "(shuffled_questions, shuffled_answers) = parallel_shuffle(sorted_questions, sorted_answers)\n",
    "\n",
    "train_questions = shuffled_questions[train_valid_split:]\n",
    "train_answers = shuffled_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = shuffled_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "\n",
    "#VALIDATION\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_questions))//train_batch_size//2)-1 #Check validation loss every half-epoch\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "#Minimum number of epochs before we start checking sample output with beam search\n",
    "min_epochs_before_validation = 0\n",
    "\n",
    "checkpoint = \"./checkpoints/best_model.ckpt\" \n",
    "\n",
    "early_stopping_metric = \"perplexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model parameters\n",
      "Using perplexity for early stopping after 5 stalled steps\n",
      "Shuffling training data . . .\n",
      "Epoch   1/50 Batch    0/1308 - Loss:  2.305951, Seconds: 120.15\n",
      "Shuffling validation data . . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1bc057e60877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                     [valid_xent, valid_perp, beam_output] = sess.run( [xent, perplexity, beams],\n\u001b[1;32m     43\u001b[0m                         {input_data: questions_batch, targets: answers_batch,\n\u001b[0;32m---> 44\u001b[0;31m                         source_lengths: q_lengths, target_lengths: a_lengths, keep_prob: 1})\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mtotal_xent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_xent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mtotal_perp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_perp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized model parameters\")\n",
    "    print(\"Using {} for early stopping after {} stalled steps\".format(early_stopping_metric, stop))\n",
    "    for epoch_i in range(1, epochs+1):        \n",
    "        print(\"Shuffling training data . . .\")\n",
    "        (train_questions, train_answers) = parallel_shuffle(train_questions, train_answers)\n",
    "                \n",
    "        for batch_i, (q_lengths, a_lengths, questions_batch, answers_batch) in enumerate(\n",
    "                batch_data(train_questions, train_answers, train_batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run([train_op, cost],\n",
    "                {input_data: questions_batch, targets: answers_batch,\n",
    "                 source_lengths: q_lengths, target_lengths: a_lengths,\n",
    "                 lr: learning_rate, keep_prob: keep_probability})\n",
    "            total_train_loss += loss\n",
    "            batch_time = time.time() - start_time\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>9.6f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_questions) // train_batch_size, \n",
    "                              total_train_loss / display_step, \n",
    "                              batch_time*display_step),\n",
    "                         flush=True)\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if batch_i % validation_check == 0 and epoch_i >= min_epochs_before_validation:\n",
    "                print(\"Shuffling validation data . . .\")\n",
    "                (valid_questions, valid_answers) = parallel_shuffle(valid_questions, valid_answers)\n",
    "                total_xent = 0\n",
    "                total_perp = 0\n",
    "                total_bleu = 0\n",
    "                total_wer = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                start_time = time.time()        \n",
    "                for batch_ii, (q_lengths, a_lengths, questions_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_questions, valid_answers, valid_batch_size)):\n",
    "                        \n",
    "                    [valid_xent, valid_perp, beam_output] = sess.run( [xent, perplexity, beams],\n",
    "                        {input_data: questions_batch, targets: answers_batch,\n",
    "                        source_lengths: q_lengths, target_lengths: a_lengths, keep_prob: 1})\n",
    "                    total_xent += valid_xent\n",
    "                    total_perp += valid_perp\n",
    "                    #Text-based metrics\n",
    "                    best_beams = beam_output[:, :, 0]\n",
    "                    beam_text = [int_to_text(best_beams[i], answers_int_to_vocab)\n",
    "                                     for i in range(len(best_beams))]\n",
    "                    target_text = [int_to_text(answers_batch[i], answers_int_to_vocab)\n",
    "                                       for i in range(len(answers_batch))]\n",
    "                    total_bleu += metrics.bleu(target_text, beam_text)\n",
    "                    total_wer  += metrics.batch_word_error_rate(target_text, beam_text)\n",
    "                    num_batches += 1\n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                print(\"Processed validation set in {:>4.2f} seconds\".format(batch_time))\n",
    "                show_metrics_batch(total_xent, total_perp, total_bleu, total_wer, num_batches)\n",
    "                calc_metrics_beams(beam_output[-1, :, :], questions_batch[-1], answers_batch[-1])\n",
    "\n",
    "                # Reduce learning rate, but not below its minimum value\n",
    "                learning_rate *= learning_rate_decay\n",
    "                if learning_rate < min_learning_rate:\n",
    "                    learning_rate = min_learning_rate\n",
    "\n",
    "                avg_valid_loss = total_xent / num_batches\n",
    "                print(summary_valid_loss)\n",
    "                if (len(summary_valid_loss) > 0) and (avg_valid_loss >= min(summary_valid_loss)):\n",
    "                    print(\"No improvement for {}.\".format(early_stopping_metric))\n",
    "                    stop_early += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"New record for {}!\".format(early_stopping_metric)) \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "                summary_valid_loss.append(avg_valid_loss)\n",
    "\n",
    "                if stop_early == stop:\n",
    "                        break\n",
    "                        \n",
    "\n",
    "\n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping training after {} stalled steps\".format(stop))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int, int_to_vocab):\n",
    "    '''Prepare the question for the model'''\n",
    "    cleaned_question = Corpus.clean_sequence(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int[UNK]) for word in cleaned_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/best_model.ckpt\n",
      "Prompt\n",
      "  Word Ids: [278]\n",
      "      Text: ['hello']\n",
      "Actual Answer\n",
      "  Word Ids: [5, 3004]\n",
      "      Text: ['it', 'elvis']\n",
      "\n",
      "Beam Answer 0\n",
      "  Word Ids: []\n",
      "      Text: []\n",
      "\n",
      "Beam Answer 1\n",
      "  Word Ids: [1]\n",
      "      Text: ['i']\n",
      "\n",
      "Beam Answer 2\n",
      "  Word Ids: [1, 1]\n",
      "      Text: ['i', 'i']\n",
      "\n",
      "Beam Answer 3\n",
      "  Word Ids: [1, 1, 1]\n",
      "      Text: ['i', 'i', 'i']\n",
      "\n",
      "Beam Answer 4\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 5\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 6\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 1, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'i', 'a', 'a']\n",
      "\n",
      "Beam Answer 7\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 8\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 1]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'i']\n",
      "\n",
      "Beam Answer 9\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 1]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'i']\n"
     ]
    }
   ],
   "source": [
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(sorted_questions))\n",
    "question_int = sorted_questions[random]\n",
    "answer_int = sorted_answers[random]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Run the model with the input question\n",
    "    saver.restore(sess, checkpoint)\n",
    "    check_response(sess, question_int, answer_int, best_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
