{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a chatbot using conversations from Cornell University's [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). The main features of our model are LSTM cells, a bidirectional dynamic RNN, and decoders with attention. \n",
    "\n",
    "The conversations will be cleaned rather extensively to help the model to produce better responses. As part of the cleaning process, punctuation will be removed, rare words will be replaced with \"UNK\" (our \"unknown\" token), longer sentences will not be used, and all letters will be in the lowercase. \n",
    "\n",
    "With a larger amount of data, it would be more practical to keep features, such as punctuation. However, I am using FloydHub's GPU services and I don't want to get carried away with too training for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Local libraries\n",
    "import metrics\n",
    "import loss_functions\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code to load the data is courtesy of https://github.com/suriyadeepan/practical_seq2seq/blob/master/datasets/cornell_corpus/data.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = os.path.join(\"corpora\")\n",
    "var_names = [\"train_prompts\", \"train_answers\", \"valid_prompts\", \"valid_answers\", \"vocab_lines\"]\n",
    "file_names = [os.path.join(corpus_dir, var_name + \".txt\") for var_name in var_names[:-1]] + [\"vocab.txt\"]\n",
    "\n",
    "for (file_name, var_name) in zip(file_names, var_names):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as r:\n",
    "        text = [ [token for token in line.strip().split(\" \")] for line in r.readlines()]\n",
    "        exec(\"{} = {}\".format(var_name, text))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2int = {pair[0]:int(pair[1]) for pair in vocab_lines}\n",
    "int2vocab = {index:word for (word, index) in vocab2int.items()}\n",
    "(questions_vocab_to_int, questions_int_to_vocab) = (vocab2int, int2vocab)\n",
    "(prompts_vocab_to_int, prompts_int_to_vocab) = (vocab2int, int2vocab) #Alternative names to ease the transition\n",
    "\n",
    "\n",
    "(answers_vocab_to_int, answers_int_to_vocab) = (vocab2int, int2vocab)\n",
    "\n",
    "UNK = vocab_lines[0][0]\n",
    "METATOKEN_INDEX = len(vocab2int)\n",
    "META = \"<META>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "GO = \"<GO>\"\n",
    "codes = [META, EOS, PAD, GO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(sequence, int2vocab):\n",
    "    return [int2vocab[index] for index in sequence if index != METATOKEN_INDEX]\n",
    "\n",
    "def text_to_int(sequence, vocab2int):\n",
    "    return [vocab2int.get(token, vocab2int[UNK]) for token in sequence if token not in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in train_prompts]\n",
    "train_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in train_answers]\n",
    "valid_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in valid_prompts]\n",
    "valid_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in valid_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'says', 'she', 'does', 'not', 'see', 'how', 'you', 'do', 'it', '.']\n",
      "[0, 292, 54, 103, 9, 75, 57, 3, 12, 11, 1]\n",
      "['somewhere', '.', 'see', 'i', 'was', 'taken', 'away', 'by', 'the', '<UNK>', 'when', 'i', 'was', 'a', 'baby', '.', 'i', 'was', 'adopted', '.']\n",
      "[575, 1, 75, 4, 30, 605, 177, 129, 6, 0, 84, 4, 30, 10, 305, 1, 4, 30, 5491, 1]\n",
      "['it', 'is', 'just', 'a', 'work', 'in', 'progress', ',', 'kinda', 'rough', '.']\n",
      "[11, 8, 40, 10, 155, 21, 2377, 2, 788, 1412, 1]\n",
      "['your', 'majesty', ',', 'herr', 'mozart', '-']\n",
      "[29, 1381, 2, 2833, 2494, 25]\n",
      "['<UNK>', 'is', 'quite', 'all', 'right', '.', 'relax', ',', '<UNK>']\n",
      "[0, 8, 419, 42, 65, 1, 850, 2, 0]\n",
      "['it', 'is', 'my', 'father', ',', 'gone', '<UNK>', '.', 'the', 'baron', 'couer', 'de', 'noir', 'is', 'his', 'guest', 'and', 'must', 'be', 'provided', 'with', 'some', 'sport', '.']\n",
      "[11, 8, 32, 224, 2, 333, 0, 1, 6, 1686, 7605, 1093, 5537, 8, 83, 1555, 15, 153, 33, 7071, 36, 91, 2501, 1]\n",
      "['i', 'appreciate', 'that', ',', 'because', 'some', 'of', 'that', 'stuff', '.', 'you', 'know', ',', 'it', 'was', 'two', 'in', 'the', 'morning', 'and', '.']\n",
      "[4, 895, 13, 2, 132, 91, 17, 13, 300, 1, 3, 28, 2, 11, 30, 135, 21, 6, 285, 15, 1]\n",
      "['but', 'if', 'cable', 'killed', 'grunemann', 'why', 'would', 'he', 'get', 'you', 'hired', 'to', 'look', 'for', 'grunemann', '?']\n",
      "[37, 52, 2836, 274, 5682, 67, 44, 24, 46, 3, 1395, 7, 100, 27, 5682, 5]\n",
      "['yes', ',', 'monsieur', '.']\n",
      "[72, 2, 1995, 1]\n",
      "['nice', '<UNK>', '.', 'good', 'boy', '.']\n",
      "[222, 0, 1, 78, 256, 1]\n",
      "['what', 'is', 'wrong', 'with', 'her', '.']\n",
      "[16, 8, 212, 36, 69, 1]\n",
      "['adopted', '.', 'i', 'should', 'have', 'know', '.', 'of', 'course', '.', 'if', 'it', 'was', 'a', 'snake', ',', 'it', 'would', 'bit', 'me', '!']\n",
      "[5491, 1, 4, 117, 20, 28, 1, 17, 194, 1, 52, 11, 30, 10, 2332, 2, 11, 44, 464, 22, 18]\n",
      "['this', 'guy', 'killed', 'a', 'mess', 'of', 'people', '.']\n",
      "[26, 168, 274, 10, 1091, 17, 125, 1]\n",
      "['yes', ',', 'what', 'about', 'him', '?']\n",
      "[72, 2, 16, 43, 56, 5]\n",
      "['<UNK>', 'is', 'a', 'great', 'honor', ',', 'sir', '.', 'i-', 'i-']\n",
      "[0, 8, 10, 189, 840, 2, 145, 1, 2234, 2234]\n",
      "['sport', ',', 'indeed', '.']\n",
      "[2501, 2, 1447, 1]\n",
      "['-', 'the', 'part', 'about', '``', 'we', 'should', 'embrace', 'what', 'it', 'is', 'still', '<UNK>', 'about', 'our', 'enthusiasm', \"''\", '-']\n",
      "[25, 6, 347, 43, 109, 19, 117, 8691, 16, 11, 8, 149, 0, 43, 128, 10215, 112, 25]\n",
      "['because', 'he', 'knew', 'i', 'could', 'not', 'leave', 'the', 'case', 'alone', '.', 'and', 'this', 'way', 'at', 'least', 'he', 'would', 'keep', 'track', 'of', 'it', '.', 'and', 'me', '.']\n",
      "[132, 24, 253, 4, 79, 9, 206, 6, 332, 318, 1, 15, 26, 107, 62, 381, 24, 44, 185, 1400, 17, 11, 1, 15, 22, 1]\n",
      "['right', 'away', '.', 'instantly', '!']\n",
      "[65, 177, 1, 8389, 18]\n",
      "['walk', 'away', ',', 'jack', '.']\n",
      "[451, 177, 2, 406, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_prompts[i])\n",
    "    print(train_prompts_int[i])\n",
    "for i in range(10):\n",
    "    print(train_answers[i])\n",
    "    print(train_answers_int[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus = train_prompts + train_answers + valid_prompts + valid_answers\n",
    "len(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<UNK>', 'says', 'she', 'does', 'not', 'see', 'how', 'you', 'do', 'it', '.'],\n",
       " ['somewhere',\n",
       "  '.',\n",
       "  'see',\n",
       "  'i',\n",
       "  'was',\n",
       "  'taken',\n",
       "  'away',\n",
       "  'by',\n",
       "  'the',\n",
       "  '<UNK>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'was',\n",
       "  'a',\n",
       "  'baby',\n",
       "  '.',\n",
       "  'i',\n",
       "  'was',\n",
       "  'adopted',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'just',\n",
       "  'a',\n",
       "  'work',\n",
       "  'in',\n",
       "  'progress',\n",
       "  ',',\n",
       "  'kinda',\n",
       "  'rough',\n",
       "  '.'],\n",
       " ['your', 'majesty', ',', 'herr', 'mozart', '-'],\n",
       " ['<UNK>', 'is', 'quite', 'all', 'right', '.', 'relax', ',', '<UNK>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 1024\n",
    "model = Word2Vec(sentences=combined_corpus, size=embedding_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['well'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.zeros((len(model.wv.vocab),1024))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "        word_vecs[vocab2int[word]] = model[word]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary lengths\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary lengths\")\n",
    "print(len(word_vecs))\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs.npy',word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Word2Affect Vector - VAD </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_vad=pd.read_excel('Warriner, Kuperman, Brysbaert - 2013 BRM-ANEW expanded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>D.Rat.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.64</td>\n",
       "      <td>21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>11</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  A.SD.Sum  \\\n",
       "1     aardvark        6.26      2.21         19        2.41      1.40   \n",
       "2      abalone        5.30      1.59         20        2.65      1.90   \n",
       "3      abandon        2.84      1.54         19        3.73      2.43   \n",
       "4  abandonment        2.63      1.74         19        4.95      2.64   \n",
       "5        abbey        5.85      1.69         20        2.20      1.70   \n",
       "\n",
       "   A.Rat.Sum  D.Mean.Sum  D.SD.Sum  D.Rat.Sum   ...     A.Rat.L  A.Mean.H  \\\n",
       "1         22        4.27      1.75         15   ...          11      2.55   \n",
       "2         20        4.95      1.79         22   ...          12      2.38   \n",
       "3         22        3.32      2.50         22   ...          11      3.82   \n",
       "4         21        2.64      1.81         28   ...          14      5.29   \n",
       "5         20        5.00      2.02         25   ...           9      2.55   \n",
       "\n",
       "   A.SD.H  A.Rat.H  D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "1    1.29       11      4.12    1.64        8      4.43    1.99        7  \n",
       "2    1.92        8      5.55    2.21       11      4.36    1.03       11  \n",
       "3    2.14       11      2.77    2.09       13      4.11    2.93        9  \n",
       "4    2.63        7      2.31    1.45       16      3.08    2.19       12  \n",
       "5    1.92       11      4.83    2.18       18      5.43    1.62        7  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vad.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wordvecs=[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    list_wordvecs.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vad = set(df_vad['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return lemmatizer.lemmatize(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vad['Lemma_Word'] = df_vad.Word.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "word_vecs_vad = np.zeros((len(model.wv.vocab),1027))\n",
    "count_vad=0\n",
    "count_neutral=0\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    if lemma in set(df_vad['Lemma_Word']):\n",
    "        #print(word)\n",
    "        count_vad=count_vad+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=df_vad.loc[df_vad['Lemma_Word'] == lemma, 'V.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1025]=df_vad.loc[df_vad['Lemma_Word'] == lemma, 'A.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1026]=df_vad.loc[df_vad['Lemma_Word'] == lemma, 'D.Mean.Sum'].iloc[0]\n",
    "        #print(word_vecs_vad[i])\n",
    "    else:\n",
    "        #print(\"out\")\n",
    "        count_neutral=count_neutral+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=5\n",
    "        word_vecs_vad[vocab2int[word]][1025]=1\n",
    "        word_vecs_vad[vocab2int[word]][1026]=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6850\n",
      "5151\n"
     ]
    }
   ],
   "source": [
    "print(count_vad)\n",
    "print(count_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352    6.05\n",
       "Name: V.Mean.Sum, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val =df_vad[df_vad['Word'] == \"mailbox\"][\"V.Mean.Sum\"]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_VAD.npy',word_vecs_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - counterfitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_counterfit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_counterfit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "list_counterfit =list(model_counterfit_affect.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lemma_counterfit={}\n",
    "for word in list_counterfit:\n",
    "    dict_lemma_counterfit[lemmatize_text(word)]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs_counterfit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    if lemma in dict_lemma_counterfit.keys():\n",
    "        word_vecs_counterfit_affect[vocab2int[word]] = model_counterfit_affect[dict_lemma_counterfit[lemma]]\n",
    "    else:\n",
    "        list_word_not_found.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_unknown = np.mean(word_vecs_counterfit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_unknown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_word_not_found:\n",
    "    word_vecs_counterfit_affect[i] = word_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12001, 303)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs_counterfit_affect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_counterfit_affect.npy',word_vecs_counterfit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - retrofitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_retrofit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_retrofit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "list_retrofit =list(model_retrofit_affect.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lemma_retrofit={}\n",
    "for word in list_retrofit:\n",
    "    dict_lemma_retrofit[lemmatize_text(word)]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs_retrofit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found_retro =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    if lemma in dict_lemma_retrofit.keys():\n",
    "        word_vecs_retrofit_affect[vocab2int[word]] = model_retrofit_affect[dict_lemma_retrofit[lemma]]\n",
    "    else:\n",
    "        list_word_not_found_retro.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found_retro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_unknown_retro = np.mean(word_vecs_retrofit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_word_not_found_retro:\n",
    "    word_vecs_retrofit_affect[i] = word_unknown_retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_retrofit_affect.npy',word_vecs_retrofit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'wrong', 'with', 'her', '.', '<EOS>'], ['adopted', '.', 'i', 'should', 'have', 'know', '.', 'of', 'course', '.', 'if', 'it', 'was', 'a', 'snake', ',', 'it', 'would', 'bit', 'me', '!', '<EOS>'], ['this', 'guy', 'killed', 'a', 'mess', 'of', 'people', '.', '<EOS>'], ['yes', ',', 'what', 'about', 'him', '?', '<EOS>'], ['<UNK>', 'is', 'a', 'great', 'honor', ',', 'sir', '.', 'i-', 'i-', '<EOS>']]\n",
      "[[16, 8, 212, 36, 69, 1, 12001], [5491, 1, 4, 117, 20, 28, 1, 17, 194, 1, 52, 11, 30, 10, 2332, 2, 11, 44, 464, 22, 18, 12001], [26, 168, 274, 10, 1091, 17, 125, 1, 12001], [72, 2, 16, 43, 56, 5, 12001], [0, 8, 10, 189, 840, 2, 145, 1, 2234, 2234, 12001]]\n"
     ]
    }
   ],
   "source": [
    "#Add EOS tokens to target data now that the embeddings have been trained\n",
    "def append_eos(answers_text, answers_int):\n",
    "    appended_text = [sequence + [EOS] for sequence in answers_text]\n",
    "    appended_ints = [sequence + [METATOKEN_INDEX] for sequence in answers_int]\n",
    "    return (appended_text, appended_ints)\n",
    "\n",
    "(train_answers, train_answers_int) = append_eos(train_answers, train_answers_int)\n",
    "(valid_answers, valid_answers_int) = append_eos(valid_answers, valid_answers_int)\n",
    "\n",
    "print(train_answers[:5])\n",
    "print(train_answers_int[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat( [tf.fill([batch_size, 1], METATOKEN_INDEX), ending], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_cell(rnn_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "def multi_dropout_cell(rnn_size, keep_prob, num_layers):    \n",
    "    return tf.contrib.rnn.MultiRNNCell( [dropout_cell(rnn_size, keep_prob) for _ in range(num_layers)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Create the encoding layer\n",
    "    \n",
    "    Returns a tuple `(outputs, output_states)` where\n",
    "      outputs is a 2-tuple of vectors of dimensions [sequence_length, rnn_size] for the forward and backward passes\n",
    "      output_states is a 2-tupe of the final hidden states of the forward and backward passes\n",
    "    \n",
    "    \"\"\"\n",
    "    forward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    backward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw = forward_cell,\n",
    "                                                   cell_bw = backward_cell,\n",
    "                                                   sequence_length = sequence_lengths,\n",
    "                                                   inputs = rnn_inputs,\n",
    "                                                    dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(enc_state, enc_outputs, dec_embed_input, dec_embeddings, #Inputs\n",
    "                        attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                        keep_prob,  #Hypeparameters\n",
    "                        source_lengths, target_lengths, batch_size): \n",
    "   \n",
    "    with tf.variable_scope(\"decoding\") as scope:\n",
    "        dec_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "        init_dec_state_size = batch_size\n",
    "        attn_mech = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size, memory=enc_outputs,\n",
    "                                                         memory_sequence_length=source_lengths)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        init_dec_state = attn_cell.zero_state(init_dec_state_size, tf.float32).clone(cell_state=enc_state)\n",
    "        \n",
    "        decoder_gen = lambda helper: tf.contrib.seq2seq.BasicDecoder(attn_cell, helper, init_dec_state,\n",
    "                                        output_layer = output_layer)\n",
    "        \n",
    "        #TRAINING\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_lengths)\n",
    "        train_decoder = decoder_gen(train_helper)\n",
    "        train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, scope=scope)\n",
    "        train_logits = train_outputs.rnn_output\n",
    "\n",
    "        #INFERENCE\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                                start_tokens = tf.tile([METATOKEN_INDEX],\n",
    "                                                                                       [batch_size]),\n",
    "                                                                 end_token = METATOKEN_INDEX)\n",
    "        infer_decoder = decoder_gen(infer_helper)\n",
    "        infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, scope=scope, maximum_iterations=100)\n",
    "        infer_ids = infer_outputs.sample_id\n",
    "                \n",
    "    return train_logits, infer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(enc_embed_input, dec_embed_input, dec_embeddings, #Inputs\n",
    "                  source_lengths, target_lengths, batch_size, #Dimensions\n",
    "                  attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                  keep_prob): #Hyperparameters\n",
    "    \n",
    "    enc_outputs, enc_states = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, source_lengths)    \n",
    "    concatenated_enc_output = tf.concat(enc_outputs, -1)\n",
    "    init_dec_state = enc_states[0]    \n",
    "    \n",
    "    \n",
    "    train_logits, infer_ids = decoding_layer(init_dec_state,\n",
    "                            concatenated_enc_output,\n",
    "                            dec_embed_input,\n",
    "                            dec_embeddings,\n",
    "                            attn_size,\n",
    "                            rnn_size, \n",
    "                            num_layers,\n",
    "                            output_layer,\n",
    "                            keep_prob,\n",
    "                            source_lengths,\n",
    "                            target_lengths, \n",
    "                            batch_size\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return train_logits, infer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_with_meta = 12002\n",
      "METATOKEN_INDEX = 12001\n",
      "wordVecsWithMeta.shape = (12002, 1027)\n",
      "wordVecsWithMeta[METATOKEN_INDEX] = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#FLAGS\n",
    "flag_affect_functions = False # change this flag to false if affect functions are not used\n",
    "flag_vad_values = True # Set to true if using VAD values appended onto existing embeddings\n",
    "\n",
    "#Settings used by Asghar et al.\n",
    "rnn_size = 1024\n",
    "num_layers = 1\n",
    "attention_size = 256\n",
    "epochs_before_affective_loss = 40\n",
    "epochs = 50\n",
    "train_batch_size = 64\n",
    "\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.0001\n",
    "keep_probability = 0.75\n",
    "\n",
    "#Validation\n",
    "valid_batch_size = 64\n",
    "\n",
    "embedding_model_path = \"word_Vecs_VAD.npy\"\n",
    "wordVecs = np.load(embedding_model_path).astype(np.float32)\n",
    "\n",
    "embedding_size = wordVecs.shape[1] #Dynamically determine embedding size from loaded embedding file\n",
    "\n",
    "metatoken_embedding = np.zeros((1, embedding_size), dtype=wordVecs.dtype)\n",
    "wordVecsWithMeta = np.concatenate( (wordVecs, metatoken_embedding), axis=0 )\n",
    "vocab_size_with_meta = wordVecsWithMeta.shape[0]\n",
    "\n",
    "print(\"vocab_size_with_meta =\", vocab_size_with_meta)\n",
    "print(\"METATOKEN_INDEX =\", METATOKEN_INDEX)\n",
    "print(\"wordVecsWithMeta.shape =\", wordVecsWithMeta.shape)\n",
    "print(\"wordVecsWithMeta[METATOKEN_INDEX] =\", wordVecsWithMeta[METATOKEN_INDEX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#                                      batch_size, sequence_length\n",
    "input_data = tf.placeholder(tf.int32, [None,       None], name='input')\n",
    "targets = tf.placeholder(tf.int32,    [None,       None], name='targets')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "#Determines whether we use a normal loss function or an affective loss function\n",
    "train_affect = tf.placeholder(tf.bool, shape=(), name=\"train_affect\")\n",
    "\n",
    "\n",
    "\n",
    "#                                          batch_size\n",
    "source_lengths = tf.placeholder(tf.int32, [None], name=\"source_lengths\")\n",
    "target_lengths = tf.placeholder(tf.int32, [None], name=\"target_lengths\")\n",
    "batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "full_embeddings = tf.Variable(wordVecsWithMeta,trainable=False,name=\"Weight\")\n",
    "enc_embed_input = tf.nn.embedding_lookup(full_embeddings, input_data)\n",
    "dec_embed_input = tf.nn.embedding_lookup(full_embeddings, process_decoding_input(targets, batch_size))\n",
    "\n",
    "output_layer = tf.layers.Dense(vocab_size_with_meta,bias_initializer=tf.zeros_initializer(),activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, infer_ids = \\\n",
    "seq2seq_model(enc_embed_input, dec_embed_input, full_embeddings,\n",
    "        source_lengths, target_lengths, batch_size, \n",
    "        attention_size, rnn_size, num_layers, output_layer,\n",
    "        keep_prob)\n",
    "\n",
    "\n",
    "# Find the shape of the input data for sequence_loss\n",
    "with tf.name_scope(\"optimization\"): \n",
    "    \n",
    "    mask = tf.sequence_mask(target_lengths, dtype=tf.float32)\n",
    "    xent = loss_functions.cross_entropy(train_logits, targets, mask)\n",
    "    perplexity = tf.contrib.seq2seq.sequence_loss(train_logits, targets, mask, metrics.perplexity)\n",
    "    \n",
    "    \n",
    "    vad_values = full_embeddings[:, -3:]\n",
    "    input_vad_values =enc_embed_input[:,:,1024:1027]\n",
    "    lambda_param_max_affective_content = 0.5\n",
    "    lambda_param_min_affective_dissonance=0.5\n",
    "    lambda_param_max_affective_dissonance = 0.4\n",
    "    neutral_vector = tf.constant([5.0, 1.0, 5.0], dtype=tf.float32)\n",
    "    max_affective_content = loss_functions.max_affective_content(lambda_param_max_affective_content,train_logits, targets, \n",
    "                                                                 vad_values,neutral_vector, mask)\n",
    "    min_affective_dissonance = loss_functions.min_affective_dissonance(lambda_param_min_affective_dissonance,train_logits, targets, \n",
    "                                                                 input_vad_values,vad_values, mask)\n",
    "    max_affective_dissonance = loss_functions.max_affective_dissonance(lambda_param_max_affective_dissonance,train_logits, targets, \n",
    "                                                                 input_vad_values,vad_values, mask)\n",
    "    \n",
    "    \n",
    "    train_cost = tf.cond(train_affect, true_fn=lambda: max_affective_content, false_fn=lambda: xent)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(train_cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response(prompt_int, prediction, answer_int = None):\n",
    "    pad_q = METATOKEN_INDEX\n",
    "    print(\"Prompt\")\n",
    "    print(\"  Word Ids: {}\".format([i for i in prompt_int if i != pad_q]))\n",
    "    print(\"      Text: {}\".format(int_to_text(prompt_int, prompts_int_to_vocab)))\n",
    "    \n",
    "    pad_a = METATOKEN_INDEX\n",
    "    if answer_int is not None:\n",
    "        print(\"Target Answer\")\n",
    "        print(\"  Word Ids: {}\".format([i for i in answer_int if i != pad_a]))\n",
    "        print(\"      Text: {}\".format(int_to_text(answer_int, answers_int_to_vocab)))\n",
    "\n",
    "    print(\"\\nPrediction\")\n",
    "    print('  Word Ids: {}'.format([i for i in prediction if i != pad_a]))\n",
    "    print('      Text: {}'.format(int_to_text(prediction, answers_int_to_vocab)))\n",
    "        \n",
    "def check_response(session, prompt_int, answer_int=None):\n",
    "    \"\"\"\n",
    "    session - the TensorFlow session\n",
    "    question_int - a list of integers\n",
    "    answer - the actual, correct response (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    two_d_prompt_int = [prompt_int]\n",
    "    p_lengths = [len(prompt_int)]\n",
    "    \n",
    "    [infer_ids_output] = session.run([infer_ids], feed_dict = {input_data: np.array(two_d_prompt_int, dtype=np.float32),\n",
    "                                                      source_lengths: p_lengths,\n",
    "                                                      keep_prob: 1})\n",
    "    \n",
    "    show_response(prompt_int, infer_ids_output[0], answer_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    pad_int = METATOKEN_INDEX\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_length - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        source_lengths = np.array( [len(sentence) for sentence in questions_batch] )\n",
    "        target_lengths = np.array( [len(sentence) for sentence in answers_batch])\n",
    "        \n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield source_lengths, target_lengths, pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shuffle(source_sequences, target_sequences):\n",
    "    if len(source_sequences) != len(target_sequences):\n",
    "        raise ValueError(\"Cannot shuffle parallel sets with different numbers of sequences\")\n",
    "    indices = np.random.permutation(len(source_sequences))\n",
    "    shuffled_source = [source_sequences[indices[i]] for i in range(len(indices))]\n",
    "    shuffled_target = [target_sequences[indices[i]] for i in range(len(indices))]\n",
    "    \n",
    "    return (shuffled_source, shuffled_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "\n",
    "#VALIDATION\n",
    "validation_check = ((len(train_prompts))//train_batch_size//2)-1 #Check validation loss every half-epoch\n",
    "#Minimum number of epochs before we start checking sample output\n",
    "min_epochs_before_validation = 1\n",
    "\n",
    "#Used to make uniquely directories, not to identify when a model is saved\n",
    "time_string = time.strftime(\"%b%d_%H:%M:%S\")\n",
    "\n",
    "checkpoint_dir = os.path.join(\"checkpoints\", time_string)\n",
    "if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n",
    "checkpoint_best = str(checkpoint_dir) + \"/\" + \"best_model.ckpt\" \n",
    "checkpoint_latest = str(checkpoint_dir) + \"/\" + \"latest_model.ckpt\"\n",
    "\n",
    "log_dir = os.path.join(\"logging\", time_string)\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "train_log = os.path.join(log_dir, \"train.csv\")\n",
    "valid_log = os.path.join(log_dir, \"valid.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_entries(csv_path, *fields, header = False):\n",
    "    if len(fields[0]) < 1: return\n",
    "    mode = \"w\" if header else \"a\"\n",
    "    with open(csv_path, mode, encoding=\"utf-8\") as log:\n",
    "        lines = []\n",
    "        num_lines = len(fields[0])\n",
    "        lines = \"\\n\".join(\",\".join([str(field[i]) for field in fields]) \n",
    "                          for i in range(num_lines)\n",
    "        )\n",
    "        log.write(lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_fields(log_fields):\n",
    "    for field in log_fields:\n",
    "        field.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty training log logging/Jun23_17:17:35/train.csv, validation log logging/Jun23_17:17:35/valid.csv\n",
      "Initialized model parameters, wrote initial model to checkpoints/Jun23_17:17:35/latest_model.ckpt\n",
      "Beginning training with cross-entropy loss.\n",
      "Shuffling training data . . .\n",
      "Epoch   1/50 Batch    0/2683 - Loss-per-Token:  9.397799, Seconds: 33.76\n"
     ]
    }
   ],
   "source": [
    "train_epoch_nos = []\n",
    "train_batch_tokens = [] #Number of tokens in a batch\n",
    "train_batch_losses = [] #Per-token loss for a batch\n",
    "train_log_fields = [train_epoch_nos, train_batch_tokens, train_batch_losses]\n",
    "\n",
    "valid_epoch_nos = []\n",
    "valid_check_nos = []\n",
    "valid_batch_tokens = []\n",
    "valid_batch_losses = []\n",
    "valid_log_fields = [valid_epoch_nos, valid_check_nos, valid_batch_tokens, valid_batch_losses]\n",
    "\n",
    "log_entries(train_log, [\"epoch\"], [\"num_tokens\"], [\"loss_per_token\"], header=True)\n",
    "log_entries(valid_log, [\"epoch\"], [\"check\"], [\"num_tokens\"], [\"perplexity_per_token\"], header=True)\n",
    "print(\"Initialized empty training log {}, validation log {}\".format(train_log, valid_log))\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "use_affect_func = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().save(sess, checkpoint_latest)\n",
    "    print(\"Initialized model parameters, wrote initial model to {}\".format(checkpoint_latest))\n",
    "    print(\"Beginning training with cross-entropy loss.\")\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        if not use_affect_func and epoch_i > epochs_before_affective_loss:\n",
    "            print(\"Switching from cross-entropy loss to an affective loss function\")\n",
    "            use_affect_func = True        \n",
    "        \n",
    "        print(\"Shuffling training data . . .\")\n",
    "        (train_prompts_int, train_answers_int) = parallel_shuffle(train_prompts_int, train_answers_int)\n",
    "        \n",
    "        valid_check_no = 1\n",
    "        \n",
    "        for batch_i, (p_lengths, a_lengths, prompts_batch, answers_batch) in enumerate(\n",
    "                batch_data(train_prompts_int, train_answers_int, train_batch_size)):\n",
    "            train_start_time = time.time()\n",
    "            \n",
    "            #VALIDATION CHECK\n",
    "            if batch_i % validation_check == 0 and epoch_i > min_epochs_before_validation:\n",
    "                print(\"Shuffling validation data . . .\")\n",
    "                (valid_prompts_int, valid_answers_int) = parallel_shuffle(valid_prompts_int, valid_answers_int)\n",
    "                \n",
    "                clear_fields(valid_log_fields)\n",
    "\n",
    "                \n",
    "                valid_start_time = time.time()\n",
    "                for batch_ii, (p_lengths, a_lengths, prompts_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_prompts_int, valid_answers_int, valid_batch_size)):\n",
    "\n",
    "                    [valid_loss] = sess.run([perplexity],\n",
    "                        {input_data: prompts_batch, targets: answers_batch,\n",
    "                        source_lengths: p_lengths, target_lengths: a_lengths, keep_prob: 1})\n",
    "                    valid_epoch_nos.append(epoch_i)\n",
    "                    valid_check_nos.append(valid_check_no)\n",
    "                    valid_batch_tokens.append(sum(a_lengths))\n",
    "                    valid_batch_losses.append(valid_loss)\n",
    "\n",
    "                \n",
    "                valid_check_no += 1\n",
    "                duration = time.time() - valid_start_time\n",
    "                avg_valid_loss = sum(loss*tokens \n",
    "                        for (loss, tokens) in zip(valid_batch_losses, valid_batch_tokens)) / sum(valid_batch_tokens)\n",
    "                \n",
    "                log_entries(valid_log, *(valid_log_fields))\n",
    "                clear_fields(valid_log_fields)\n",
    "                print(\"Processed validation set in {:>4.2f} seconds\".format(duration))\n",
    "                print(\"Average perplexity per token = {}\".format(avg_valid_loss))\n",
    "                if avg_valid_loss >= best_valid_loss:\n",
    "                    print(\"No improvement for validation loss.\")\n",
    "                else:\n",
    "                    best_valid_loss = avg_valid_loss\n",
    "                    print(\"New record for validation loss!\")\n",
    "                    print(\"Saving best model to {}\".format(checkpoint_best))\n",
    "                    tf.train.Saver().save(sess, checkpoint_best)\n",
    "                check_response(sess, prompts_batch[-1], answers_batch[-1])\n",
    "            \n",
    "            #TRAINING\n",
    "            _, loss = sess.run([train_op, train_cost],\n",
    "                {input_data: prompts_batch, targets: answers_batch,\n",
    "                 source_lengths: p_lengths, target_lengths: a_lengths,\n",
    "                 keep_prob: keep_probability,\n",
    "                 train_affect: use_affect_func})\n",
    "            train_epoch_nos.append(epoch_i)\n",
    "            train_batch_losses.append(loss)\n",
    "            train_batch_tokens.append(sum(a_lengths))\n",
    "            \n",
    "            if batch_i % display_step == 0:\n",
    "                duration = time.time() - train_start_time\n",
    "                avg_train_loss = sum(loss*tokens \n",
    "                        for (loss, tokens) in zip(train_batch_losses, train_batch_tokens)) / sum(train_batch_tokens)\n",
    "                    \n",
    "                log_entries(train_log, *(train_log_fields))\n",
    "                clear_fields(train_log_fields)\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss-per-Token: {:>9.6f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i, epochs, batch_i, len(train_prompts_int) // train_batch_size, \n",
    "                              avg_train_loss, duration),\n",
    "                         flush=True)\n",
    "\n",
    "        print(\"{} epochs completed, saving model to {}\".format(epoch_i, checkpoint_latest))\n",
    "        tf.train.Saver().save(sess, checkpoint_latest)\n",
    "        log_entries(train_log, *(train_log_fields))\n",
    "        clear_fields(train_log_fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int, int_to_vocab):\n",
    "    '''Prepare the question for the model'''\n",
    "    cleaned_question = Corpus.clean_sequence(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int[UNK]) for word in cleaned_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(train_prompts_int))\n",
    "prompt_int = train_prompts_int[random]\n",
    "answer_int = train_answers_int[random]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Run the model with the input question\n",
    "    saver.restore(sess, checkpoint)\n",
    "    check_response(sess, prompt_int, answer_int, best_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
