{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a chatbot using conversations from Cornell University's [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). The main features of our model are LSTM cells, a bidirectional dynamic RNN, and decoders with attention. \n",
    "\n",
    "The conversations will be cleaned rather extensively to help the model to produce better responses. As part of the cleaning process, punctuation will be removed, rare words will be replaced with \"UNK\" (our \"unknown\" token), longer sentences will not be used, and all letters will be in the lowercase. \n",
    "\n",
    "With a larger amount of data, it would be more practical to keep features, such as punctuation. However, I am using FloydHub's GPU services and I don't want to get carried away with too training for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Local libraries\n",
    "import metrics\n",
    "import loss_functions\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code to load the data is courtesy of https://github.com/suriyadeepan/practical_seq2seq/blob/master/datasets/cornell_corpus/data.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = os.path.join(\"corpora\")\n",
    "var_names = [\"train_prompts\", \"train_answers\", \"valid_prompts\", \"valid_answers\", \"vocab_lines\"]\n",
    "file_names = [os.path.join(corpus_dir, var_name + \".txt\") for var_name in var_names[:-1]] + [\"vocab.txt\"]\n",
    "\n",
    "for (file_name, var_name) in zip(file_names, var_names):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as r:\n",
    "        text = [ [token for token in line.strip().split(\" \")] for line in r.readlines()]\n",
    "        exec(\"{} = {}\".format(var_name, text))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2int = {pair[0]:int(pair[1]) for pair in vocab_lines}\n",
    "int2vocab = {index:word for (word, index) in vocab2int.items()}\n",
    "(questions_vocab_to_int, questions_int_to_vocab) = (vocab2int, int2vocab)\n",
    "(prompts_vocab_to_int, prompts_int_to_vocab) = (vocab2int, int2vocab) #Alternative names to ease the transition\n",
    "\n",
    "\n",
    "(answers_vocab_to_int, answers_int_to_vocab) = (vocab2int, int2vocab)\n",
    "\n",
    "UNK = vocab_lines[0][0]\n",
    "METATOKEN_INDEX = len(vocab2int)\n",
    "META = \"<META>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "GO = \"<GO>\"\n",
    "codes = [META, EOS, PAD, GO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(sequence, int2vocab):\n",
    "    return [int2vocab[index] for index in sequence if index != METATOKEN_INDEX]\n",
    "\n",
    "def text_to_int(sequence, vocab2int):\n",
    "    return [vocab2int.get(token, vocab2int[UNK]) for token in sequence if token not in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in train_prompts]\n",
    "train_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in train_answers]\n",
    "valid_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in valid_prompts]\n",
    "valid_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in valid_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'says', 'she', 'does', 'not', 'see', 'how', 'you', 'do', 'it', '.']\n",
      "[0, 292, 54, 103, 9, 75, 57, 3, 12, 11, 1]\n",
      "['somewhere', '.', 'see', 'i', 'was', 'taken', 'away', 'by', 'the', '<UNK>', 'when', 'i', 'was', 'a', 'baby', '.', 'i', 'was', 'adopted', '.']\n",
      "[575, 1, 75, 4, 30, 605, 177, 129, 6, 0, 84, 4, 30, 10, 305, 1, 4, 30, 5491, 1]\n",
      "['it', 'is', 'just', 'a', 'work', 'in', 'progress', ',', 'kinda', 'rough', '.']\n",
      "[11, 8, 40, 10, 155, 21, 2377, 2, 788, 1412, 1]\n",
      "['your', 'majesty', ',', 'herr', 'mozart', '-']\n",
      "[29, 1381, 2, 2833, 2494, 25]\n",
      "['<UNK>', 'is', 'quite', 'all', 'right', '.', 'relax', ',', '<UNK>']\n",
      "[0, 8, 419, 42, 65, 1, 850, 2, 0]\n",
      "['it', 'is', 'my', 'father', ',', 'gone', '<UNK>', '.', 'the', 'baron', 'couer', 'de', 'noir', 'is', 'his', 'guest', 'and', 'must', 'be', 'provided', 'with', 'some', 'sport', '.']\n",
      "[11, 8, 32, 224, 2, 333, 0, 1, 6, 1686, 7605, 1093, 5537, 8, 83, 1555, 15, 153, 33, 7071, 36, 91, 2501, 1]\n",
      "['i', 'appreciate', 'that', ',', 'because', 'some', 'of', 'that', 'stuff', '.', 'you', 'know', ',', 'it', 'was', 'two', 'in', 'the', 'morning', 'and', '.']\n",
      "[4, 895, 13, 2, 132, 91, 17, 13, 300, 1, 3, 28, 2, 11, 30, 135, 21, 6, 285, 15, 1]\n",
      "['but', 'if', 'cable', 'killed', 'grunemann', 'why', 'would', 'he', 'get', 'you', 'hired', 'to', 'look', 'for', 'grunemann', '?']\n",
      "[37, 52, 2836, 274, 5682, 67, 44, 24, 46, 3, 1395, 7, 100, 27, 5682, 5]\n",
      "['yes', ',', 'monsieur', '.']\n",
      "[72, 2, 1995, 1]\n",
      "['nice', '<UNK>', '.', 'good', 'boy', '.']\n",
      "[222, 0, 1, 78, 256, 1]\n",
      "['what', 'is', 'wrong', 'with', 'her', '.']\n",
      "[16, 8, 212, 36, 69, 1]\n",
      "['adopted', '.', 'i', 'should', 'have', 'know', '.', 'of', 'course', '.', 'if', 'it', 'was', 'a', 'snake', ',', 'it', 'would', 'bit', 'me', '!']\n",
      "[5491, 1, 4, 117, 20, 28, 1, 17, 194, 1, 52, 11, 30, 10, 2332, 2, 11, 44, 464, 22, 18]\n",
      "['this', 'guy', 'killed', 'a', 'mess', 'of', 'people', '.']\n",
      "[26, 168, 274, 10, 1091, 17, 125, 1]\n",
      "['yes', ',', 'what', 'about', 'him', '?']\n",
      "[72, 2, 16, 43, 56, 5]\n",
      "['<UNK>', 'is', 'a', 'great', 'honor', ',', 'sir', '.', 'i-', 'i-']\n",
      "[0, 8, 10, 189, 840, 2, 145, 1, 2234, 2234]\n",
      "['sport', ',', 'indeed', '.']\n",
      "[2501, 2, 1447, 1]\n",
      "['-', 'the', 'part', 'about', '``', 'we', 'should', 'embrace', 'what', 'it', 'is', 'still', '<UNK>', 'about', 'our', 'enthusiasm', \"''\", '-']\n",
      "[25, 6, 347, 43, 109, 19, 117, 8691, 16, 11, 8, 149, 0, 43, 128, 10215, 112, 25]\n",
      "['because', 'he', 'knew', 'i', 'could', 'not', 'leave', 'the', 'case', 'alone', '.', 'and', 'this', 'way', 'at', 'least', 'he', 'would', 'keep', 'track', 'of', 'it', '.', 'and', 'me', '.']\n",
      "[132, 24, 253, 4, 79, 9, 206, 6, 332, 318, 1, 15, 26, 107, 62, 381, 24, 44, 185, 1400, 17, 11, 1, 15, 22, 1]\n",
      "['right', 'away', '.', 'instantly', '!']\n",
      "[65, 177, 1, 8389, 18]\n",
      "['walk', 'away', ',', 'jack', '.']\n",
      "[451, 177, 2, 406, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_prompts[i])\n",
    "    print(train_prompts_int[i])\n",
    "for i in range(10):\n",
    "    print(train_answers[i])\n",
    "    print(train_answers_int[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus = train_prompts + train_answers + valid_prompts + valid_answers\n",
    "len(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<UNK>', 'says', 'she', 'does', 'not', 'see', 'how', 'you', 'do', 'it', '.'],\n",
       " ['somewhere',\n",
       "  '.',\n",
       "  'see',\n",
       "  'i',\n",
       "  'was',\n",
       "  'taken',\n",
       "  'away',\n",
       "  'by',\n",
       "  'the',\n",
       "  '<UNK>',\n",
       "  'when',\n",
       "  'i',\n",
       "  'was',\n",
       "  'a',\n",
       "  'baby',\n",
       "  '.',\n",
       "  'i',\n",
       "  'was',\n",
       "  'adopted',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'just',\n",
       "  'a',\n",
       "  'work',\n",
       "  'in',\n",
       "  'progress',\n",
       "  ',',\n",
       "  'kinda',\n",
       "  'rough',\n",
       "  '.'],\n",
       " ['your', 'majesty', ',', 'herr', 'mozart', '-'],\n",
       " ['<UNK>', 'is', 'quite', 'all', 'right', '.', 'relax', ',', '<UNK>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 1024\n",
    "model = Word2Vec(sentences=combined_corpus, size=embedding_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['well'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ',',\n",
       " 'you',\n",
       " 'i',\n",
       " '?',\n",
       " '<UNK>',\n",
       " 'the',\n",
       " 'to',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'it',\n",
       " 'do',\n",
       " 'that',\n",
       " 'are',\n",
       " 'and',\n",
       " 'what',\n",
       " 'of',\n",
       " '!',\n",
       " 'we',\n",
       " 'have',\n",
       " 'in',\n",
       " 'me',\n",
       " 'am',\n",
       " 'he',\n",
       " '-',\n",
       " 'this',\n",
       " 'for',\n",
       " 'know',\n",
       " 'your',\n",
       " 'was',\n",
       " 'no',\n",
       " 'my',\n",
       " 'be',\n",
       " 'on',\n",
       " 'can',\n",
       " 'with',\n",
       " 'but',\n",
       " 'they',\n",
       " 'did',\n",
       " 'just',\n",
       " 'like',\n",
       " 'all',\n",
       " 'about',\n",
       " 'would',\n",
       " 'there',\n",
       " 'get',\n",
       " 'going',\n",
       " \"'s\",\n",
       " 'got',\n",
       " 'so',\n",
       " 'out',\n",
       " 'if',\n",
       " 'here',\n",
       " 'she',\n",
       " 'shall',\n",
       " 'him',\n",
       " 'how',\n",
       " 'want',\n",
       " 'up',\n",
       " 'will',\n",
       " 'think',\n",
       " 'at',\n",
       " 'one',\n",
       " 'well',\n",
       " 'right',\n",
       " 'go',\n",
       " 'why',\n",
       " 'now',\n",
       " 'her',\n",
       " \"'\",\n",
       " 'who',\n",
       " 'yes',\n",
       " 'oh',\n",
       " 'us',\n",
       " 'see',\n",
       " 'yeah',\n",
       " 'as',\n",
       " 'good',\n",
       " 'could',\n",
       " 'where',\n",
       " '<',\n",
       " '>',\n",
       " 'his',\n",
       " 'when',\n",
       " 'tell',\n",
       " 'an',\n",
       " 'from',\n",
       " 'come',\n",
       " 'been',\n",
       " 'time',\n",
       " 'some',\n",
       " 'say',\n",
       " 'or',\n",
       " 'had',\n",
       " 'back',\n",
       " 'let',\n",
       " 'them',\n",
       " 'then',\n",
       " 'something',\n",
       " 'look',\n",
       " 'man',\n",
       " 'take',\n",
       " 'does',\n",
       " 'never',\n",
       " 'mean',\n",
       " 'too',\n",
       " 'way',\n",
       " 'really',\n",
       " '``',\n",
       " 'sure',\n",
       " 'make',\n",
       " \"''\",\n",
       " 'okay',\n",
       " 'has',\n",
       " 'little',\n",
       " 'more',\n",
       " 'should',\n",
       " 'down',\n",
       " 'any',\n",
       " 'need',\n",
       " 'said',\n",
       " 'maybe',\n",
       " 'only',\n",
       " 'over',\n",
       " 'people',\n",
       " 'thing',\n",
       " 'very',\n",
       " 'our',\n",
       " 'by',\n",
       " 'off',\n",
       " 'much',\n",
       " 'because',\n",
       " 'nothing',\n",
       " 'give',\n",
       " 'two',\n",
       " 'anything',\n",
       " 'doing',\n",
       " 'sorry',\n",
       " 'talk',\n",
       " 'mr.',\n",
       " 'thought',\n",
       " '/you',\n",
       " 'love',\n",
       " 'even',\n",
       " 'sir',\n",
       " 'call',\n",
       " 'ever',\n",
       " 'find',\n",
       " 'still',\n",
       " 'before',\n",
       " 'into',\n",
       " 'told',\n",
       " 'must',\n",
       " 'work',\n",
       " 'money',\n",
       " 'night',\n",
       " 'life',\n",
       " 'other',\n",
       " 'better',\n",
       " 'around',\n",
       " 'first',\n",
       " 'things',\n",
       " 'help',\n",
       " 'than',\n",
       " 'last',\n",
       " 'these',\n",
       " 'believe',\n",
       " 'guy',\n",
       " 'long',\n",
       " 'please',\n",
       " 'always',\n",
       " 'those',\n",
       " 'hey',\n",
       " 'god',\n",
       " 'name',\n",
       " 'put',\n",
       " 'away',\n",
       " 'after',\n",
       " 'shit',\n",
       " 'again',\n",
       " 'years',\n",
       " 'day',\n",
       " 'old',\n",
       " 'place',\n",
       " 'keep',\n",
       " 'hell',\n",
       " 'home',\n",
       " 'new',\n",
       " 'great',\n",
       " 'feel',\n",
       " 'everything',\n",
       " 'big',\n",
       " 'kind',\n",
       " 'course',\n",
       " 'bad',\n",
       " 'their',\n",
       " 'talking',\n",
       " 'remember',\n",
       " 'guess',\n",
       " 'fuck',\n",
       " 'understand',\n",
       " 'dead',\n",
       " 'lot',\n",
       " 'kill',\n",
       " 'thank',\n",
       " 'leave',\n",
       " 'might',\n",
       " 'else',\n",
       " 'ask',\n",
       " 'wait',\n",
       " 'girl',\n",
       " 'wrong',\n",
       " 'fine',\n",
       " 'three',\n",
       " 'happened',\n",
       " 'hear',\n",
       " 'made',\n",
       " 'ta',\n",
       " 'enough',\n",
       " 'stop',\n",
       " 'wanted',\n",
       " 'nice',\n",
       " 'someone',\n",
       " 'father',\n",
       " 'real',\n",
       " 'mind',\n",
       " 'huh',\n",
       " 'listen',\n",
       " 'every',\n",
       " 'yourself',\n",
       " 'done',\n",
       " 'getting',\n",
       " 'uh',\n",
       " 'house',\n",
       " 'stay',\n",
       " 'through',\n",
       " 'fucking',\n",
       " 'another',\n",
       " 'mother',\n",
       " 'own',\n",
       " 'job',\n",
       " 'left',\n",
       " 'care',\n",
       " 'trying',\n",
       " 'thanks',\n",
       " 'saw',\n",
       " 'came',\n",
       " 'car',\n",
       " 'try',\n",
       " 'heard',\n",
       " 'being',\n",
       " 'world',\n",
       " 'knew',\n",
       " 'friend',\n",
       " 'seen',\n",
       " 'boy',\n",
       " 'same',\n",
       " 'tonight',\n",
       " 'may',\n",
       " 'went',\n",
       " 'coming',\n",
       " 'looking',\n",
       " ';',\n",
       " 'room',\n",
       " 'show',\n",
       " 'pretty',\n",
       " 'guys',\n",
       " ':',\n",
       " 'whole',\n",
       " 'used',\n",
       " 'business',\n",
       " 'live',\n",
       " 'miss',\n",
       " 'killed',\n",
       " 'ya',\n",
       " 'best',\n",
       " 'idea',\n",
       " 'which',\n",
       " 'matter',\n",
       " 'many',\n",
       " 'tomorrow',\n",
       " \"'em\",\n",
       " 'next',\n",
       " 'called',\n",
       " 'morning',\n",
       " 'found',\n",
       " 'five',\n",
       " 'yet',\n",
       " 'wife',\n",
       " 'without',\n",
       " 'already',\n",
       " 'says',\n",
       " 'woman',\n",
       " 'run',\n",
       " 'once',\n",
       " 'myself',\n",
       " 'saying',\n",
       " 'use',\n",
       " 'while',\n",
       " 'stuff',\n",
       " 'few',\n",
       " 'start',\n",
       " 'head',\n",
       " 'men',\n",
       " 'baby',\n",
       " 'ago',\n",
       " 'dad',\n",
       " 'most',\n",
       " 'play',\n",
       " 'probably',\n",
       " 'together',\n",
       " 'since',\n",
       " 'hundred',\n",
       " 'today',\n",
       " 'nobody',\n",
       " 'somebody',\n",
       " 'meet',\n",
       " 'alone',\n",
       " 'crazy',\n",
       " 'days',\n",
       " 'hard',\n",
       " 'took',\n",
       " 'afraid',\n",
       " 'school',\n",
       " 'wants',\n",
       " 'forget',\n",
       " 'exactly',\n",
       " 'such',\n",
       " 'problem',\n",
       " 'anyone',\n",
       " 'minute',\n",
       " 'case',\n",
       " 'gone',\n",
       " 'four',\n",
       " 'kid',\n",
       " 'ten',\n",
       " 'son',\n",
       " 'anyway',\n",
       " 'friends',\n",
       " 'deal',\n",
       " 'hello',\n",
       " 'hope',\n",
       " 'until',\n",
       " 'worry',\n",
       " 'damn',\n",
       " 'point',\n",
       " 'part',\n",
       " 'jesus',\n",
       " 'read',\n",
       " 'knows',\n",
       " 'doctor',\n",
       " 'bring',\n",
       " 'looks',\n",
       " 'minutes',\n",
       " 'na',\n",
       " 'supposed',\n",
       " 'wan',\n",
       " 'makes',\n",
       " 'thinking',\n",
       " 'word',\n",
       " 'die',\n",
       " 'true',\n",
       " 'hi',\n",
       " 'mom',\n",
       " 'shut',\n",
       " 'lost',\n",
       " 'hurt',\n",
       " 'week',\n",
       " 'working',\n",
       " 'hold',\n",
       " 'actually',\n",
       " 'watch',\n",
       " 'mrs.',\n",
       " 'story',\n",
       " 'late',\n",
       " 'brother',\n",
       " 'happen',\n",
       " 'year',\n",
       " 'alright',\n",
       " 'family',\n",
       " 'least',\n",
       " 'pay',\n",
       " 'soon',\n",
       " 'everybody',\n",
       " 'sleep',\n",
       " 'drink',\n",
       " 'face',\n",
       " 'ass',\n",
       " 'turn',\n",
       " 'town',\n",
       " 'move',\n",
       " 'married',\n",
       " 'both',\n",
       " 'john',\n",
       " 'six',\n",
       " 'ready',\n",
       " 'under',\n",
       " 'asked',\n",
       " 'happy',\n",
       " 'open',\n",
       " 'times',\n",
       " 'having',\n",
       " 'thousand',\n",
       " 'phone',\n",
       " 'far',\n",
       " 'jack',\n",
       " 'gave',\n",
       " 'kids',\n",
       " 'easy',\n",
       " 'question',\n",
       " 'whatever',\n",
       " 'sit',\n",
       " 'anybody',\n",
       " 'trouble',\n",
       " 'beautiful',\n",
       " 'different',\n",
       " 'half',\n",
       " 'hate',\n",
       " 'quite',\n",
       " 'cut',\n",
       " 'end',\n",
       " 'wish',\n",
       " 'mine',\n",
       " 'young',\n",
       " 'police',\n",
       " 'taking',\n",
       " 'shot',\n",
       " 'hit',\n",
       " 'each',\n",
       " 'couple',\n",
       " 'door',\n",
       " 'gets',\n",
       " 'yours',\n",
       " 'change',\n",
       " 'eat',\n",
       " 'later',\n",
       " 'important',\n",
       " 'chance',\n",
       " 'making',\n",
       " 'dollars',\n",
       " 'dr.',\n",
       " 'trust',\n",
       " 'check',\n",
       " 'met',\n",
       " 'fuckin',\n",
       " 'rest',\n",
       " \"'d\",\n",
       " 'death',\n",
       " 'hours',\n",
       " 'anymore',\n",
       " 'walk',\n",
       " 'person',\n",
       " 'everyone',\n",
       " 'close',\n",
       " 'set',\n",
       " 'suppose',\n",
       " 'telling',\n",
       " 'hand',\n",
       " 'second',\n",
       " 'funny',\n",
       " 'sometimes',\n",
       " 'gun',\n",
       " 'bit',\n",
       " 'truth',\n",
       " 'heart',\n",
       " 'bet',\n",
       " 'reason',\n",
       " 'comes',\n",
       " 'honey',\n",
       " 'either',\n",
       " 'side',\n",
       " 'answer',\n",
       " 'captain',\n",
       " 'party',\n",
       " 'inside',\n",
       " 'number',\n",
       " 'means',\n",
       " 'goes',\n",
       " 'buy',\n",
       " 'war',\n",
       " 'tried',\n",
       " 'excuse',\n",
       " 'bed',\n",
       " 'its',\n",
       " 'sort',\n",
       " 'though',\n",
       " 'game',\n",
       " \"'ll\",\n",
       " 'sick',\n",
       " 'send',\n",
       " 'eyes',\n",
       " 'stand',\n",
       " 'against',\n",
       " 'break',\n",
       " 'office',\n",
       " 'waiting',\n",
       " 'christ',\n",
       " 'white',\n",
       " 'along',\n",
       " 'women',\n",
       " 'pick',\n",
       " 'serious',\n",
       " 'ah',\n",
       " 'died',\n",
       " 'water',\n",
       " 'body',\n",
       " 'daddy',\n",
       " 'almost',\n",
       " 'months',\n",
       " 'stupid',\n",
       " 'high',\n",
       " 'alive',\n",
       " 'million',\n",
       " 'fire',\n",
       " 'book',\n",
       " 'husband',\n",
       " 'speak',\n",
       " 'running',\n",
       " 'dinner',\n",
       " 'black',\n",
       " 'country',\n",
       " 'seem',\n",
       " 'hands',\n",
       " 'figure',\n",
       " 'ship',\n",
       " 'blood',\n",
       " 'seems',\n",
       " 'hour',\n",
       " 'fact',\n",
       " 'drive',\n",
       " 'sounds',\n",
       " 'goddamn',\n",
       " 'shoot',\n",
       " 'line',\n",
       " 'twenty',\n",
       " 'behind',\n",
       " 'started',\n",
       " 'city',\n",
       " 'between',\n",
       " 'lady',\n",
       " 'george',\n",
       " 'sense',\n",
       " 'perhaps',\n",
       " 'power',\n",
       " 'full',\n",
       " 'bullshit',\n",
       " 'scared',\n",
       " 'dear',\n",
       " 'also',\n",
       " 'ahead',\n",
       " 'feeling',\n",
       " 'street',\n",
       " 'living',\n",
       " 'news',\n",
       " 'asking',\n",
       " 'boys',\n",
       " 'safe',\n",
       " 'himself',\n",
       " 'outside',\n",
       " 'frank',\n",
       " 'happens',\n",
       " 'lose',\n",
       " 'able',\n",
       " 'hot',\n",
       " 'fight',\n",
       " 'sent',\n",
       " 'promise',\n",
       " 'fun',\n",
       " 'were',\n",
       " 'sound',\n",
       " 'weeks',\n",
       " 'girls',\n",
       " 'dog',\n",
       " 'write',\n",
       " 'somewhere',\n",
       " 'bill',\n",
       " 'york',\n",
       " 'children',\n",
       " 'brought',\n",
       " 'glad',\n",
       " 'sister',\n",
       " 'president',\n",
       " 'needs',\n",
       " 'free',\n",
       " 'front',\n",
       " 'moment',\n",
       " 'cool',\n",
       " 'ok',\n",
       " 'plan',\n",
       " 'poor',\n",
       " \"'because\",\n",
       " 'eight',\n",
       " 'movie',\n",
       " 'save',\n",
       " 'possible',\n",
       " 'harry',\n",
       " 'lives',\n",
       " 'sex',\n",
       " 'beat',\n",
       " 'lie',\n",
       " 'child',\n",
       " 'thinks',\n",
       " 'light',\n",
       " 'till',\n",
       " 'taken',\n",
       " 'wonderful',\n",
       " 'daughter',\n",
       " 'lucky',\n",
       " 'cold',\n",
       " 'fifty',\n",
       " 'leaving',\n",
       " 'rather',\n",
       " 'tired',\n",
       " 'calling',\n",
       " 'kidding',\n",
       " 'worth',\n",
       " 'touch',\n",
       " 'expect',\n",
       " 'picture',\n",
       " 'i.',\n",
       " 'fast',\n",
       " 'food',\n",
       " 'company',\n",
       " 'pull',\n",
       " 'special',\n",
       " 'human',\n",
       " 'piece',\n",
       " 'questions',\n",
       " 'king',\n",
       " 'none',\n",
       " 'learn',\n",
       " 'small',\n",
       " 'hair',\n",
       " 'parents',\n",
       " 'straight',\n",
       " 'looked',\n",
       " 'perfect',\n",
       " 'words',\n",
       " 'playing',\n",
       " ']',\n",
       " 'worked',\n",
       " 'air',\n",
       " 'luck',\n",
       " 'mary',\n",
       " 'miles',\n",
       " 'interested',\n",
       " 'mister',\n",
       " 'known',\n",
       " 'certainly',\n",
       " 'hospital',\n",
       " 'order',\n",
       " 'coffee',\n",
       " 'explain',\n",
       " 'bitch',\n",
       " 'catch',\n",
       " \"c'mon\",\n",
       " 'works',\n",
       " 'follow',\n",
       " 'accident',\n",
       " 'control',\n",
       " 'clear',\n",
       " 'ride',\n",
       " 'state',\n",
       " 'dream',\n",
       " 'act',\n",
       " 'date',\n",
       " 'except',\n",
       " 'hotel',\n",
       " 'sam',\n",
       " 'besides',\n",
       " 'absolutely',\n",
       " 'unless',\n",
       " 'general',\n",
       " 'mr',\n",
       " 'meeting',\n",
       " 'secret',\n",
       " 'seven',\n",
       " 'wonder',\n",
       " 'buddy',\n",
       " 'seeing',\n",
       " 'mouth',\n",
       " 'outta',\n",
       " 'worse',\n",
       " 'past',\n",
       " '[',\n",
       " 'takes',\n",
       " 'others',\n",
       " 'talked',\n",
       " 'hang',\n",
       " 'drop',\n",
       " 'top',\n",
       " 'tom',\n",
       " 'swear',\n",
       " 'simple',\n",
       " 'report',\n",
       " 'major',\n",
       " 'throw',\n",
       " 'clean',\n",
       " 'mad',\n",
       " 'handle',\n",
       " 'apartment',\n",
       " 'yesterday',\n",
       " 'information',\n",
       " 'felt',\n",
       " 'worried',\n",
       " 'madam',\n",
       " 'charlie',\n",
       " 'boss',\n",
       " 'fault',\n",
       " 'earth',\n",
       " 'feet',\n",
       " 'wear',\n",
       " 'red',\n",
       " 'quit',\n",
       " 'dark',\n",
       " 'paid',\n",
       " 'david',\n",
       " 'less',\n",
       " 'paul',\n",
       " 'clock',\n",
       " 'joe',\n",
       " 'bob',\n",
       " 'cops',\n",
       " 'army',\n",
       " 'meant',\n",
       " 'clothes',\n",
       " 'sign',\n",
       " 'strange',\n",
       " 'mistake',\n",
       " 'uh-huh',\n",
       " 'month',\n",
       " 'finish',\n",
       " 'thirty',\n",
       " 'busy',\n",
       " 'giving',\n",
       " 'fall',\n",
       " 'smart',\n",
       " 'choice',\n",
       " 'loved',\n",
       " 'difference',\n",
       " 'marry',\n",
       " 'anywhere',\n",
       " 'nine',\n",
       " 'music',\n",
       " 'blow',\n",
       " 'road',\n",
       " 'bank',\n",
       " 'walter',\n",
       " 'sweet',\n",
       " 'turned',\n",
       " 'class',\n",
       " 'cop',\n",
       " 'win',\n",
       " 'lying',\n",
       " 'liked',\n",
       " 'plane',\n",
       " 'private',\n",
       " 'michael',\n",
       " 'eh',\n",
       " 'figured',\n",
       " 'changed',\n",
       " 'early',\n",
       " 'terrible',\n",
       " 'ones',\n",
       " 'lord',\n",
       " 'careful',\n",
       " 'fair',\n",
       " 'hurry',\n",
       " 'personal',\n",
       " 'trip',\n",
       " 'paper',\n",
       " 'near',\n",
       " 'caught',\n",
       " 'become',\n",
       " 'record',\n",
       " 'law',\n",
       " 'happening',\n",
       " 'murder',\n",
       " '.I',\n",
       " 'moving',\n",
       " 'bucks',\n",
       " 'peter',\n",
       " 'jimmy',\n",
       " 'ben',\n",
       " 'honest',\n",
       " 'watching',\n",
       " 'goodbye',\n",
       " 'kinda',\n",
       " 'broke',\n",
       " 'lunch',\n",
       " 'interesting',\n",
       " 'store',\n",
       " 'sake',\n",
       " 'system',\n",
       " 'tough',\n",
       " 'favor',\n",
       " 'rich',\n",
       " 'kept',\n",
       " 'building',\n",
       " 'eye',\n",
       " 'killing',\n",
       " 'american',\n",
       " 'longer',\n",
       " 'boat',\n",
       " 'tv',\n",
       " 'agent',\n",
       " 'dude',\n",
       " 'ought',\n",
       " 'given',\n",
       " 'rose',\n",
       " 'fucked',\n",
       " 'blue',\n",
       " 'imagine',\n",
       " 'middle',\n",
       " 'nick',\n",
       " 'fifteen',\n",
       " 'sell',\n",
       " 'completely',\n",
       " 'officer',\n",
       " 'situation',\n",
       " 'twelve',\n",
       " 'quiet',\n",
       " 'dick',\n",
       " 'problems',\n",
       " 'ray',\n",
       " 'future',\n",
       " 'evening',\n",
       " 'darling',\n",
       " 'dance',\n",
       " 'chief',\n",
       " 'involved',\n",
       " 'brain',\n",
       " 'max',\n",
       " 'books',\n",
       " 'certain',\n",
       " 'dangerous',\n",
       " 'mama',\n",
       " 'born',\n",
       " 'calls',\n",
       " 'honor',\n",
       " 'likes',\n",
       " 'radio',\n",
       " 'floor',\n",
       " 'kiss',\n",
       " 'weird',\n",
       " 'respect',\n",
       " 'relax',\n",
       " 'cash',\n",
       " 'security',\n",
       " 'english',\n",
       " 'johnny',\n",
       " 'james',\n",
       " 'count',\n",
       " 'message',\n",
       " 'killer',\n",
       " 'jim',\n",
       " 'court',\n",
       " 'afternoon',\n",
       " 'list',\n",
       " 'realize',\n",
       " 'offer',\n",
       " 'fool',\n",
       " 'drunk',\n",
       " 'key',\n",
       " 'history',\n",
       " 'asshole',\n",
       " 'needed',\n",
       " 'sitting',\n",
       " 'letter',\n",
       " 'uncle',\n",
       " 'finished',\n",
       " 'stick',\n",
       " 'nervous',\n",
       " 'bother',\n",
       " 'service',\n",
       " 'lawyer',\n",
       " 'bag',\n",
       " 'picked',\n",
       " 'staying',\n",
       " 'wake',\n",
       " 'owe',\n",
       " 'hardly',\n",
       " 'neither',\n",
       " 'pictures',\n",
       " 'jail',\n",
       " 'jake',\n",
       " 'college',\n",
       " 'eddie',\n",
       " 'pain',\n",
       " 'instead',\n",
       " 'bought',\n",
       " 'bastard',\n",
       " 'tape',\n",
       " 'upset',\n",
       " 'appreciate',\n",
       " 'forgive',\n",
       " 'forgot',\n",
       " 'welcome',\n",
       " 'grand',\n",
       " 'ice',\n",
       " 'team',\n",
       " '&',\n",
       " 'doubt',\n",
       " 'wrote',\n",
       " 'land',\n",
       " 'finally',\n",
       " 'totally',\n",
       " 'suit',\n",
       " 'window',\n",
       " 'stopped',\n",
       " 'carry',\n",
       " 'spend',\n",
       " 'missed',\n",
       " 'lived',\n",
       " 'machine',\n",
       " 'christmas',\n",
       " 'across',\n",
       " 'missing',\n",
       " 'voice',\n",
       " 'hungry',\n",
       " 'wow',\n",
       " 'table',\n",
       " 'fix',\n",
       " 'ring',\n",
       " 'wearing',\n",
       " 'driving',\n",
       " 'prove',\n",
       " 'space',\n",
       " 'ran',\n",
       " 'calm',\n",
       " 'named',\n",
       " 'age',\n",
       " 'decided',\n",
       " 'evil',\n",
       " 'pleasure',\n",
       " 'lots',\n",
       " 'club',\n",
       " 'ha',\n",
       " 'surprise',\n",
       " 'join',\n",
       " 'computer',\n",
       " 'third',\n",
       " 'pass',\n",
       " 'listening',\n",
       " 'strong',\n",
       " 'train',\n",
       " 'dress',\n",
       " 'deep',\n",
       " 'quick',\n",
       " 'fly',\n",
       " 'slow',\n",
       " 'fear',\n",
       " 'gives',\n",
       " 'putting',\n",
       " 'attack',\n",
       " 'um',\n",
       " 'station',\n",
       " 'cost',\n",
       " 'public',\n",
       " 'short',\n",
       " 'gotten',\n",
       " 'dying',\n",
       " 'return',\n",
       " 'island',\n",
       " 'birthday',\n",
       " 'bunch',\n",
       " 'moved',\n",
       " 'force',\n",
       " 'joke',\n",
       " 'saved',\n",
       " 'bar',\n",
       " 'charge',\n",
       " 'seconds',\n",
       " '/i',\n",
       " 'usually',\n",
       " 'prison',\n",
       " \"'bout\",\n",
       " 'dreams',\n",
       " 'standing',\n",
       " 'crime',\n",
       " 'mike',\n",
       " 'reach',\n",
       " 'girlfriend',\n",
       " 'ma',\n",
       " 'papers',\n",
       " 'art',\n",
       " 'beer',\n",
       " 'french',\n",
       " 'present',\n",
       " 'church',\n",
       " 'fat',\n",
       " 'mark',\n",
       " 'detective',\n",
       " 'enjoy',\n",
       " 'hide',\n",
       " 'experience',\n",
       " 'movies',\n",
       " 'truck',\n",
       " 'star',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.zeros((len(model.wv.vocab),1024))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "        word_vecs[vocab2int[word]] = model[word]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary lengths\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary lengths\")\n",
    "print(len(word_vecs))\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs.npy',word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Word2Affect Vector - VAD </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_vad=pd.read_excel('Warriner, Kuperman, Brysbaert - 2013 BRM-ANEW expanded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>D.Rat.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.64</td>\n",
       "      <td>21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>11</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  A.SD.Sum  \\\n",
       "1     aardvark        6.26      2.21         19        2.41      1.40   \n",
       "2      abalone        5.30      1.59         20        2.65      1.90   \n",
       "3      abandon        2.84      1.54         19        3.73      2.43   \n",
       "4  abandonment        2.63      1.74         19        4.95      2.64   \n",
       "5        abbey        5.85      1.69         20        2.20      1.70   \n",
       "\n",
       "   A.Rat.Sum  D.Mean.Sum  D.SD.Sum  D.Rat.Sum   ...     A.Rat.L  A.Mean.H  \\\n",
       "1         22        4.27      1.75         15   ...          11      2.55   \n",
       "2         20        4.95      1.79         22   ...          12      2.38   \n",
       "3         22        3.32      2.50         22   ...          11      3.82   \n",
       "4         21        2.64      1.81         28   ...          14      5.29   \n",
       "5         20        5.00      2.02         25   ...           9      2.55   \n",
       "\n",
       "   A.SD.H  A.Rat.H  D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "1    1.29       11      4.12    1.64        8      4.43    1.99        7  \n",
       "2    1.92        8      5.55    2.21       11      4.36    1.03       11  \n",
       "3    2.14       11      2.77    2.09       13      4.11    2.93        9  \n",
       "4    2.63        7      2.31    1.45       16      3.08    2.19       12  \n",
       "5    1.92       11      4.83    2.18       18      5.43    1.62        7  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vad.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_wordvecs=[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    list_wordvecs.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'radical',\n",
       " 'sandwich',\n",
       " 'embarrassed',\n",
       " 'electronic',\n",
       " 'temper',\n",
       " 'clutch',\n",
       " 'spirit',\n",
       " 'efficiency',\n",
       " 'square',\n",
       " 'exhibition',\n",
       " 'chunk',\n",
       " 'tattoo',\n",
       " 'favorite',\n",
       " 'journalist',\n",
       " 'enemy',\n",
       " 'surfer',\n",
       " 'elite',\n",
       " 'speck',\n",
       " 'simulator',\n",
       " 'pope',\n",
       " 'poet',\n",
       " 'catch',\n",
       " 'pal',\n",
       " 'common',\n",
       " 'static',\n",
       " 'claim',\n",
       " 'parrot',\n",
       " 'paradise',\n",
       " 'irregular',\n",
       " 'sheet',\n",
       " 'steer',\n",
       " 'germs',\n",
       " 'tale',\n",
       " 'state',\n",
       " 'assume',\n",
       " 'punk',\n",
       " 'ruling',\n",
       " 'paranoia',\n",
       " 'unfortunate',\n",
       " 'cage',\n",
       " 'ability',\n",
       " 'bush',\n",
       " 'father',\n",
       " 'cylinder',\n",
       " 'natural',\n",
       " 'manpower',\n",
       " 'dining',\n",
       " 'owe',\n",
       " 'fraud',\n",
       " 'upset',\n",
       " 'embarrassing',\n",
       " 'political',\n",
       " 'pin',\n",
       " 'partnership',\n",
       " 'neutralize',\n",
       " 'impressed',\n",
       " 'wormhole',\n",
       " 'rock',\n",
       " 'electricity',\n",
       " 'crazy',\n",
       " 'bikini',\n",
       " 'miserable',\n",
       " 'formal',\n",
       " 'food',\n",
       " 'competitive',\n",
       " 'sausage',\n",
       " 'momma',\n",
       " 'follow',\n",
       " 'tea',\n",
       " 'incredible',\n",
       " 'blue',\n",
       " 'bishop',\n",
       " 'ape',\n",
       " 'sax',\n",
       " 'powwow',\n",
       " 'interview',\n",
       " 'caller',\n",
       " 'mind',\n",
       " 'certified',\n",
       " 'flood',\n",
       " 'sap',\n",
       " 'web',\n",
       " 'demise',\n",
       " 'stain',\n",
       " 'rider',\n",
       " 'technique',\n",
       " 'pervert',\n",
       " 'parking',\n",
       " 'meaningless',\n",
       " 'walk',\n",
       " 'acceptable',\n",
       " 'horse',\n",
       " 'model',\n",
       " 'moonlight',\n",
       " 'altitude',\n",
       " 'convince',\n",
       " 'accuse',\n",
       " 'throw',\n",
       " 'barbecue',\n",
       " 'poem',\n",
       " 'actual',\n",
       " 'chart',\n",
       " 'unlock',\n",
       " 'teeth',\n",
       " 'pink',\n",
       " 'mute',\n",
       " 'improve',\n",
       " 'shade',\n",
       " 'saxophone',\n",
       " 'ledge',\n",
       " 'pudding',\n",
       " 'lesbian',\n",
       " 'circulation',\n",
       " 'gamble',\n",
       " 'housewife',\n",
       " 'bout',\n",
       " 'give',\n",
       " 'restricted',\n",
       " 'explorer',\n",
       " 'committee',\n",
       " 'series',\n",
       " 'district',\n",
       " 'language',\n",
       " 'beard',\n",
       " 'strap',\n",
       " 'deceased',\n",
       " 'react',\n",
       " 'thirteen',\n",
       " 'jelly',\n",
       " 'handful',\n",
       " 'injustice',\n",
       " 'bombing',\n",
       " 'dart',\n",
       " 'sensation',\n",
       " 'additional',\n",
       " 'sub',\n",
       " 'oversight',\n",
       " 'rainy',\n",
       " 'get',\n",
       " 'dull',\n",
       " 'mild',\n",
       " 'environmental',\n",
       " 'tequila',\n",
       " 'fight',\n",
       " 'association',\n",
       " 'skip',\n",
       " 'struggle',\n",
       " 'facility',\n",
       " 'forbid',\n",
       " 'smile',\n",
       " 'tray',\n",
       " 'chronic',\n",
       " 'latch',\n",
       " 'rookie',\n",
       " 'multiple',\n",
       " 'pattern',\n",
       " 'disaster',\n",
       " 'annoy',\n",
       " 'nightfall',\n",
       " 'usual',\n",
       " 'guard',\n",
       " 'bounce',\n",
       " 'construct',\n",
       " 'tide',\n",
       " 'plug',\n",
       " 'trapper',\n",
       " 'patience',\n",
       " 'containment',\n",
       " 'deposit',\n",
       " 'network',\n",
       " 'fly',\n",
       " 'classical',\n",
       " 'stitch',\n",
       " 'fit',\n",
       " 'confidentiality',\n",
       " 'infamous',\n",
       " 'evolution',\n",
       " 'instant',\n",
       " 'stake',\n",
       " 'prosecution',\n",
       " 'prominent',\n",
       " 'stenographer',\n",
       " 'response',\n",
       " 'hobby',\n",
       " 'cruel',\n",
       " 'born',\n",
       " 'event',\n",
       " 'lazy',\n",
       " 'stream',\n",
       " 'pint',\n",
       " 'download',\n",
       " 'remove',\n",
       " 'property',\n",
       " 'worthy',\n",
       " 'sign',\n",
       " 'apartment',\n",
       " 'continue',\n",
       " 'gross',\n",
       " 'comply',\n",
       " 'bondage',\n",
       " 'sector',\n",
       " 'canvas',\n",
       " 'mold',\n",
       " 'physics',\n",
       " 'exchange',\n",
       " 'secrecy',\n",
       " 'morbid',\n",
       " 'approach',\n",
       " 'establish',\n",
       " 'architect',\n",
       " 'space',\n",
       " 'lieutenant',\n",
       " 'internal',\n",
       " 'hungry',\n",
       " 'democratic',\n",
       " 'forge',\n",
       " 'swear',\n",
       " 'drunken',\n",
       " 'aisle',\n",
       " 'bottle',\n",
       " 'boner',\n",
       " 'ambitious',\n",
       " 'rub',\n",
       " 'ridiculous',\n",
       " 'past',\n",
       " 'imagine',\n",
       " 'scene',\n",
       " 'appeal',\n",
       " 'undo',\n",
       " 'cashier',\n",
       " 'team',\n",
       " 'flux',\n",
       " 'literary',\n",
       " 'defendant',\n",
       " 'bummer',\n",
       " 'slight',\n",
       " 'slide',\n",
       " 'superior',\n",
       " 'topside',\n",
       " 'global',\n",
       " 'answer',\n",
       " 'glum',\n",
       " 'shave',\n",
       " 'freak',\n",
       " 'source',\n",
       " 'real',\n",
       " 'crap',\n",
       " 'confusing',\n",
       " 'wound',\n",
       " 'scrap',\n",
       " 'moment',\n",
       " 'saddle',\n",
       " 'vulgar',\n",
       " 'bucket',\n",
       " 'hairy',\n",
       " 'bare',\n",
       " 'apply',\n",
       " 'noon',\n",
       " 'north',\n",
       " 'knee',\n",
       " 'silver',\n",
       " 'steel',\n",
       " 'quadrant',\n",
       " 'typewriter',\n",
       " 'fireworks',\n",
       " 'inheritance',\n",
       " 'despair',\n",
       " 'minute',\n",
       " 'love',\n",
       " 'administration',\n",
       " 'indication',\n",
       " 'rescue',\n",
       " 'relationship',\n",
       " 'word',\n",
       " 'parallel',\n",
       " 'buster',\n",
       " 'know',\n",
       " 'vicious',\n",
       " 'loosen',\n",
       " 'comprehend',\n",
       " 'unable',\n",
       " 'call',\n",
       " 'flow',\n",
       " 'fox',\n",
       " 'buttercup',\n",
       " 'ignorance',\n",
       " 'wake',\n",
       " 'comrade',\n",
       " 'cute',\n",
       " 'win',\n",
       " 'rocket',\n",
       " 'inherit',\n",
       " 'conditioning',\n",
       " 'ram',\n",
       " 'sweet',\n",
       " 'workshop',\n",
       " 'necklace',\n",
       " 'puff',\n",
       " 'bodega',\n",
       " 'riddle',\n",
       " 'crawl',\n",
       " 'regular',\n",
       " 'pupil',\n",
       " 'concrete',\n",
       " 'worker',\n",
       " 'accurate',\n",
       " 'fret',\n",
       " 'lamp',\n",
       " 'protocol',\n",
       " 'bookkeeper',\n",
       " 'analysis',\n",
       " 'designated',\n",
       " 'hospital',\n",
       " 'craft',\n",
       " 'survive',\n",
       " 'surgeon',\n",
       " 'official',\n",
       " 'encounter',\n",
       " 'tackle',\n",
       " 'peep',\n",
       " 'substance',\n",
       " 'famous',\n",
       " 'cathedral',\n",
       " 'breast',\n",
       " 'wildlife',\n",
       " 'chat',\n",
       " 'definition',\n",
       " 'grass',\n",
       " 'jet',\n",
       " 'block',\n",
       " 'lean',\n",
       " 'whistle',\n",
       " 'battlefield',\n",
       " 'pair',\n",
       " 'daylight',\n",
       " 'acute',\n",
       " 'read',\n",
       " 'hiss',\n",
       " 'student',\n",
       " 'globe',\n",
       " 'rack',\n",
       " 'play',\n",
       " 'shut',\n",
       " 'imitate',\n",
       " 'die',\n",
       " 'jewelry',\n",
       " 'temptation',\n",
       " 'mouthful',\n",
       " 'continental',\n",
       " 'agree',\n",
       " 'bus',\n",
       " 'definitive',\n",
       " 'disagree',\n",
       " 'commit',\n",
       " 'defy',\n",
       " 'raid',\n",
       " 'quince',\n",
       " 'knowledge',\n",
       " 'dive',\n",
       " 'goodness',\n",
       " 'absence',\n",
       " 'big',\n",
       " 'commodity',\n",
       " 'trim',\n",
       " 'qualify',\n",
       " 'price',\n",
       " 'hat',\n",
       " 'prefer',\n",
       " 'ropes',\n",
       " 'curve',\n",
       " 'taste',\n",
       " 'burrito',\n",
       " 'yacht',\n",
       " 'dish',\n",
       " 'yank',\n",
       " 'tooth',\n",
       " 'sword',\n",
       " 'crash',\n",
       " 'gateway',\n",
       " 'key',\n",
       " 'withdraw',\n",
       " 'hall',\n",
       " 'destination',\n",
       " 'extent',\n",
       " 'boast',\n",
       " 'body',\n",
       " 'conscience',\n",
       " 'learn',\n",
       " 'homosexual',\n",
       " 'painter',\n",
       " 'community',\n",
       " 'delight',\n",
       " 'restrain',\n",
       " 'drift',\n",
       " 'seed',\n",
       " 'blend',\n",
       " 'rape',\n",
       " 'shoulder',\n",
       " 'sex',\n",
       " 'purse',\n",
       " 'consequence',\n",
       " 'harm',\n",
       " 'son',\n",
       " 'unharmed',\n",
       " 'palace',\n",
       " 'fetish',\n",
       " 'represent',\n",
       " 'invisible',\n",
       " 'sequel',\n",
       " 'mountain',\n",
       " 'bypass',\n",
       " 'dubious',\n",
       " 'occasion',\n",
       " 'hatred',\n",
       " 'herring',\n",
       " 'seem',\n",
       " 'institution',\n",
       " 'punch',\n",
       " 'reception',\n",
       " 'booth',\n",
       " 'hug',\n",
       " 'sweat',\n",
       " 'privy',\n",
       " 'effective',\n",
       " 'abduction',\n",
       " 'homicide',\n",
       " 'error',\n",
       " 'harp',\n",
       " 'meadow',\n",
       " 'weekend',\n",
       " 'assembly',\n",
       " 'grid',\n",
       " 'capable',\n",
       " 'retreat',\n",
       " 'lobe',\n",
       " 'eye',\n",
       " 'mop',\n",
       " 'resign',\n",
       " 'admit',\n",
       " 'prayer',\n",
       " 'mental',\n",
       " 'act',\n",
       " 'mailbox',\n",
       " 'loose',\n",
       " 'penalty',\n",
       " 'fade',\n",
       " 'cabbie',\n",
       " 'negligence',\n",
       " 'wounds',\n",
       " 'canary',\n",
       " 'demonstrate',\n",
       " 'copy',\n",
       " 'observe',\n",
       " 'amused',\n",
       " 'yarn',\n",
       " 'papa',\n",
       " 'tan',\n",
       " 'sweater',\n",
       " 'ghost',\n",
       " 'selfish',\n",
       " 'cooking',\n",
       " 'bone',\n",
       " 'ladyship',\n",
       " 'become',\n",
       " 'earthquake',\n",
       " 'wide',\n",
       " 'pretty',\n",
       " 'alimony',\n",
       " 'tension',\n",
       " 'portable',\n",
       " 'summer',\n",
       " 'perk',\n",
       " 'postal',\n",
       " 'beer',\n",
       " 'kitten',\n",
       " 'newt',\n",
       " 'lively',\n",
       " 'select',\n",
       " 'wheelchair',\n",
       " 'maintenance',\n",
       " 'itch',\n",
       " 'sky',\n",
       " 'belt',\n",
       " 'erase',\n",
       " 'flaw',\n",
       " 'bless',\n",
       " 'empire',\n",
       " 'crew',\n",
       " 'skirt',\n",
       " 'career',\n",
       " 'behavior',\n",
       " 'recognize',\n",
       " 'involve',\n",
       " 'glorious',\n",
       " 'sissy',\n",
       " 'tone',\n",
       " 'psychiatrist',\n",
       " 'blast',\n",
       " 'chap',\n",
       " 'antique',\n",
       " 'ooze',\n",
       " 'hacker',\n",
       " 'income',\n",
       " 'vent',\n",
       " 'fence',\n",
       " 'magician',\n",
       " 'jerk',\n",
       " 'tender',\n",
       " 'seat',\n",
       " 'thought',\n",
       " 'disgust',\n",
       " 'prisoner',\n",
       " 'gallon',\n",
       " 'sour',\n",
       " 'newspaper',\n",
       " 'impress',\n",
       " 'grudge',\n",
       " 'schizophrenic',\n",
       " 'respect',\n",
       " 'southern',\n",
       " 'declaration',\n",
       " 'psychopath',\n",
       " 'put',\n",
       " 'relax',\n",
       " 'quest',\n",
       " 'spook',\n",
       " 'sequence',\n",
       " 'deal',\n",
       " 'art',\n",
       " 'sensitive',\n",
       " 'logical',\n",
       " 'possible',\n",
       " 'keeper',\n",
       " 'corporate',\n",
       " 'cabinet',\n",
       " 'attitude',\n",
       " 'piss',\n",
       " 'ideal',\n",
       " 'fame',\n",
       " 'heck',\n",
       " 'blessing',\n",
       " 'posse',\n",
       " 'screen',\n",
       " 'blackjack',\n",
       " 'foul',\n",
       " 'junction',\n",
       " 'move',\n",
       " 'pension',\n",
       " 'sentiment',\n",
       " 'circuit',\n",
       " 'succeed',\n",
       " 'fire',\n",
       " 'deviant',\n",
       " 'season',\n",
       " 'west',\n",
       " 'sup',\n",
       " 'deed',\n",
       " 'sentimental',\n",
       " 'laser',\n",
       " 'steps',\n",
       " 'nest',\n",
       " 'labor',\n",
       " 'man',\n",
       " 'gambling',\n",
       " 'sale',\n",
       " 'screwball',\n",
       " 'crocodile',\n",
       " 'nature',\n",
       " 'veteran',\n",
       " 'awake',\n",
       " 'truth',\n",
       " 'startled',\n",
       " 'coil',\n",
       " 'effect',\n",
       " 'specialty',\n",
       " 'second',\n",
       " 'clip',\n",
       " 'lawyer',\n",
       " 'gangster',\n",
       " 'wear',\n",
       " 'ancient',\n",
       " 'replace',\n",
       " 'profession',\n",
       " 'breach',\n",
       " 'van',\n",
       " 'distance',\n",
       " 'gum',\n",
       " 'prime',\n",
       " 'pier',\n",
       " 'polar',\n",
       " 'foundation',\n",
       " 'decent',\n",
       " 'review',\n",
       " 'exhibit',\n",
       " 'credit',\n",
       " 'wit',\n",
       " 'imperial',\n",
       " 'senor',\n",
       " 'fruitcake',\n",
       " 'cigar',\n",
       " 'champion',\n",
       " 'snap',\n",
       " 'address',\n",
       " 'pry',\n",
       " 'difficulty',\n",
       " 'salad',\n",
       " 'aid',\n",
       " 'ambition',\n",
       " 'vengeance',\n",
       " 'laddie',\n",
       " 'induce',\n",
       " 'meatloaf',\n",
       " 'skin',\n",
       " 'porch',\n",
       " 'thirsty',\n",
       " 'purpose',\n",
       " 'applicant',\n",
       " 'artery',\n",
       " 'bacon',\n",
       " 'mail',\n",
       " 'depression',\n",
       " 'lady',\n",
       " 'translate',\n",
       " 'hot',\n",
       " 'path',\n",
       " 'trifle',\n",
       " 'sticky',\n",
       " 'servant',\n",
       " 'physical',\n",
       " 'risk',\n",
       " 'congratulations',\n",
       " 'hesitate',\n",
       " 'setup',\n",
       " 'conceive',\n",
       " 'practice',\n",
       " 'valium',\n",
       " 'spare',\n",
       " 'bourbon',\n",
       " 'lever',\n",
       " 'lawsuit',\n",
       " 'modest',\n",
       " 'initiation',\n",
       " 'slavery',\n",
       " 'army',\n",
       " 'bull',\n",
       " 'example',\n",
       " 'sea',\n",
       " 'trash',\n",
       " 'roast',\n",
       " 'raise',\n",
       " 'doll',\n",
       " 'fishing',\n",
       " 'pull',\n",
       " 'mask',\n",
       " 'player',\n",
       " 'republican',\n",
       " 'haul',\n",
       " 'tomb',\n",
       " 'nightcap',\n",
       " 'intellect',\n",
       " 'homicidal',\n",
       " 'jackass',\n",
       " 'camper',\n",
       " 'impose',\n",
       " 'anonymous',\n",
       " 'book',\n",
       " 'zip',\n",
       " 'primer',\n",
       " 'cellar',\n",
       " 'exception',\n",
       " 'commissioner',\n",
       " 'gun',\n",
       " 'canal',\n",
       " 'shortage',\n",
       " 'penance',\n",
       " 'headquarters',\n",
       " 'transport',\n",
       " 'bankrupt',\n",
       " 'frightened',\n",
       " 'round',\n",
       " 'vivid',\n",
       " 'transporter',\n",
       " 'purple',\n",
       " 'invest',\n",
       " 'considerable',\n",
       " 'lens',\n",
       " 'constitution',\n",
       " 'campus',\n",
       " 'character',\n",
       " 'wreck',\n",
       " 'barrel',\n",
       " 'dispatch',\n",
       " 'historical',\n",
       " 'habit',\n",
       " 'develop',\n",
       " 'genius',\n",
       " 'retire',\n",
       " 'malfunction',\n",
       " 'daughter',\n",
       " 'briefcase',\n",
       " 'illness',\n",
       " 'crappy',\n",
       " 'stool',\n",
       " 'infrared',\n",
       " 'dig',\n",
       " 'link',\n",
       " 'wedding',\n",
       " 'times',\n",
       " 'radiator',\n",
       " 'sock',\n",
       " 'mister',\n",
       " 'reform',\n",
       " 'relation',\n",
       " 'liar',\n",
       " 'elect',\n",
       " 'doctor',\n",
       " 'hitter',\n",
       " 'organization',\n",
       " 'magnetic',\n",
       " 'overrated',\n",
       " 'recommend',\n",
       " 'wander',\n",
       " 'fin',\n",
       " 'noir',\n",
       " 'piano',\n",
       " 'recon',\n",
       " 'particle',\n",
       " 'chicken',\n",
       " 'separate',\n",
       " 'recommendation',\n",
       " 'shit',\n",
       " 'killer',\n",
       " 'kill',\n",
       " 'marvelous',\n",
       " 'armor',\n",
       " 'would',\n",
       " 'cook',\n",
       " 'athlete',\n",
       " 'muddy',\n",
       " 'surface',\n",
       " 'laugh',\n",
       " 'patriot',\n",
       " 'miracle',\n",
       " 'eleven',\n",
       " 'story',\n",
       " 'hooker',\n",
       " 'sexual',\n",
       " 'governor',\n",
       " 'delicate',\n",
       " 'deaf',\n",
       " 'silence',\n",
       " 'freaky',\n",
       " 'spaghetti',\n",
       " 'strategic',\n",
       " 'mercury',\n",
       " 'asset',\n",
       " 'eternity',\n",
       " 'yield',\n",
       " 'protected',\n",
       " 'spade',\n",
       " 'pot',\n",
       " 'stripe',\n",
       " 'exit',\n",
       " 'initial',\n",
       " 'catholic',\n",
       " 'oral',\n",
       " 'wallpaper',\n",
       " 'one',\n",
       " 'blasphemy',\n",
       " 'law',\n",
       " 'heel',\n",
       " 'conservative',\n",
       " 'ammunition',\n",
       " 'identification',\n",
       " 'resort',\n",
       " 'want',\n",
       " 'pause',\n",
       " 'cripple',\n",
       " 'eccentric',\n",
       " 'shine',\n",
       " 'arrogant',\n",
       " 'cream',\n",
       " 'musician',\n",
       " 'confusion',\n",
       " 'cannon',\n",
       " 'squash',\n",
       " 'luck',\n",
       " 'salmon',\n",
       " 'politics',\n",
       " 'specialist',\n",
       " 'divide',\n",
       " 'accept',\n",
       " 'hole',\n",
       " 'accusation',\n",
       " 'supposed',\n",
       " 'mouse',\n",
       " 'effort',\n",
       " 'sum',\n",
       " 'nobleman',\n",
       " 'infection',\n",
       " 'antiques',\n",
       " 'fluid',\n",
       " 'daytime',\n",
       " 'fucking',\n",
       " 'wooden',\n",
       " 'rhyme',\n",
       " 'agony',\n",
       " 'expert',\n",
       " 'replacement',\n",
       " 'puffy',\n",
       " 'overcome',\n",
       " 'stray',\n",
       " 'persuade',\n",
       " 'transmit',\n",
       " 'immediate',\n",
       " 'cow',\n",
       " 'chew',\n",
       " 'relay',\n",
       " 'proposal',\n",
       " 'contagious',\n",
       " 'fertilizer',\n",
       " 'fantasy',\n",
       " 'madness',\n",
       " 'marquis',\n",
       " 'able',\n",
       " 'importance',\n",
       " 'office',\n",
       " 'other',\n",
       " 'observation',\n",
       " 'surgical',\n",
       " 'unit',\n",
       " 'cable',\n",
       " 'alphabet',\n",
       " 'trespass',\n",
       " 'superhero',\n",
       " 'lover',\n",
       " 'human',\n",
       " 'helpless',\n",
       " 'shatter',\n",
       " 'pyramid',\n",
       " 'psychological',\n",
       " 'buyer',\n",
       " 'ignorant',\n",
       " 'soul',\n",
       " 'dash',\n",
       " 'mill',\n",
       " 'conquer',\n",
       " 'ace',\n",
       " 'swallow',\n",
       " 'immense',\n",
       " 'prevent',\n",
       " 'speech',\n",
       " 'animal',\n",
       " 'paint',\n",
       " 'magazine',\n",
       " 'sunlight',\n",
       " 'kin',\n",
       " 'groovy',\n",
       " 'movement',\n",
       " 'gasket',\n",
       " 'obsessed',\n",
       " 'tolerate',\n",
       " 'apartments',\n",
       " 'bloke',\n",
       " 'arrange',\n",
       " 'squid',\n",
       " 'kid',\n",
       " 'anticipate',\n",
       " 'stepmother',\n",
       " 'uranium',\n",
       " 'spice',\n",
       " 'farm',\n",
       " 'production',\n",
       " 'eight',\n",
       " 'cinnamon',\n",
       " 'opinion',\n",
       " 'award',\n",
       " 'counterfeit',\n",
       " 'specific',\n",
       " 'great',\n",
       " 'pictures',\n",
       " 'sewing',\n",
       " 'foreign',\n",
       " 'mall',\n",
       " 'valuable',\n",
       " 'phony',\n",
       " 'limo',\n",
       " 'humpback',\n",
       " 'escape',\n",
       " 'passion',\n",
       " 'bunk',\n",
       " 'janitor',\n",
       " 'gilded',\n",
       " 'disco',\n",
       " 'birthday',\n",
       " 'sunrise',\n",
       " 'horn',\n",
       " 'unstable',\n",
       " 'hang',\n",
       " 'macaroni',\n",
       " 'news',\n",
       " 'recognition',\n",
       " 'terrible',\n",
       " 'exploit',\n",
       " 'dimension',\n",
       " 'thin',\n",
       " 'anxiety',\n",
       " 'violet',\n",
       " 'firepower',\n",
       " 'scrambled',\n",
       " 'garbage',\n",
       " 'mama',\n",
       " 'cloth',\n",
       " 'precision',\n",
       " 'wicked',\n",
       " 'rubbish',\n",
       " 'female',\n",
       " 'devotion',\n",
       " 'advertising',\n",
       " 'doom',\n",
       " 'barf',\n",
       " 'species',\n",
       " 'sensual',\n",
       " 'locate',\n",
       " 'guardian',\n",
       " 'godfather',\n",
       " 'baggage',\n",
       " 'oyster',\n",
       " 'style',\n",
       " 'pop',\n",
       " 'ballet',\n",
       " 'coup',\n",
       " 'discovery',\n",
       " 'minimal',\n",
       " 'context',\n",
       " 'gear',\n",
       " 'glory',\n",
       " 'intuition',\n",
       " 'nudity',\n",
       " 'league',\n",
       " 'divine',\n",
       " 'moral',\n",
       " 'tell',\n",
       " 'thick',\n",
       " 'define',\n",
       " 'poetry',\n",
       " 'happy',\n",
       " 'curtains',\n",
       " 'sonar',\n",
       " 'collar',\n",
       " 'scent',\n",
       " 'terrorism',\n",
       " 'stick',\n",
       " 'cash',\n",
       " 'nun',\n",
       " 'flare',\n",
       " 'footage',\n",
       " 'name',\n",
       " 'greeting',\n",
       " 'criticize',\n",
       " 'surprise',\n",
       " 'traffic',\n",
       " 'input',\n",
       " 'threat',\n",
       " 'flesh',\n",
       " 'unemployment',\n",
       " 'due',\n",
       " 'bug',\n",
       " 'northeast',\n",
       " 'peel',\n",
       " 'disturbing',\n",
       " 'store',\n",
       " 'casket',\n",
       " 'children',\n",
       " 'scrub',\n",
       " 'interrogation',\n",
       " 'sorority',\n",
       " 'study',\n",
       " 'undercover',\n",
       " 'industry',\n",
       " 'jar',\n",
       " 'wager',\n",
       " 'promote',\n",
       " 'plane',\n",
       " 'twist',\n",
       " 'compromise',\n",
       " 'panic',\n",
       " 'passenger',\n",
       " 'need',\n",
       " 'reunion',\n",
       " 'volume',\n",
       " 'robin',\n",
       " 'engine',\n",
       " 'garden',\n",
       " 'carpet',\n",
       " 'dose',\n",
       " 'unlimited',\n",
       " 'probability',\n",
       " 'huge',\n",
       " 'lesson',\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list3 = set(list_wordvecs) & set(df_vad[\"Word\"])\n",
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5516"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12001"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_vad = set(df_vad['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word_vecs_vad = np.zeros((len(model.wv.vocab),1027))\n",
    "count_vad=0\n",
    "count_neutral=0\n",
    "for i,word in enumerate(model.wv.index2word):   \n",
    "    if word in set(list_vad):\n",
    "        #print(word)\n",
    "        count_vad=count_vad+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=df_vad.loc[df_vad[\"Word\"] == word, 'V.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1025]=df_vad.loc[df_vad[\"Word\"] == word, 'A.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1026]=df_vad.loc[df_vad[\"Word\"] == word, 'D.Mean.Sum'].iloc[0]\n",
    "        #print(word_vecs_vad[i])\n",
    "    else:\n",
    "        #print(\"out\")\n",
    "        count_neutral=count_neutral+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=5\n",
    "        word_vecs_vad[vocab2int[word]][1025]=1\n",
    "        word_vecs_vad[vocab2int[word]][1026]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5516\n",
      "6485\n"
     ]
    }
   ],
   "source": [
    "print(count_vad)\n",
    "print(count_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352    6.05\n",
       "Name: V.Mean.Sum, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val =df_vad[df_vad['Word'] == \"mailbox\"][\"V.Mean.Sum\"]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('word_Vecs_VAD.npy',word_vecs_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - counterfitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './w2v_counterfit_append_affect.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-808b70dd75fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load Google's pre-trained Word2Vec model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_counterfit_affect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./w2v_counterfit_append_affect.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './w2v_counterfit_append_affect.bin'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_counterfit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_counterfit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.068298,  0.01397 , -0.02441 ,  0.054046,  0.033629, -0.01007 ,\n",
       "       -0.031107,  0.040267, -0.024809, -0.023129,  0.035075, -0.061981,\n",
       "        0.094889,  0.038437, -0.018818,  0.039997,  0.077868,  0.027647,\n",
       "       -0.0072  , -0.000698, -0.02708 ,  0.056055, -0.053605,  0.001178,\n",
       "        0.081652,  0.042793, -0.091373,  0.02682 , -0.010707, -0.034362,\n",
       "       -0.027781, -0.004621,  0.085808, -0.004652,  0.075245,  0.084456,\n",
       "       -0.050868, -0.044668, -0.008077,  0.009224,  0.096167,  0.001957,\n",
       "        0.03171 ,  0.026655, -0.043085,  0.038686,  0.014165,  0.06862 ,\n",
       "        0.050164, -0.005307,  0.145519,  0.081177, -0.056621,  0.014355,\n",
       "       -0.044101, -0.001805, -0.031732,  0.003166,  0.096613, -0.089282,\n",
       "       -0.090452,  0.046647, -0.198193, -0.04675 , -0.009436,  0.075172,\n",
       "       -0.052017, -0.028137, -0.078767,  0.027518,  0.006175, -0.017348,\n",
       "        0.179998, -0.017671, -0.091829, -0.055601,  0.006504,  0.079339,\n",
       "       -0.006983, -0.022379, -0.025881,  0.029811,  0.013226,  0.115053,\n",
       "        0.032207, -0.04349 , -0.051668,  0.027526, -0.046608,  0.115959,\n",
       "        0.057365,  0.090153, -0.064874, -0.027923,  0.029423, -0.092529,\n",
       "       -0.013401,  0.080153, -0.043774, -0.010028,  0.028602,  0.02152 ,\n",
       "       -0.044785, -0.006796, -0.034181, -0.010616, -0.034523, -0.019031,\n",
       "       -0.013247,  0.007484,  0.055436,  0.052278, -0.126453,  0.026781,\n",
       "        0.071985,  0.024007,  0.031252, -0.067289, -0.042609, -0.02263 ,\n",
       "        0.014178,  0.011003,  0.022296,  0.093423, -0.076267, -0.028193,\n",
       "        0.012271, -0.096434, -0.023858,  0.004527, -0.058173, -0.046978,\n",
       "       -0.051532, -0.016989, -0.061828, -0.07654 ,  0.106001,  0.10083 ,\n",
       "        0.023096,  0.046153, -0.052389,  0.065611,  0.096181, -0.014762,\n",
       "       -0.015398, -0.044217, -0.009704, -0.045321,  0.063446, -0.008449,\n",
       "       -0.03746 ,  0.159556, -0.063952,  0.10805 , -0.040134,  0.052322,\n",
       "       -0.005815, -0.000742, -0.026841,  0.003803,  0.016281,  0.079311,\n",
       "        0.060642,  0.070299,  0.133207, -0.013334, -0.094392, -0.039325,\n",
       "       -0.021152, -0.036935, -0.092639,  0.068085,  0.053983, -0.035672,\n",
       "        0.004502,  0.129921,  0.030324, -0.064907, -0.01776 ,  0.032938,\n",
       "       -0.074088, -0.136206, -0.037866, -0.070047, -0.119126, -0.030005,\n",
       "       -0.081447,  0.015276,  0.01613 , -0.029709, -0.027911,  0.085703,\n",
       "        0.044309, -0.107859,  0.015405, -0.132433, -0.045625, -0.063961,\n",
       "       -0.040616, -0.106   ,  0.011839,  0.021633,  0.027181,  0.024199,\n",
       "       -0.02978 , -0.083825, -0.022189, -0.048264,  0.016572,  0.112918,\n",
       "       -0.025803,  0.053843, -0.041338,  0.051484, -0.003051, -0.04204 ,\n",
       "        0.113067,  0.036121, -0.020523,  0.023554,  0.002181, -0.057482,\n",
       "       -0.08471 , -0.044766,  0.006952,  0.005777, -0.049501, -0.044739,\n",
       "        0.020132,  0.041329, -0.012819, -0.077212,  0.096833, -0.022325,\n",
       "        0.015486,  0.007067, -0.14609 , -0.078302, -0.014663,  0.002906,\n",
       "        0.005719,  0.075205,  0.039659,  0.005798, -0.030796, -0.007436,\n",
       "        0.018907, -0.039181, -0.142236, -0.075287, -0.027013,  0.109294,\n",
       "        0.015772, -0.004132,  0.012398,  0.007734, -0.050636,  0.026038,\n",
       "       -0.004614, -0.101044, -0.010447, -0.026595, -0.031416,  0.007031,\n",
       "        0.043274,  0.009761, -0.00074 , -0.065493, -0.027458, -0.011578,\n",
       "       -0.043572, -0.03095 ,  0.049361,  0.024768,  0.019395, -0.025657,\n",
       "        0.024031, -0.126927, -0.081736,  0.076594,  0.066419,  0.054371,\n",
       "       -0.014845,  0.078717, -0.033811,  0.046969, -0.05406 , -0.01149 ,\n",
       "        0.028577,  0.070959, -0.00128 , -0.018592, -0.072032,  0.002022,\n",
       "        0.014795,  0.069068,  0.021019, -0.0405  , -0.017028, -0.064674,\n",
       "       -0.003139,  0.013415, -0.011306], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_counterfit_affect[\"well\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "word_vecs_counterfit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    if word in model_counterfit_affect.wv.vocab:\n",
    "        word_vecs_counterfit_affect[vocab2int[word]] = model_counterfit_affect[word]\n",
    "    else:\n",
    "        list_word_not_found.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_unknown = np.mean(word_vecs_counterfit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_unknown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_word_not_found:\n",
    "    word_vecs_counterfit_affect[i] = word_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8101, 303)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs_counterfit_affect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('word_Vecs_counterfit_affect.npy',word_vecs_counterfit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - retrofitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_retrofit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_retrofit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.23350e-02, -3.88500e-03, -6.49750e-02,  3.56760e-02,\n",
       "       -3.79590e-02, -2.26900e-03,  8.23700e-03,  4.43840e-02,\n",
       "        2.83620e-02, -4.37600e-02,  5.67000e-03, -9.95770e-02,\n",
       "        8.63900e-02,  4.60080e-02, -2.64400e-02,  4.40500e-03,\n",
       "        1.02670e-02,  5.38950e-02, -4.03780e-02,  9.87700e-03,\n",
       "       -1.51570e-02,  4.20510e-02, -3.27200e-02, -4.11550e-02,\n",
       "        1.38780e-01,  2.17640e-02, -8.32230e-02, -5.89670e-02,\n",
       "       -3.76710e-02, -2.49510e-02, -1.35740e-02,  4.56560e-02,\n",
       "        6.87190e-02, -4.15760e-02,  1.90190e-02,  7.76660e-02,\n",
       "       -3.91100e-02,  1.02170e-02, -2.43200e-03, -3.44060e-02,\n",
       "        1.62309e-01,  9.66700e-03,  4.85220e-02,  1.48780e-02,\n",
       "       -1.43820e-02,  6.88200e-03, -6.29780e-02,  5.76300e-03,\n",
       "        5.62570e-02, -1.36200e-02,  1.05286e-01,  4.24430e-02,\n",
       "       -2.99170e-02,  3.37360e-02,  1.19460e-02,  5.95990e-02,\n",
       "       -5.28090e-02,  3.39970e-02,  1.04224e-01, -3.99220e-02,\n",
       "       -8.26270e-02,  8.65720e-02, -1.31603e-01, -9.60920e-02,\n",
       "       -1.99100e-02,  3.15280e-02, -2.13130e-02,  1.05400e-03,\n",
       "       -1.21629e-01,  1.23040e-02,  1.10550e-02,  1.95210e-02,\n",
       "        1.77765e-01, -3.91810e-02, -6.95640e-02, -2.92770e-02,\n",
       "        4.99680e-02,  1.25507e-01,  4.85040e-02,  2.35210e-02,\n",
       "       -3.44020e-02,  3.82310e-02, -5.77900e-03,  5.75700e-02,\n",
       "        7.47600e-03, -1.29390e-02, -1.43235e-01,  4.85440e-02,\n",
       "       -6.69710e-02,  3.27610e-02,  8.71550e-02,  1.17898e-01,\n",
       "       -8.04350e-02,  2.03950e-02,  8.24950e-02, -1.81700e-02,\n",
       "       -5.05350e-02,  2.46020e-02, -4.66390e-02,  2.51410e-02,\n",
       "        2.78640e-02, -3.22780e-02, -6.77830e-02,  2.05000e-02,\n",
       "       -1.12660e-02, -2.68060e-02, -3.76340e-02, -2.05500e-02,\n",
       "        2.07940e-02,  5.14310e-02,  1.87820e-02,  4.83210e-02,\n",
       "       -6.98370e-02, -1.41580e-02,  3.17920e-02,  4.18620e-02,\n",
       "        5.51990e-02, -2.19910e-02, -1.80590e-02,  1.83780e-02,\n",
       "        1.44100e-03, -2.18150e-02, -6.56300e-03,  9.51770e-02,\n",
       "       -4.47260e-02, -1.49000e-04, -2.62760e-02, -2.97560e-02,\n",
       "        6.68700e-02, -2.43010e-02,  4.77600e-03, -4.95810e-02,\n",
       "       -6.06360e-02, -3.61200e-02, -1.39711e-01, -4.42110e-02,\n",
       "        1.15905e-01,  1.04244e-01,  4.80440e-02,  9.28500e-03,\n",
       "       -2.21940e-02,  9.51280e-02,  8.00830e-02, -2.18480e-02,\n",
       "        5.48820e-02, -1.95630e-02, -3.21440e-02,  1.17570e-02,\n",
       "       -2.71500e-03,  5.51410e-02, -3.72040e-02,  1.09867e-01,\n",
       "       -2.99810e-02,  5.38400e-02, -2.05020e-02,  1.78600e-02,\n",
       "       -1.68950e-02,  2.08630e-02, -1.08970e-02,  3.21570e-02,\n",
       "        8.78380e-02,  1.40484e-01,  1.88510e-02,  9.09000e-02,\n",
       "        1.22892e-01, -1.19820e-02, -8.70400e-02, -3.43210e-02,\n",
       "       -1.93800e-03, -4.41520e-02, -1.09983e-01,  5.42150e-02,\n",
       "        3.39810e-02, -3.60460e-02,  1.96360e-02,  8.10820e-02,\n",
       "       -2.06260e-02, -8.10390e-02,  3.73710e-02,  2.45080e-02,\n",
       "       -3.18570e-02, -9.11560e-02, -9.33580e-02, -7.67050e-02,\n",
       "       -5.87040e-02,  8.21220e-02, -3.80480e-02,  5.60550e-02,\n",
       "        2.75920e-02,  3.05020e-02,  1.28540e-02,  3.69340e-02,\n",
       "        8.27580e-02, -9.79310e-02, -1.68440e-02, -9.16430e-02,\n",
       "       -8.14810e-02, -6.61900e-03,  3.90090e-02, -6.81060e-02,\n",
       "        3.44750e-02,  6.53300e-03,  5.83300e-03,  7.30600e-03,\n",
       "        1.90760e-02, -9.64750e-02, -3.46510e-02, -4.62300e-02,\n",
       "        2.11670e-02,  4.07630e-02, -2.84750e-02,  2.31460e-02,\n",
       "       -4.21440e-02, -1.13120e-02, -2.53450e-02, -3.93650e-02,\n",
       "        1.02872e-01, -2.48100e-03, -3.99310e-02, -7.74400e-03,\n",
       "       -3.52600e-03, -6.49670e-02, -6.61850e-02, -8.23090e-02,\n",
       "        1.98780e-02,  5.93100e-03, -2.32790e-02, -9.60140e-02,\n",
       "        6.97640e-02, -2.87520e-02,  4.76300e-03, -1.22470e-01,\n",
       "        8.86810e-02, -5.91370e-02,  1.05750e-02,  2.48840e-02,\n",
       "       -1.28944e-01, -1.46230e-02,  1.73130e-02, -3.27380e-02,\n",
       "       -5.55830e-02,  7.94720e-02,  8.93170e-02, -4.37700e-03,\n",
       "       -6.04770e-02, -9.04600e-03, -1.26280e-02, -1.79360e-02,\n",
       "       -1.27710e-01, -9.60200e-03, -1.29010e-02,  7.23800e-02,\n",
       "        4.80580e-02, -8.95660e-02,  4.49370e-02, -5.80800e-03,\n",
       "       -3.60920e-02,  2.89270e-02, -2.50700e-03, -1.21058e-01,\n",
       "        3.88800e-02,  1.79070e-02, -2.59000e-02,  3.53600e-02,\n",
       "        2.20000e-05,  1.26016e-01, -8.27400e-03, -1.48720e-02,\n",
       "        1.09660e-02, -1.19170e-02, -1.63020e-02, -7.11760e-02,\n",
       "        5.31760e-02,  4.53180e-02, -4.78360e-02, -6.62720e-02,\n",
       "        5.78510e-02, -1.68187e-01, -6.94980e-02,  3.03440e-02,\n",
       "        9.42100e-02,  7.30580e-02, -1.13400e-03,  4.18600e-02,\n",
       "        4.88000e-03,  1.29690e-02, -5.07840e-02, -3.64300e-02,\n",
       "        3.89070e-02,  7.12970e-02, -1.76920e-02, -6.80100e-02,\n",
       "       -1.02963e-01,  2.39220e-02,  3.75600e-03,  8.64140e-02,\n",
       "        2.08100e-03, -1.24210e-02,  1.22830e-02, -1.07682e-01,\n",
       "       -3.16200e-03,  1.35120e-02, -1.13870e-02], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_retrofit_affect.wv[\"well\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "word_vecs_retrofit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found_retro =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    if word in model_retrofit_affect.wv.vocab:\n",
    "        word_vecs_retrofit_affect[vocab2int[word]] = model_retrofit_affect[word]\n",
    "    else:\n",
    "        list_word_not_found_retro.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found_retro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_unknown_retro = np.mean(word_vecs_retrofit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list_word_not_found_retro:\n",
    "    word_vecs_retrofit_affect[i] = word_unknown_retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('word_Vecs_retrofit_affect.npy',word_vecs_retrofit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'wrong', 'with', 'her', '.', '<EOS>'], ['adopted', '.', 'i', 'should', 'have', 'know', '.', 'of', 'course', '.', 'if', 'it', 'was', 'a', 'snake', ',', 'it', 'would', 'bit', 'me', '!', '<EOS>'], ['this', 'guy', 'killed', 'a', 'mess', 'of', 'people', '.', '<EOS>'], ['yes', ',', 'what', 'about', 'him', '?', '<EOS>'], ['<UNK>', 'is', 'a', 'great', 'honor', ',', 'sir', '.', 'i-', 'i-', '<EOS>']]\n",
      "[[16, 8, 212, 36, 69, 1, 12001], [5491, 1, 4, 117, 20, 28, 1, 17, 194, 1, 52, 11, 30, 10, 2332, 2, 11, 44, 464, 22, 18, 12001], [26, 168, 274, 10, 1091, 17, 125, 1, 12001], [72, 2, 16, 43, 56, 5, 12001], [0, 8, 10, 189, 840, 2, 145, 1, 2234, 2234, 12001]]\n"
     ]
    }
   ],
   "source": [
    "#Add EOS tokens to target data now that the embeddings have been trained\n",
    "def append_eos(answers_text, answers_int):\n",
    "    appended_text = [sequence + [EOS] for sequence in answers_text]\n",
    "    appended_ints = [sequence + [METATOKEN_INDEX] for sequence in answers_int]\n",
    "    return (appended_text, appended_ints)\n",
    "\n",
    "(train_answers, train_answers_int) = append_eos(train_answers, train_answers_int)\n",
    "(valid_answers, valid_answers_int) = append_eos(valid_answers, valid_answers_int)\n",
    "\n",
    "print(train_answers[:5])\n",
    "print(train_answers_int[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat( [tf.fill([batch_size, 1], METATOKEN_INDEX), ending], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_cell(rnn_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "def multi_dropout_cell(rnn_size, keep_prob, num_layers):    \n",
    "    return tf.contrib.rnn.MultiRNNCell( [dropout_cell(rnn_size, keep_prob) for _ in range(num_layers)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Create the encoding layer\n",
    "    \n",
    "    Returns a tuple `(outputs, output_states)` where\n",
    "      outputs is a 2-tuple of vectors of dimensions [sequence_length, rnn_size] for the forward and backward passes\n",
    "      output_states is a 2-tupe of the final hidden states of the forward and backward passes\n",
    "    \n",
    "    \"\"\"\n",
    "    forward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    backward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw = forward_cell,\n",
    "                                                   cell_bw = backward_cell,\n",
    "                                                   sequence_length = sequence_lengths,\n",
    "                                                   inputs = rnn_inputs,\n",
    "                                                    dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(enc_state, enc_outputs, dec_embed_input, dec_embeddings, #Inputs\n",
    "                        attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                        keep_prob, beam_width, #Hypeparameters\n",
    "                        source_lengths, target_lengths, batch_size): \n",
    "   \n",
    "    with tf.variable_scope(\"decoding\", reuse=tf.AUTO_REUSE) as decoding_scope:\n",
    "        dec_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "        init_dec_state_size = batch_size\n",
    "        #TRAINING\n",
    "        train_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size, memory=enc_outputs,\n",
    "                                                         memory_sequence_length=source_lengths)\n",
    "        \n",
    "        train_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, train_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_lengths, time_major=False)\n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(train_cell, helper,\n",
    "                            train_cell.zero_state(init_dec_state_size, tf.float32)\n",
    "                                                        .clone(cell_state=enc_state),\n",
    "                            output_layer = output_layer)\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, scope=decoding_scope)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        #INFERENCE\n",
    "        #Tile inputs\n",
    "        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, beam_width)\n",
    "        enc_outputs = tf.contrib.seq2seq.tile_batch(enc_outputs, beam_width)\n",
    "        tiled_source_lengths = tf.contrib.seq2seq.tile_batch(source_lengths, beam_width)\n",
    "        init_dec_state_size *= beam_width\n",
    "        \n",
    "        infer_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size,memory=enc_outputs,\n",
    "                                                          memory_sequence_length=tiled_source_lengths)\n",
    "        infer_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, infer_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        start_tokens = tf.tile( [METATOKEN_INDEX], [batch_size]) #Not by batch_size*beam_width, strangely\n",
    "        end_token = METATOKEN_INDEX\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = infer_cell,\n",
    "            embedding = dec_embeddings,\n",
    "            start_tokens = start_tokens, \n",
    "            end_token = end_token,\n",
    "            beam_width = beam_width,\n",
    "            initial_state = infer_cell.zero_state(init_dec_state_size, tf.float32).clone(cell_state=enc_state),\n",
    "            output_layer = output_layer\n",
    "        )  \n",
    "        final_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, scope=decoding_scope,\n",
    "                                                                      maximum_iterations=100)\n",
    "        \n",
    "        ids = final_decoder_output.predicted_ids\n",
    "        beams = ids\n",
    "                \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(enc_embed_input, dec_embed_input, dec_embeddings, #Inputs\n",
    "                  source_lengths, target_lengths, batch_size, #Dimensions\n",
    "                  attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                  keep_prob, beam_width): #Hyperparameters\n",
    "    \n",
    "    enc_outputs, enc_states = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, source_lengths)    \n",
    "    concatenated_enc_output = tf.concat(enc_outputs, -1)\n",
    "    init_dec_state = enc_states[0]    \n",
    "    \n",
    "    \n",
    "    logits, beams = decoding_layer(init_dec_state,\n",
    "                            concatenated_enc_output,\n",
    "                            dec_embed_input,\n",
    "                            dec_embeddings,\n",
    "                            attn_size,\n",
    "                            rnn_size, \n",
    "                            num_layers,\n",
    "                            output_layer,\n",
    "                            keep_prob,\n",
    "                            beam_width,\n",
    "                            source_lengths,\n",
    "                            target_lengths, \n",
    "                            batch_size\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_with_meta = 12002\n",
      "METATOKEN_INDEX = 12001\n",
      "wordVecsWithMeta.shape = (12002, 1024)\n",
      "wordVecsWithMeta[METATOKEN_INDEX] = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Settings used by Asghar et al.\n",
    "rnn_size = 1024\n",
    "num_layers = 1\n",
    "embedding_size = 1024     ## 1027 if word2Affect vect - VAD.  .... 303 if word2vec,retrofit or counterfit\n",
    "\n",
    "flag_affect_functions = False # change this flag to false if affect functions are not used\n",
    "attention_size = 256\n",
    "\n",
    "#Training\n",
    "epochs = 50\n",
    "train_batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "keep_probability = 0.75\n",
    "\n",
    "#Inference\n",
    "beam_width = 5\n",
    "\n",
    "#Validation\n",
    "valid_batch_size = 64\n",
    "\n",
    "wordVecs = np.load(\"word_Vecs.npy\").astype(np.float32)\n",
    "#wordVecs = np.load('word_Vecs_VAD.npy').astype(np.float32)\n",
    "metatoken_embedding = np.zeros((1, embedding_size), dtype=wordVecs.dtype)\n",
    "wordVecsWithMeta = np.concatenate( (wordVecs, metatoken_embedding), axis=0 )\n",
    "vocab_size_with_meta = wordVecsWithMeta.shape[0]\n",
    "\n",
    "print(\"vocab_size_with_meta =\", vocab_size_with_meta)\n",
    "print(\"METATOKEN_INDEX =\", METATOKEN_INDEX)\n",
    "print(\"wordVecsWithMeta.shape =\", wordVecsWithMeta.shape)\n",
    "print(\"wordVecsWithMeta[METATOKEN_INDEX] =\", wordVecsWithMeta[METATOKEN_INDEX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#                                      batch_size, time\n",
    "input_data = tf.placeholder(tf.int32, [None,       None], name='input')\n",
    "targets = tf.placeholder(tf.int32,    [None,       None], name='targets')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "#                                          batch_size\n",
    "source_lengths = tf.placeholder(tf.int32, [None], name=\"source_lengths\")\n",
    "target_lengths = tf.placeholder(tf.int32, [None], name=\"target_lengths\")\n",
    "batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "full_embeddings = tf.Variable(wordVecsWithMeta,trainable=False,name=\"Weight\")\n",
    "enc_embed_input = tf.nn.embedding_lookup(full_embeddings, input_data)\n",
    "dec_embed_input = tf.nn.embedding_lookup(full_embeddings, process_decoding_input(targets, batch_size))\n",
    "\n",
    "output_layer = tf.layers.Dense(vocab_size_with_meta,bias_initializer=tf.zeros_initializer(),activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, beams = \\\n",
    "seq2seq_model(enc_embed_input, dec_embed_input, full_embeddings,\n",
    "        source_lengths, target_lengths, batch_size, \n",
    "        attention_size, rnn_size, num_layers, output_layer,\n",
    "        keep_prob, beam_width)\n",
    "\n",
    "# Find the shape of the input data for sequence_loss\n",
    "with tf.name_scope(\"optimization\"): \n",
    "    \n",
    "    if(flag_affect_functions):\n",
    "        enc_emot_embed_input = enc_embed_input[:, :, 1024:1027]\n",
    "        #FIXME: We should be getting the embeddings of the train_logits\n",
    "        target_emot_embed_input = dec_embed_input[:,:,1024:1027]\n",
    "        \n",
    "        \n",
    "    #xent = loss_functions.cross_entropy(train_logits, targets, target_lengths)\n",
    "    \n",
    "    mask = tf.sequence_mask(target_lengths, dtype=tf.float32)\n",
    "    #mask = tf.ones( [batch_size, tf.reduce_max(target_lengths)], dtype=tf.float32)\n",
    "\n",
    "    xent_tokens = tf.contrib.seq2seq.sequence_loss(train_logits, targets, mask,\n",
    "                                                   average_across_timesteps=False,\n",
    "                                                   average_across_batch=False)\n",
    "    xent = tf.reduce_mean(xent_tokens)\n",
    "    \n",
    "    perplexity = tf.contrib.seq2seq.sequence_loss(train_logits, targets, mask,\n",
    "                                                  softmax_loss_function=metrics.perplexity)\n",
    "\n",
    "    cost = xent\n",
    "    \n",
    "    '''--param values are 0.5, 0.4 and 0.5 as per the hyper parameter tuning mentioned in paper for LDMIN,LDMAX,LDAC functions--\n",
    "    '''\n",
    "    #cost = loss_functions.min_affective_dissonance(0.5,train_logits,targets,\n",
    "    #    target_lengths,input_emot_embed_input,target_emot_embed_input)\n",
    "    #cost = loss_functions.max_affective_dissonance(0.4,train_logits,targets,\n",
    "    #    target_lengths,input_emot_embed_input,target_emot_embed_input)\n",
    "    #cost = loss_functions.max_affective_content(0.5,train_logits,targets,\n",
    "    #    target_lengths,input_emot_embed_input,target_emot_embed_input)\n",
    "    \n",
    "\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response(question_int, beams, answer_int = None, best_only=True):\n",
    "    pad_q = METATOKEN_INDEX\n",
    "    print(\"Prompt\")\n",
    "    print(\"  Word Ids: {}\".format([i for i in question_int if i != pad_q]))\n",
    "    print(\"      Text: {}\".format(int_to_text(question_int, questions_int_to_vocab)))\n",
    "    \n",
    "    pad_a = METATOKEN_INDEX\n",
    "    if answer_int is not None:\n",
    "        print(\"Actual Answer\")\n",
    "        print(\"  Word Ids: {}\".format([i for i in answer_int if i != pad_a]))\n",
    "        print(\"      Text: {}\".format(int_to_text(answer_int, answers_int_to_vocab)))\n",
    "\n",
    "    limit = 1 if best_only else beam_width\n",
    "    for i in range(limit):\n",
    "        beam = beams[:, i]\n",
    "        print(\"\\nBeam Answer\", i)\n",
    "        print('  Word Ids: {}'.format([i for i in beam if i != pad_a]))\n",
    "        print('      Text: {}'.format(int_to_text(beam, answers_int_to_vocab)))\n",
    "        \n",
    "def check_response(session, question_int, answer_int=None, best_only=True):\n",
    "    \"\"\"\n",
    "    session - the TensorFlow session\n",
    "    question_int - a list of integers\n",
    "    answer - the actual, correct response (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    two_d_question_int = [question_int]\n",
    "    q_lengths = [len(question_int)]\n",
    "    \n",
    "    [beam_output] = session.run([beams], feed_dict = {input_data: np.array(two_d_question_int, dtype=np.float32),\n",
    "                                                      source_lengths: q_lengths,\n",
    "                                                      keep_prob: 1})\n",
    "    \n",
    "    show_response(question_int, beam_output[0], answer_int, best_only=best_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    pad_int = METATOKEN_INDEX\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_length - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        source_lengths = np.array( [len(sentence) for sentence in questions_batch] )\n",
    "        target_lengths = np.array( [len(sentence) for sentence in answers_batch])\n",
    "        \n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield source_lengths, target_lengths, pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shuffle(source_sequences, target_sequences):\n",
    "    if len(source_sequences) != len(target_sequences):\n",
    "        raise ValueError(\"Cannot shuffle parallel sets with different numbers of sequences\")\n",
    "    indices = np.random.permutation(len(source_sequences))\n",
    "    shuffled_source = [source_sequences[indices[i]] for i in range(len(indices))]\n",
    "    shuffled_target = [target_sequences[indices[i]] for i in range(len(indices))]\n",
    "    \n",
    "    return (shuffled_source, shuffled_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "\n",
    "#VALIDATION\n",
    "validation_check = ((len(train_prompts))//train_batch_size//2)-1 #Check validation loss every half-epoch\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "#Minimum number of epochs before we start checking sample output with beam search\n",
    "min_epochs_before_validation = 1\n",
    "\n",
    "checkpoint = \"./checkpoints/best_model.ckpt\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model parameters\n",
      "Shuffling training data . . .\n",
      "Epoch   1/50 Batch    0/2683 - Loss:  0.566554, Seconds: 236.17\n",
      "Shuffling validation data . . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-b5382edb32f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                     [valid_perp] = sess.run([perplexity],\n\u001b[1;32m     38\u001b[0m                         {input_data: prompts_batch, targets: answers_batch,\n\u001b[0;32m---> 39\u001b[0;31m                         source_lengths: q_lengths, target_lengths: a_lengths, keep_prob: 1})\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mtotal_perp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_perp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mnum_valid_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().save(sess, checkpoint)\n",
    "    print(\"Initialized model parameters\")\n",
    "    for epoch_i in range(1, epochs+1):        \n",
    "        print(\"Shuffling training data . . .\")\n",
    "        (train_prompts_int, train_answers_int) = parallel_shuffle(train_prompts_int, train_answers_int)\n",
    "                \n",
    "        for batch_i, (q_lengths, a_lengths, prompts_batch, answers_batch) in enumerate(\n",
    "                batch_data(train_prompts_int, train_answers_int, train_batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss, xent_output, logits_output = sess.run([train_op, cost, xent_tokens, train_logits],\n",
    "                {input_data: prompts_batch, targets: answers_batch,\n",
    "                 source_lengths: q_lengths, target_lengths: a_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            total_train_loss += loss\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            if batch_i % display_step == 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>9.6f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i, epochs, batch_i, len(train_prompts_int) // train_batch_size, \n",
    "                              total_train_loss / display_step, batch_time*display_step),\n",
    "                         flush=True)\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if batch_i % validation_check == 0 and epoch_i >= min_epochs_before_validation:\n",
    "                print(\"Shuffling validation data . . .\")\n",
    "                (valid_prompts_int, valid_answers_int) = parallel_shuffle(valid_prompts_int, valid_answers_int)\n",
    "\n",
    "                total_perp = 0\n",
    "                num_valid_batches = 0\n",
    "                \n",
    "                start_time = time.time()\n",
    "                for batch_ii, (q_lengths, a_lengths, prompts_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_prompts_int, valid_answers_int, valid_batch_size)):\n",
    "\n",
    "                    [valid_perp] = sess.run([perplexity],\n",
    "                        {input_data: prompts_batch, targets: answers_batch,\n",
    "                        source_lengths: q_lengths, target_lengths: a_lengths, keep_prob: 1})\n",
    "                    total_perp += valid_perp                  \n",
    "                    num_valid_batches += 1\n",
    "                    \n",
    "                batch_time = time.time() - start_time\n",
    "                avg_valid_loss = total_perp / num_valid_batches\n",
    "                print(\"Processed validation set in {:>4.2f} seconds\".format(batch_time))\n",
    "                print(\"Average perplexity = {}\".format(avg_valid_loss))\n",
    "                check_response(sess, prompts_batch[-1], answers_batch[-1], best_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int, int_to_vocab):\n",
    "    '''Prepare the question for the model'''\n",
    "    cleaned_question = Corpus.clean_sequence(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int[UNK]) for word in cleaned_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/best_model.ckpt\n",
      "Prompt\n",
      "  Word Ids: [210, 1]\n",
      "      Text: ['wait', '.']\n",
      "Actual Answer\n",
      "  Word Ids: [210, 1]\n",
      "      Text: ['wait', '.']\n",
      "\n",
      "Beam Answer 0\n",
      "  Word Ids: [181, 225, 10544, 10544, 2340, 2340, 2340, 3239, 3239, 3336, 5235, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577]\n",
      "      Text: ['years', 'real', 'mcgann', 'mcgann', 'systems', 'systems', 'systems', 'twisted', 'twisted', 'pat', 'kindly', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves']\n",
      "\n",
      "Beam Answer 1\n",
      "  Word Ids: [181, 225, 10544, 10544, 2340, 2340, 2340, 3239, 3239, 3336, 5235, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 9215]\n",
      "      Text: ['years', 'real', 'mcgann', 'mcgann', 'systems', 'systems', 'systems', 'twisted', 'twisted', 'pat', 'kindly', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'pep']\n",
      "\n",
      "Beam Answer 2\n",
      "  Word Ids: [181, 225, 10544, 10544, 2340, 2340, 2340, 3239, 3239, 3239, 3336, 5235, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577]\n",
      "      Text: ['years', 'real', 'mcgann', 'mcgann', 'systems', 'systems', 'systems', 'twisted', 'twisted', 'twisted', 'pat', 'kindly', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves']\n",
      "\n",
      "Beam Answer 3\n",
      "  Word Ids: [181, 225, 10544, 10544, 2340, 2340, 2340, 3239, 3239, 3336, 5235, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 9215, 4577]\n",
      "      Text: ['years', 'real', 'mcgann', 'mcgann', 'systems', 'systems', 'systems', 'twisted', 'twisted', 'pat', 'kindly', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'pep', 'deserves']\n",
      "\n",
      "Beam Answer 4\n",
      "  Word Ids: [181, 225, 10544, 10544, 2340, 2340, 2340, 3239, 3239, 3336, 5235, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 4577, 3345]\n",
      "      Text: ['years', 'real', 'mcgann', 'mcgann', 'systems', 'systems', 'systems', 'twisted', 'twisted', 'pat', 'kindly', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'deserves', 'luther']\n"
     ]
    }
   ],
   "source": [
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(train_prompts_int))\n",
    "prompt_int = train_prompts_int[random]\n",
    "answer_int = train_answers_int[random]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Run the model with the input question\n",
    "    saver.restore(sess, checkpoint)\n",
    "    check_response(sess, prompt_int, answer_int, best_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
