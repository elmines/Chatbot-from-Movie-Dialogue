{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a chatbot using conversations from Cornell University's [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). The main features of our model are LSTM cells, a bidirectional dynamic RNN, and decoders with attention. \n",
    "\n",
    "The conversations will be cleaned rather extensively to help the model to produce better responses. As part of the cleaning process, punctuation will be removed, rare words will be replaced with \"UNK\" (our \"unknown\" token), longer sentences will not be used, and all letters will be in the lowercase. \n",
    "\n",
    "With a larger amount of data, it would be more practical to keep features, such as punctuation. However, I am using FloydHub's GPU services and I don't want to get carried away with too training for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#Local libraries\n",
    "from corpus import Corpus\n",
    "import metrics\n",
    "import loss_functions\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code to load the data is courtesy of https://github.com/suriyadeepan/practical_seq2seq/blob/master/datasets/cornell_corpus/data.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_corpus = Corpus(\"movie_lines.txt\", \"movie_conversations.txt\", max_vocab=8100, max_line_length=30)\n",
    "\n",
    "questions_text = cornell_corpus.prompts\n",
    "answers_text = cornell_corpus.answers\n",
    "questions_int = cornell_corpus.prompts_int\n",
    "answers_int = cornell_corpus.answers_int\n",
    "\n",
    "UNK = cornell_corpus.unk\n",
    "vocab2int = cornell_corpus.vocab2int\n",
    "int2vocab = cornell_corpus.int2vocab\n",
    "\n",
    "METATOKEN_INDEX = len(vocab2int)\n",
    "META = \"<META>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "GO = \"<GO>\"\n",
    "codes = [META, EOS, PAD, GO]\n",
    "    \n",
    "\n",
    "source_vocab_size = len(vocab2int)\n",
    "dest_vocab_size = len(vocab2int)\n",
    "\n",
    "vocab_dicts = (vocab2int, int2vocab)\n",
    "\n",
    "(questions_vocab_to_int, questions_int_to_vocab) = vocab_dicts\n",
    "(answers_vocab_to_int, answers_int_to_vocab) = vocab_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'you': 0,\n",
       "  'i': 1,\n",
       "  'the': 2,\n",
       "  'not': 3,\n",
       "  'to': 4,\n",
       "  'it': 5,\n",
       "  'a': 6,\n",
       "  'do': 7,\n",
       "  'that': 8,\n",
       "  'what': 9,\n",
       "  'are': 10,\n",
       "  'and': 11,\n",
       "  'of': 12,\n",
       "  'have': 13,\n",
       "  'me': 14,\n",
       "  'is': 15,\n",
       "  'in': 16,\n",
       "  'we': 17,\n",
       "  'he': 18,\n",
       "  'this': 19,\n",
       "  'know': 20,\n",
       "  'no': 21,\n",
       "  'for': 22,\n",
       "  'your': 23,\n",
       "  'will': 24,\n",
       "  'was': 25,\n",
       "  'my': 26,\n",
       "  'be': 27,\n",
       "  'on': 28,\n",
       "  'just': 29,\n",
       "  'did': 30,\n",
       "  'with': 31,\n",
       "  'but': 32,\n",
       "  'would': 33,\n",
       "  'they': 34,\n",
       "  'like': 35,\n",
       "  'about': 36,\n",
       "  'there': 37,\n",
       "  'all': 38,\n",
       "  'get': 39,\n",
       "  'here': 40,\n",
       "  'got': 41,\n",
       "  'so': 42,\n",
       "  'how': 43,\n",
       "  'she': 44,\n",
       "  'out': 45,\n",
       "  'if': 46,\n",
       "  'him': 47,\n",
       "  'want': 48,\n",
       "  'can': 49,\n",
       "  'think': 50,\n",
       "  'up': 51,\n",
       "  'well': 52,\n",
       "  'right': 53,\n",
       "  'why': 54,\n",
       "  'go': 55,\n",
       "  'at': 56,\n",
       "  'one': 57,\n",
       "  'yes': 58,\n",
       "  'now': 59,\n",
       "  'oh': 60,\n",
       "  'yeah': 61,\n",
       "  'going': 62,\n",
       "  'her': 63,\n",
       "  'who': 64,\n",
       "  'see': 65,\n",
       "  'where': 66,\n",
       "  'good': 67,\n",
       "  'tell': 68,\n",
       "  'could': 69,\n",
       "  'come': 70,\n",
       "  'ca': 71,\n",
       "  'were': 72,\n",
       "  'as': 73,\n",
       "  'an': 74,\n",
       "  'u': 75,\n",
       "  'when': 76,\n",
       "  'from': 77,\n",
       "  'say': 78,\n",
       "  'been': 79,\n",
       "  'time': 80,\n",
       "  'his': 81,\n",
       "  'some': 82,\n",
       "  'let': 83,\n",
       "  'back': 84,\n",
       "  'or': 85,\n",
       "  'look': 86,\n",
       "  'then': 87,\n",
       "  'them': 88,\n",
       "  'something': 89,\n",
       "  'mean': 90,\n",
       "  'take': 91,\n",
       "  'us': 92,\n",
       "  'man': 93,\n",
       "  'never': 94,\n",
       "  'had': 95,\n",
       "  'okay': 96,\n",
       "  'does': 97,\n",
       "  'too': 98,\n",
       "  'sure': 99,\n",
       "  'na': 100,\n",
       "  'really': 101,\n",
       "  'way': 102,\n",
       "  'make': 103,\n",
       "  'should': 104,\n",
       "  'said': 105,\n",
       "  'any': 106,\n",
       "  'down': 107,\n",
       "  'more': 108,\n",
       "  'little': 109,\n",
       "  'need': 110,\n",
       "  'maybe': 111,\n",
       "  'gon': 112,\n",
       "  'very': 113,\n",
       "  'over': 114,\n",
       "  'off': 115,\n",
       "  'thing': 116,\n",
       "  'only': 117,\n",
       "  'mr': 118,\n",
       "  'sorry': 119,\n",
       "  'much': 120,\n",
       "  'has': 121,\n",
       "  'am': 122,\n",
       "  'people': 123,\n",
       "  'anything': 124,\n",
       "  'by': 125,\n",
       "  'two': 126,\n",
       "  'our': 127,\n",
       "  'sir': 128,\n",
       "  'talk': 129,\n",
       "  'nothing': 130,\n",
       "  'give': 131,\n",
       "  'doing': 132,\n",
       "  'thought': 133,\n",
       "  'call': 134,\n",
       "  'love': 135,\n",
       "  'because': 136,\n",
       "  'told': 137,\n",
       "  'still': 138,\n",
       "  'must': 139,\n",
       "  'work': 140,\n",
       "  'better': 141,\n",
       "  'find': 142,\n",
       "  'ever': 143,\n",
       "  'before': 144,\n",
       "  'please': 145,\n",
       "  'night': 146,\n",
       "  'even': 147,\n",
       "  'hey': 148,\n",
       "  'money': 149,\n",
       "  'help': 150,\n",
       "  'long': 151,\n",
       "  'wo': 152,\n",
       "  'around': 153,\n",
       "  'god': 154,\n",
       "  'name': 155,\n",
       "  'last': 156,\n",
       "  'first': 157,\n",
       "  'guy': 158,\n",
       "  'believe': 159,\n",
       "  'into': 160,\n",
       "  'other': 161,\n",
       "  'these': 162,\n",
       "  'things': 163,\n",
       "  'always': 164,\n",
       "  'than': 165,\n",
       "  'life': 166,\n",
       "  'those': 167,\n",
       "  'hell': 168,\n",
       "  'shit': 169,\n",
       "  'again': 170,\n",
       "  'away': 171,\n",
       "  'put': 172,\n",
       "  'old': 173,\n",
       "  'home': 174,\n",
       "  'after': 175,\n",
       "  'thank': 176,\n",
       "  'great': 177,\n",
       "  'place': 178,\n",
       "  'keep': 179,\n",
       "  'course': 180,\n",
       "  'everything': 181,\n",
       "  'guess': 182,\n",
       "  'new': 183,\n",
       "  'feel': 184,\n",
       "  'fuck': 185,\n",
       "  'talking': 186,\n",
       "  'day': 187,\n",
       "  'fine': 188,\n",
       "  'bad': 189,\n",
       "  'wait': 190,\n",
       "  'kind': 191,\n",
       "  'years': 192,\n",
       "  'big': 193,\n",
       "  'remember': 194,\n",
       "  'uh': 195,\n",
       "  'dead': 196,\n",
       "  'leave': 197,\n",
       "  'wrong': 198,\n",
       "  'kill': 199,\n",
       "  'huh': 200,\n",
       "  'ask': 201,\n",
       "  'else': 202,\n",
       "  'nice': 203,\n",
       "  'ai': 204,\n",
       "  'might': 205,\n",
       "  'hear': 206,\n",
       "  'happened': 207,\n",
       "  'girl': 208,\n",
       "  'understand': 209,\n",
       "  'lot': 210,\n",
       "  'stop': 211,\n",
       "  'three': 212,\n",
       "  'ta': 213,\n",
       "  'thanks': 214,\n",
       "  'mind': 215,\n",
       "  'enough': 216,\n",
       "  'wanted': 217,\n",
       "  'father': 218,\n",
       "  'yourself': 219,\n",
       "  'made': 220,\n",
       "  'their': 221,\n",
       "  'listen': 222,\n",
       "  'real': 223,\n",
       "  'someone': 224,\n",
       "  'stay': 225,\n",
       "  'done': 226,\n",
       "  'getting': 227,\n",
       "  'fucking': 228,\n",
       "  'mother': 229,\n",
       "  'another': 230,\n",
       "  'house': 231,\n",
       "  'car': 232,\n",
       "  'try': 233,\n",
       "  'tonight': 234,\n",
       "  'left': 235,\n",
       "  'heard': 236,\n",
       "  'saw': 237,\n",
       "  'every': 238,\n",
       "  'care': 239,\n",
       "  'coming': 240,\n",
       "  'job': 241,\n",
       "  'trying': 242,\n",
       "  'friend': 243,\n",
       "  'through': 244,\n",
       "  'came': 245,\n",
       "  'seen': 246,\n",
       "  'boy': 247,\n",
       "  'looking': 248,\n",
       "  'pretty': 249,\n",
       "  'knew': 250,\n",
       "  'may': 251,\n",
       "  'room': 252,\n",
       "  'being': 253,\n",
       "  'ya': 254,\n",
       "  'own': 255,\n",
       "  'miss': 256,\n",
       "  'same': 257,\n",
       "  'went': 258,\n",
       "  'guys': 259,\n",
       "  'tomorrow': 260,\n",
       "  'idea': 261,\n",
       "  'show': 262,\n",
       "  'world': 263,\n",
       "  'killed': 264,\n",
       "  'business': 265,\n",
       "  'matter': 266,\n",
       "  'live': 267,\n",
       "  'many': 268,\n",
       "  'morning': 269,\n",
       "  'best': 270,\n",
       "  'yet': 271,\n",
       "  'called': 272,\n",
       "  'dad': 273,\n",
       "  'already': 274,\n",
       "  'next': 275,\n",
       "  'five': 276,\n",
       "  'used': 277,\n",
       "  'hello': 278,\n",
       "  'whole': 279,\n",
       "  'found': 280,\n",
       "  'today': 281,\n",
       "  'baby': 282,\n",
       "  'says': 283,\n",
       "  'which': 284,\n",
       "  'stuff': 285,\n",
       "  'run': 286,\n",
       "  'wife': 287,\n",
       "  'use': 288,\n",
       "  'play': 289,\n",
       "  'meet': 290,\n",
       "  'saying': 291,\n",
       "  'woman': 292,\n",
       "  'few': 293,\n",
       "  'probably': 294,\n",
       "  'without': 295,\n",
       "  'minute': 296,\n",
       "  'myself': 297,\n",
       "  'start': 298,\n",
       "  'alone': 299,\n",
       "  'hi': 300,\n",
       "  'ago': 301,\n",
       "  'once': 302,\n",
       "  'somebody': 303,\n",
       "  'together': 304,\n",
       "  'crazy': 305,\n",
       "  'hundred': 306,\n",
       "  'problem': 307,\n",
       "  'exactly': 308,\n",
       "  'afraid': 309,\n",
       "  'school': 310,\n",
       "  'while': 311,\n",
       "  'days': 312,\n",
       "  'son': 313,\n",
       "  'four': 314,\n",
       "  'jesus': 315,\n",
       "  'nobody': 316,\n",
       "  'men': 317,\n",
       "  'damn': 318,\n",
       "  'gone': 319,\n",
       "  'worry': 320,\n",
       "  'head': 321,\n",
       "  'hard': 322,\n",
       "  'forget': 323,\n",
       "  'looks': 324,\n",
       "  'friends': 325,\n",
       "  'hope': 326,\n",
       "  'took': 327,\n",
       "  'mrs': 328,\n",
       "  'since': 329,\n",
       "  'wants': 330,\n",
       "  'until': 331,\n",
       "  'ten': 332,\n",
       "  'mom': 333,\n",
       "  'kid': 334,\n",
       "  'anyway': 335,\n",
       "  'doctor': 336,\n",
       "  'most': 337,\n",
       "  'anyone': 338,\n",
       "  'such': 339,\n",
       "  'alright': 340,\n",
       "  'wan': 341,\n",
       "  'minutes': 342,\n",
       "  'shut': 343,\n",
       "  'read': 344,\n",
       "  'supposed': 345,\n",
       "  'bring': 346,\n",
       "  'deal': 347,\n",
       "  'true': 348,\n",
       "  'late': 349,\n",
       "  'case': 350,\n",
       "  'hold': 351,\n",
       "  'die': 352,\n",
       "  'point': 353,\n",
       "  'actually': 354,\n",
       "  'word': 355,\n",
       "  'watch': 356,\n",
       "  'hurt': 357,\n",
       "  'drink': 358,\n",
       "  'ready': 359,\n",
       "  'sleep': 360,\n",
       "  'knows': 361,\n",
       "  'working': 362,\n",
       "  'makes': 363,\n",
       "  'brother': 364,\n",
       "  'jack': 365,\n",
       "  'thinking': 366,\n",
       "  'lost': 367,\n",
       "  'week': 368,\n",
       "  'part': 369,\n",
       "  'open': 370,\n",
       "  'dr': 371,\n",
       "  'soon': 372,\n",
       "  'beautiful': 373,\n",
       "  'easy': 374,\n",
       "  'john': 375,\n",
       "  'happen': 376,\n",
       "  'married': 377,\n",
       "  'story': 378,\n",
       "  'question': 379,\n",
       "  'move': 380,\n",
       "  'turn': 381,\n",
       "  'phone': 382,\n",
       "  'asked': 383,\n",
       "  'sit': 384,\n",
       "  'under': 385,\n",
       "  'everybody': 386,\n",
       "  'year': 387,\n",
       "  'whatever': 388,\n",
       "  'having': 389,\n",
       "  'pay': 390,\n",
       "  'ass': 391,\n",
       "  'least': 392,\n",
       "  'hate': 393,\n",
       "  'kids': 394,\n",
       "  'far': 395,\n",
       "  'later': 396,\n",
       "  'quite': 397,\n",
       "  'family': 398,\n",
       "  'both': 399,\n",
       "  'six': 400,\n",
       "  'happy': 401,\n",
       "  'cut': 402,\n",
       "  'wish': 403,\n",
       "  'trouble': 404,\n",
       "  'gave': 405,\n",
       "  'anybody': 406,\n",
       "  'eat': 407,\n",
       "  'check': 408,\n",
       "  'taking': 409,\n",
       "  'mine': 410,\n",
       "  'times': 411,\n",
       "  'shot': 412,\n",
       "  'face': 413,\n",
       "  'door': 414,\n",
       "  'trust': 415,\n",
       "  'hit': 416,\n",
       "  'half': 417,\n",
       "  'thousand': 418,\n",
       "  'town': 419,\n",
       "  'funny': 420,\n",
       "  'yours': 421,\n",
       "  'police': 422,\n",
       "  'suppose': 423,\n",
       "  'ok': 424,\n",
       "  'change': 425,\n",
       "  'couple': 426,\n",
       "  'dollars': 427,\n",
       "  'captain': 428,\n",
       "  'bet': 429,\n",
       "  'chance': 430,\n",
       "  'end': 431,\n",
       "  'honey': 432,\n",
       "  'important': 433,\n",
       "  'different': 434,\n",
       "  'met': 435,\n",
       "  'second': 436,\n",
       "  'gun': 437,\n",
       "  'young': 438,\n",
       "  'fuckin': 439,\n",
       "  'answer': 440,\n",
       "  'excuse': 441,\n",
       "  'anymore': 442,\n",
       "  'walk': 443,\n",
       "  'each': 444,\n",
       "  'hours': 445,\n",
       "  'telling': 446,\n",
       "  'sometimes': 447,\n",
       "  'rest': 448,\n",
       "  'making': 449,\n",
       "  'bit': 450,\n",
       "  'everyone': 451,\n",
       "  'ah': 452,\n",
       "  'hand': 453,\n",
       "  'gets': 454,\n",
       "  'close': 455,\n",
       "  'party': 456,\n",
       "  'death': 457,\n",
       "  'truth': 458,\n",
       "  'set': 459,\n",
       "  'person': 460,\n",
       "  'either': 461,\n",
       "  'inside': 462,\n",
       "  'tried': 463,\n",
       "  'number': 464,\n",
       "  'bed': 465,\n",
       "  'sick': 466,\n",
       "  'heart': 467,\n",
       "  'sort': 468,\n",
       "  'christ': 469,\n",
       "  'game': 470,\n",
       "  'serious': 471,\n",
       "  'means': 472,\n",
       "  'reason': 473,\n",
       "  'waiting': 474,\n",
       "  'side': 475,\n",
       "  'office': 476,\n",
       "  'almost': 477,\n",
       "  'send': 478,\n",
       "  'buy': 479,\n",
       "  'eyes': 480,\n",
       "  'daddy': 481,\n",
       "  'drive': 482,\n",
       "  'though': 483,\n",
       "  'water': 484,\n",
       "  'war': 485,\n",
       "  'break': 486,\n",
       "  'stupid': 487,\n",
       "  'its': 488,\n",
       "  'pick': 489,\n",
       "  'comes': 490,\n",
       "  'died': 491,\n",
       "  'white': 492,\n",
       "  'goes': 493,\n",
       "  'stand': 494,\n",
       "  'dinner': 495,\n",
       "  'book': 496,\n",
       "  'shoot': 497,\n",
       "  'speak': 498,\n",
       "  'husband': 499,\n",
       "  'alive': 500,\n",
       "  'along': 501,\n",
       "  'ahead': 502,\n",
       "  'twenty': 503,\n",
       "  'women': 504,\n",
       "  'george': 505,\n",
       "  'running': 506,\n",
       "  'frank': 507,\n",
       "  'sounds': 508,\n",
       "  'lady': 509,\n",
       "  'fire': 510,\n",
       "  'ship': 511,\n",
       "  'asking': 512,\n",
       "  'dear': 513,\n",
       "  'behind': 514,\n",
       "  'seem': 515,\n",
       "  'hands': 516,\n",
       "  'goddamn': 517,\n",
       "  'high': 518,\n",
       "  'promise': 519,\n",
       "  'against': 520,\n",
       "  'body': 521,\n",
       "  'black': 522,\n",
       "  'harry': 523,\n",
       "  'blood': 524,\n",
       "  'perhaps': 525,\n",
       "  'scared': 526,\n",
       "  'hour': 527,\n",
       "  'dog': 528,\n",
       "  'bullshit': 529,\n",
       "  'months': 530,\n",
       "  'news': 531,\n",
       "  'safe': 532,\n",
       "  'line': 533,\n",
       "  'fun': 534,\n",
       "  'power': 535,\n",
       "  'feeling': 536,\n",
       "  'seems': 537,\n",
       "  'outside': 538,\n",
       "  'million': 539,\n",
       "  'hot': 540,\n",
       "  'figure': 541,\n",
       "  'lose': 542,\n",
       "  'full': 543,\n",
       "  'kidding': 544,\n",
       "  'brought': 545,\n",
       "  'sound': 546,\n",
       "  'somewhere': 547,\n",
       "  'weeks': 548,\n",
       "  'happens': 549,\n",
       "  'boys': 550,\n",
       "  'write': 551,\n",
       "  'city': 552,\n",
       "  'sense': 553,\n",
       "  'street': 554,\n",
       "  'glad': 555,\n",
       "  'bill': 556,\n",
       "  'cool': 557,\n",
       "  'fight': 558,\n",
       "  'sister': 559,\n",
       "  'sent': 560,\n",
       "  'started': 561,\n",
       "  'between': 562,\n",
       "  'fact': 563,\n",
       "  'girls': 564,\n",
       "  'goin': 565,\n",
       "  'living': 566,\n",
       "  'eight': 567,\n",
       "  'needs': 568,\n",
       "  'also': 569,\n",
       "  'shall': 570,\n",
       "  'movie': 571,\n",
       "  'save': 572,\n",
       "  'country': 573,\n",
       "  'light': 574,\n",
       "  'lucky': 575,\n",
       "  'free': 576,\n",
       "  'front': 577,\n",
       "  'calling': 578,\n",
       "  'leaving': 579,\n",
       "  'president': 580,\n",
       "  \"c'mon\": 581,\n",
       "  'fifty': 582,\n",
       "  'lie': 583,\n",
       "  'sex': 584,\n",
       "  'wonderful': 585,\n",
       "  'tired': 586,\n",
       "  'himself': 587,\n",
       "  'moment': 588,\n",
       "  'plan': 589,\n",
       "  'possible': 590,\n",
       "  'fast': 591,\n",
       "  'picture': 592,\n",
       "  'questions': 593,\n",
       "  'mary': 594,\n",
       "  'coffee': 595,\n",
       "  'certainly': 596,\n",
       "  'luck': 597,\n",
       "  'daughter': 598,\n",
       "  'cold': 599,\n",
       "  'children': 600,\n",
       "  'till': 601,\n",
       "  'beat': 602,\n",
       "  'rather': 603,\n",
       "  'able': 604,\n",
       "  'bitch': 605,\n",
       "  'sam': 606,\n",
       "  'poor': 607,\n",
       "  'expect': 608,\n",
       "  'hair': 609,\n",
       "  'special': 610,\n",
       "  'mister': 611,\n",
       "  'pull': 612,\n",
       "  'child': 613,\n",
       "  'york': 614,\n",
       "  'touch': 615,\n",
       "  'playing': 616,\n",
       "  'thinks': 617,\n",
       "  'learn': 618,\n",
       "  'accident': 619,\n",
       "  'follow': 620,\n",
       "  'control': 621,\n",
       "  'worth': 622,\n",
       "  'hotel': 623,\n",
       "  'date': 624,\n",
       "  'ride': 625,\n",
       "  'food': 626,\n",
       "  'parents': 627,\n",
       "  'miles': 628,\n",
       "  'hospital': 629,\n",
       "  'goodbye': 630,\n",
       "  'looked': 631,\n",
       "  'absolutely': 632,\n",
       "  'company': 633,\n",
       "  'lives': 634,\n",
       "  'piece': 635,\n",
       "  'small': 636,\n",
       "  'perfect': 637,\n",
       "  'dream': 638,\n",
       "  'air': 639,\n",
       "  'catch': 640,\n",
       "  'words': 641,\n",
       "  'straight': 642,\n",
       "  'king': 643,\n",
       "  'cause': 644,\n",
       "  'order': 645,\n",
       "  'none': 646,\n",
       "  'uhhuh': 647,\n",
       "  'taken': 648,\n",
       "  'explain': 649,\n",
       "  'meeting': 650,\n",
       "  'interested': 651,\n",
       "  'doin': 652,\n",
       "  'works': 653,\n",
       "  'seven': 654,\n",
       "  'worse': 655,\n",
       "  'others': 656,\n",
       "  'worked': 657,\n",
       "  'tom': 658,\n",
       "  'outta': 659,\n",
       "  'mouth': 660,\n",
       "  'general': 661,\n",
       "  'charlie': 662,\n",
       "  'wonder': 663,\n",
       "  'act': 664,\n",
       "  'swear': 665,\n",
       "  'worried': 666,\n",
       "  'hang': 667,\n",
       "  'walter': 668,\n",
       "  'yesterday': 669,\n",
       "  'major': 670,\n",
       "  'talked': 671,\n",
       "  'known': 672,\n",
       "  'buddy': 673,\n",
       "  'david': 674,\n",
       "  'throw': 675,\n",
       "  'takes': 676,\n",
       "  'bob': 677,\n",
       "  'drop': 678,\n",
       "  'quit': 679,\n",
       "  'eh': 680,\n",
       "  'clear': 681,\n",
       "  'paul': 682,\n",
       "  'boss': 683,\n",
       "  'state': 684,\n",
       "  'handle': 685,\n",
       "  'wear': 686,\n",
       "  'sweet': 687,\n",
       "  'top': 688,\n",
       "  'ben': 689,\n",
       "  'clothes': 690,\n",
       "  'report': 691,\n",
       "  'hurry': 692,\n",
       "  'besides': 693,\n",
       "  'past': 694,\n",
       "  'joe': 695,\n",
       "  'michael': 696,\n",
       "  'unless': 697,\n",
       "  'nine': 698,\n",
       "  'human': 699,\n",
       "  'seeing': 700,\n",
       "  'except': 701,\n",
       "  'busy': 702,\n",
       "  'anywhere': 703,\n",
       "  'mad': 704,\n",
       "  'thirty': 705,\n",
       "  'sign': 706,\n",
       "  'careful': 707,\n",
       "  'dark': 708,\n",
       "  'fault': 709,\n",
       "  'lying': 710,\n",
       "  'music': 711,\n",
       "  'red': 712,\n",
       "  'clean': 713,\n",
       "  'finish': 714,\n",
       "  'meant': 715,\n",
       "  'mistake': 716,\n",
       "  'feet': 717,\n",
       "  'secret': 718,\n",
       "  'apartment': 719,\n",
       "  'difference': 720,\n",
       "  'cops': 721,\n",
       "  'less': 722,\n",
       "  'blow': 723,\n",
       "  'road': 724,\n",
       "  'choice': 725,\n",
       "  'strange': 726,\n",
       "  'nothin': 727,\n",
       "  'tv': 728,\n",
       "  'early': 729,\n",
       "  'marry': 730,\n",
       "  'darling': 731,\n",
       "  'terrible': 732,\n",
       "  'moving': 733,\n",
       "  'jimmy': 734,\n",
       "  'cop': 735,\n",
       "  'smart': 736,\n",
       "  'dude': 737,\n",
       "  'liked': 738,\n",
       "  'um': 739,\n",
       "  'nick': 740,\n",
       "  'lord': 741,\n",
       "  'peter': 742,\n",
       "  'lunch': 743,\n",
       "  'earth': 744,\n",
       "  'figured': 745,\n",
       "  'paid': 746,\n",
       "  'information': 747,\n",
       "  'fair': 748,\n",
       "  'fall': 749,\n",
       "  'bank': 750,\n",
       "  'class': 751,\n",
       "  'bucks': 752,\n",
       "  \"ma'am\": 753,\n",
       "  'tough': 754,\n",
       "  'month': 755,\n",
       "  'changed': 756,\n",
       "  'interesting': 757,\n",
       "  'giving': 758,\n",
       "  'trip': 759,\n",
       "  'plane': 760,\n",
       "  'ray': 761,\n",
       "  'simple': 762,\n",
       "  'near': 763,\n",
       "  'broke': 764,\n",
       "  'quiet': 765,\n",
       "  'fucked': 766,\n",
       "  'sake': 767,\n",
       "  'jim': 768,\n",
       "  'favor': 769,\n",
       "  'fifteen': 770,\n",
       "  'felt': 771,\n",
       "  'evening': 772,\n",
       "  'relax': 773,\n",
       "  'ought': 774,\n",
       "  'store': 775,\n",
       "  'private': 776,\n",
       "  'ones': 777,\n",
       "  'caught': 778,\n",
       "  'building': 779,\n",
       "  'weird': 780,\n",
       "  'dick': 781,\n",
       "  'boat': 782,\n",
       "  'dance': 783,\n",
       "  'max': 784,\n",
       "  'personal': 785,\n",
       "  'happening': 786,\n",
       "  'calls': 787,\n",
       "  'wow': 788,\n",
       "  'watching': 789,\n",
       "  'twelve': 790,\n",
       "  'honest': 791,\n",
       "  'rich': 792,\n",
       "  'loved': 793,\n",
       "  'kinda': 794,\n",
       "  'win': 795,\n",
       "  'chief': 796,\n",
       "  'nervous': 797,\n",
       "  'rose': 798,\n",
       "  'forgot': 799,\n",
       "  'murder': 800,\n",
       "  'blue': 801,\n",
       "  'paper': 802,\n",
       "  'law': 803,\n",
       "  'longer': 804,\n",
       "  'eddie': 805,\n",
       "  'ice': 806,\n",
       "  'problems': 807,\n",
       "  'finished': 808,\n",
       "  'message': 809,\n",
       "  'dangerous': 810,\n",
       "  'system': 811,\n",
       "  'james': 812,\n",
       "  'likes': 813,\n",
       "  'officer': 814,\n",
       "  'asshole': 815,\n",
       "  'bastard': 816,\n",
       "  'turned': 817,\n",
       "  \"o'clock\": 818,\n",
       "  'american': 819,\n",
       "  'johnny': 820,\n",
       "  'count': 821,\n",
       "  'jake': 822,\n",
       "  'fool': 823,\n",
       "  'welcome': 824,\n",
       "  'eye': 825,\n",
       "  'kiss': 826,\n",
       "  'missed': 827,\n",
       "  'stick': 828,\n",
       "  'mama': 829,\n",
       "  'army': 830,\n",
       "  'bother': 831,\n",
       "  'floor': 832,\n",
       "  'books': 833,\n",
       "  'owe': 834,\n",
       "  'sell': 835,\n",
       "  'staying': 836,\n",
       "  'record': 837,\n",
       "  'uncle': 838,\n",
       "  'killer': 839,\n",
       "  'agent': 840,\n",
       "  'involved': 841,\n",
       "  'upset': 842,\n",
       "  'hungry': 843,\n",
       "  'cash': 844,\n",
       "  'honor': 845,\n",
       "  'offer': 846,\n",
       "  'middle': 847,\n",
       "  'key': 848,\n",
       "  'english': 849,\n",
       "  'drunk': 850,\n",
       "  'imagine': 851,\n",
       "  'killing': 852,\n",
       "  'ha': 853,\n",
       "  'radio': 854,\n",
       "  'afternoon': 855,\n",
       "  'born': 856,\n",
       "  'completely': 857,\n",
       "  'given': 858,\n",
       "  'wake': 859,\n",
       "  'pleasure': 860,\n",
       "  'pictures': 861,\n",
       "  'tape': 862,\n",
       "  'list': 863,\n",
       "  'picked': 864,\n",
       "  'christmas': 865,\n",
       "  'become': 866,\n",
       "  'hardly': 867,\n",
       "  'calm': 868,\n",
       "  'future': 869,\n",
       "  'wearing': 870,\n",
       "  'brain': 871,\n",
       "  'totally': 872,\n",
       "  'security': 873,\n",
       "  'kept': 874,\n",
       "  'letter': 875,\n",
       "  'neither': 876,\n",
       "  'suit': 877,\n",
       "  'needed': 878,\n",
       "  'ring': 879,\n",
       "  'appreciate': 880,\n",
       "  'lawyer': 881,\n",
       "  'lots': 882,\n",
       "  'truck': 883,\n",
       "  'ma': 884,\n",
       "  'realize': 885,\n",
       "  'surprise': 886,\n",
       "  'dress': 887,\n",
       "  'doubt': 888,\n",
       "  'bought': 889,\n",
       "  'bag': 890,\n",
       "  'voice': 891,\n",
       "  'window': 892,\n",
       "  'somethin': 893,\n",
       "  'quick': 894,\n",
       "  'listening': 895,\n",
       "  'grand': 896,\n",
       "  'missing': 897,\n",
       "  'service': 898,\n",
       "  'table': 899,\n",
       "  'pain': 900,\n",
       "  'train': 901,\n",
       "  'fix': 902,\n",
       "  'sitting': 903,\n",
       "  'situation': 904,\n",
       "  'slow': 905,\n",
       "  'beer': 906,\n",
       "  'certain': 907,\n",
       "  'joke': 908,\n",
       "  'forgive': 909,\n",
       "  'club': 910,\n",
       "  'dying': 911,\n",
       "  'stopped': 912,\n",
       "  'college': 913,\n",
       "  'spend': 914,\n",
       "  'history': 915,\n",
       "  'jail': 916,\n",
       "  'named': 917,\n",
       "  'respect': 918,\n",
       "  'birthday': 919,\n",
       "  'seconds': 920,\n",
       "  'attack': 921,\n",
       "  'wrote': 922,\n",
       "  'la': 923,\n",
       "  'yah': 924,\n",
       "  'short': 925,\n",
       "  'station': 926,\n",
       "  'join': 927,\n",
       "  'team': 928,\n",
       "  'driving': 929,\n",
       "  'impossible': 930,\n",
       "  'cover': 931,\n",
       "  'enjoy': 932,\n",
       "  'pardon': 933,\n",
       "  'card': 934,\n",
       "  'charge': 935,\n",
       "  'court': 936,\n",
       "  'mike': 937,\n",
       "  'space': 938,\n",
       "  'fly': 939,\n",
       "  'decided': 940,\n",
       "  'island': 941,\n",
       "  'girlfriend': 942,\n",
       "  'carry': 943,\n",
       "  'third': 944,\n",
       "  'pass': 945,\n",
       "  'land': 946,\n",
       "  'french': 947,\n",
       "  'lieutenant': 948,\n",
       "  'gives': 949,\n",
       "  'prove': 950,\n",
       "  'movies': 951,\n",
       "  'age': 952,\n",
       "  'ran': 953,\n",
       "  'detective': 954,\n",
       "  'instead': 955,\n",
       "  'awful': 956,\n",
       "  'mark': 957,\n",
       "  'evil': 958,\n",
       "  'asleep': 959,\n",
       "  'smell': 960,\n",
       "  'reach': 961,\n",
       "  'present': 962,\n",
       "  'return': 963,\n",
       "  'talkin': 964,\n",
       "  'silly': 965,\n",
       "  'fat': 966,\n",
       "  'eve': 967,\n",
       "  'crime': 968,\n",
       "  'finally': 969,\n",
       "  'cost': 970,\n",
       "  'wedding': 971,\n",
       "  'deep': 972,\n",
       "  'usually': 973,\n",
       "  'bar': 974,\n",
       "  'saved': 975,\n",
       "  'church': 976,\n",
       "  'putting': 977,\n",
       "  'across': 978,\n",
       "  'gotten': 979,\n",
       "  'machine': 980,\n",
       "  'idiot': 981,\n",
       "  'plans': 982,\n",
       "  'reading': 983,\n",
       "  'prison': 984,\n",
       "  'computer': 985,\n",
       "  'fish': 986,\n",
       "  'papers': 987,\n",
       "  'park': 988,\n",
       "  'jerry': 989,\n",
       "  'holy': 990,\n",
       "  'moved': 991,\n",
       "  'address': 992,\n",
       "  'promised': 993,\n",
       "  'plenty': 994,\n",
       "  'twice': 995,\n",
       "  'dreams': 996,\n",
       "  'wondering': 997,\n",
       "  'star': 998,\n",
       "  'department': 999,\n",
       "  ...},\n",
       " {0: 'you',\n",
       "  1: 'i',\n",
       "  2: 'the',\n",
       "  3: 'not',\n",
       "  4: 'to',\n",
       "  5: 'it',\n",
       "  6: 'a',\n",
       "  7: 'do',\n",
       "  8: 'that',\n",
       "  9: 'what',\n",
       "  10: 'are',\n",
       "  11: 'and',\n",
       "  12: 'of',\n",
       "  13: 'have',\n",
       "  14: 'me',\n",
       "  15: 'is',\n",
       "  16: 'in',\n",
       "  17: 'we',\n",
       "  18: 'he',\n",
       "  19: 'this',\n",
       "  20: 'know',\n",
       "  21: 'no',\n",
       "  22: 'for',\n",
       "  23: 'your',\n",
       "  24: 'will',\n",
       "  25: 'was',\n",
       "  26: 'my',\n",
       "  27: 'be',\n",
       "  28: 'on',\n",
       "  29: 'just',\n",
       "  30: 'did',\n",
       "  31: 'with',\n",
       "  32: 'but',\n",
       "  33: 'would',\n",
       "  34: 'they',\n",
       "  35: 'like',\n",
       "  36: 'about',\n",
       "  37: 'there',\n",
       "  38: 'all',\n",
       "  39: 'get',\n",
       "  40: 'here',\n",
       "  41: 'got',\n",
       "  42: 'so',\n",
       "  43: 'how',\n",
       "  44: 'she',\n",
       "  45: 'out',\n",
       "  46: 'if',\n",
       "  47: 'him',\n",
       "  48: 'want',\n",
       "  49: 'can',\n",
       "  50: 'think',\n",
       "  51: 'up',\n",
       "  52: 'well',\n",
       "  53: 'right',\n",
       "  54: 'why',\n",
       "  55: 'go',\n",
       "  56: 'at',\n",
       "  57: 'one',\n",
       "  58: 'yes',\n",
       "  59: 'now',\n",
       "  60: 'oh',\n",
       "  61: 'yeah',\n",
       "  62: 'going',\n",
       "  63: 'her',\n",
       "  64: 'who',\n",
       "  65: 'see',\n",
       "  66: 'where',\n",
       "  67: 'good',\n",
       "  68: 'tell',\n",
       "  69: 'could',\n",
       "  70: 'come',\n",
       "  71: 'ca',\n",
       "  72: 'were',\n",
       "  73: 'as',\n",
       "  74: 'an',\n",
       "  75: 'u',\n",
       "  76: 'when',\n",
       "  77: 'from',\n",
       "  78: 'say',\n",
       "  79: 'been',\n",
       "  80: 'time',\n",
       "  81: 'his',\n",
       "  82: 'some',\n",
       "  83: 'let',\n",
       "  84: 'back',\n",
       "  85: 'or',\n",
       "  86: 'look',\n",
       "  87: 'then',\n",
       "  88: 'them',\n",
       "  89: 'something',\n",
       "  90: 'mean',\n",
       "  91: 'take',\n",
       "  92: 'us',\n",
       "  93: 'man',\n",
       "  94: 'never',\n",
       "  95: 'had',\n",
       "  96: 'okay',\n",
       "  97: 'does',\n",
       "  98: 'too',\n",
       "  99: 'sure',\n",
       "  100: 'na',\n",
       "  101: 'really',\n",
       "  102: 'way',\n",
       "  103: 'make',\n",
       "  104: 'should',\n",
       "  105: 'said',\n",
       "  106: 'any',\n",
       "  107: 'down',\n",
       "  108: 'more',\n",
       "  109: 'little',\n",
       "  110: 'need',\n",
       "  111: 'maybe',\n",
       "  112: 'gon',\n",
       "  113: 'very',\n",
       "  114: 'over',\n",
       "  115: 'off',\n",
       "  116: 'thing',\n",
       "  117: 'only',\n",
       "  118: 'mr',\n",
       "  119: 'sorry',\n",
       "  120: 'much',\n",
       "  121: 'has',\n",
       "  122: 'am',\n",
       "  123: 'people',\n",
       "  124: 'anything',\n",
       "  125: 'by',\n",
       "  126: 'two',\n",
       "  127: 'our',\n",
       "  128: 'sir',\n",
       "  129: 'talk',\n",
       "  130: 'nothing',\n",
       "  131: 'give',\n",
       "  132: 'doing',\n",
       "  133: 'thought',\n",
       "  134: 'call',\n",
       "  135: 'love',\n",
       "  136: 'because',\n",
       "  137: 'told',\n",
       "  138: 'still',\n",
       "  139: 'must',\n",
       "  140: 'work',\n",
       "  141: 'better',\n",
       "  142: 'find',\n",
       "  143: 'ever',\n",
       "  144: 'before',\n",
       "  145: 'please',\n",
       "  146: 'night',\n",
       "  147: 'even',\n",
       "  148: 'hey',\n",
       "  149: 'money',\n",
       "  150: 'help',\n",
       "  151: 'long',\n",
       "  152: 'wo',\n",
       "  153: 'around',\n",
       "  154: 'god',\n",
       "  155: 'name',\n",
       "  156: 'last',\n",
       "  157: 'first',\n",
       "  158: 'guy',\n",
       "  159: 'believe',\n",
       "  160: 'into',\n",
       "  161: 'other',\n",
       "  162: 'these',\n",
       "  163: 'things',\n",
       "  164: 'always',\n",
       "  165: 'than',\n",
       "  166: 'life',\n",
       "  167: 'those',\n",
       "  168: 'hell',\n",
       "  169: 'shit',\n",
       "  170: 'again',\n",
       "  171: 'away',\n",
       "  172: 'put',\n",
       "  173: 'old',\n",
       "  174: 'home',\n",
       "  175: 'after',\n",
       "  176: 'thank',\n",
       "  177: 'great',\n",
       "  178: 'place',\n",
       "  179: 'keep',\n",
       "  180: 'course',\n",
       "  181: 'everything',\n",
       "  182: 'guess',\n",
       "  183: 'new',\n",
       "  184: 'feel',\n",
       "  185: 'fuck',\n",
       "  186: 'talking',\n",
       "  187: 'day',\n",
       "  188: 'fine',\n",
       "  189: 'bad',\n",
       "  190: 'wait',\n",
       "  191: 'kind',\n",
       "  192: 'years',\n",
       "  193: 'big',\n",
       "  194: 'remember',\n",
       "  195: 'uh',\n",
       "  196: 'dead',\n",
       "  197: 'leave',\n",
       "  198: 'wrong',\n",
       "  199: 'kill',\n",
       "  200: 'huh',\n",
       "  201: 'ask',\n",
       "  202: 'else',\n",
       "  203: 'nice',\n",
       "  204: 'ai',\n",
       "  205: 'might',\n",
       "  206: 'hear',\n",
       "  207: 'happened',\n",
       "  208: 'girl',\n",
       "  209: 'understand',\n",
       "  210: 'lot',\n",
       "  211: 'stop',\n",
       "  212: 'three',\n",
       "  213: 'ta',\n",
       "  214: 'thanks',\n",
       "  215: 'mind',\n",
       "  216: 'enough',\n",
       "  217: 'wanted',\n",
       "  218: 'father',\n",
       "  219: 'yourself',\n",
       "  220: 'made',\n",
       "  221: 'their',\n",
       "  222: 'listen',\n",
       "  223: 'real',\n",
       "  224: 'someone',\n",
       "  225: 'stay',\n",
       "  226: 'done',\n",
       "  227: 'getting',\n",
       "  228: 'fucking',\n",
       "  229: 'mother',\n",
       "  230: 'another',\n",
       "  231: 'house',\n",
       "  232: 'car',\n",
       "  233: 'try',\n",
       "  234: 'tonight',\n",
       "  235: 'left',\n",
       "  236: 'heard',\n",
       "  237: 'saw',\n",
       "  238: 'every',\n",
       "  239: 'care',\n",
       "  240: 'coming',\n",
       "  241: 'job',\n",
       "  242: 'trying',\n",
       "  243: 'friend',\n",
       "  244: 'through',\n",
       "  245: 'came',\n",
       "  246: 'seen',\n",
       "  247: 'boy',\n",
       "  248: 'looking',\n",
       "  249: 'pretty',\n",
       "  250: 'knew',\n",
       "  251: 'may',\n",
       "  252: 'room',\n",
       "  253: 'being',\n",
       "  254: 'ya',\n",
       "  255: 'own',\n",
       "  256: 'miss',\n",
       "  257: 'same',\n",
       "  258: 'went',\n",
       "  259: 'guys',\n",
       "  260: 'tomorrow',\n",
       "  261: 'idea',\n",
       "  262: 'show',\n",
       "  263: 'world',\n",
       "  264: 'killed',\n",
       "  265: 'business',\n",
       "  266: 'matter',\n",
       "  267: 'live',\n",
       "  268: 'many',\n",
       "  269: 'morning',\n",
       "  270: 'best',\n",
       "  271: 'yet',\n",
       "  272: 'called',\n",
       "  273: 'dad',\n",
       "  274: 'already',\n",
       "  275: 'next',\n",
       "  276: 'five',\n",
       "  277: 'used',\n",
       "  278: 'hello',\n",
       "  279: 'whole',\n",
       "  280: 'found',\n",
       "  281: 'today',\n",
       "  282: 'baby',\n",
       "  283: 'says',\n",
       "  284: 'which',\n",
       "  285: 'stuff',\n",
       "  286: 'run',\n",
       "  287: 'wife',\n",
       "  288: 'use',\n",
       "  289: 'play',\n",
       "  290: 'meet',\n",
       "  291: 'saying',\n",
       "  292: 'woman',\n",
       "  293: 'few',\n",
       "  294: 'probably',\n",
       "  295: 'without',\n",
       "  296: 'minute',\n",
       "  297: 'myself',\n",
       "  298: 'start',\n",
       "  299: 'alone',\n",
       "  300: 'hi',\n",
       "  301: 'ago',\n",
       "  302: 'once',\n",
       "  303: 'somebody',\n",
       "  304: 'together',\n",
       "  305: 'crazy',\n",
       "  306: 'hundred',\n",
       "  307: 'problem',\n",
       "  308: 'exactly',\n",
       "  309: 'afraid',\n",
       "  310: 'school',\n",
       "  311: 'while',\n",
       "  312: 'days',\n",
       "  313: 'son',\n",
       "  314: 'four',\n",
       "  315: 'jesus',\n",
       "  316: 'nobody',\n",
       "  317: 'men',\n",
       "  318: 'damn',\n",
       "  319: 'gone',\n",
       "  320: 'worry',\n",
       "  321: 'head',\n",
       "  322: 'hard',\n",
       "  323: 'forget',\n",
       "  324: 'looks',\n",
       "  325: 'friends',\n",
       "  326: 'hope',\n",
       "  327: 'took',\n",
       "  328: 'mrs',\n",
       "  329: 'since',\n",
       "  330: 'wants',\n",
       "  331: 'until',\n",
       "  332: 'ten',\n",
       "  333: 'mom',\n",
       "  334: 'kid',\n",
       "  335: 'anyway',\n",
       "  336: 'doctor',\n",
       "  337: 'most',\n",
       "  338: 'anyone',\n",
       "  339: 'such',\n",
       "  340: 'alright',\n",
       "  341: 'wan',\n",
       "  342: 'minutes',\n",
       "  343: 'shut',\n",
       "  344: 'read',\n",
       "  345: 'supposed',\n",
       "  346: 'bring',\n",
       "  347: 'deal',\n",
       "  348: 'true',\n",
       "  349: 'late',\n",
       "  350: 'case',\n",
       "  351: 'hold',\n",
       "  352: 'die',\n",
       "  353: 'point',\n",
       "  354: 'actually',\n",
       "  355: 'word',\n",
       "  356: 'watch',\n",
       "  357: 'hurt',\n",
       "  358: 'drink',\n",
       "  359: 'ready',\n",
       "  360: 'sleep',\n",
       "  361: 'knows',\n",
       "  362: 'working',\n",
       "  363: 'makes',\n",
       "  364: 'brother',\n",
       "  365: 'jack',\n",
       "  366: 'thinking',\n",
       "  367: 'lost',\n",
       "  368: 'week',\n",
       "  369: 'part',\n",
       "  370: 'open',\n",
       "  371: 'dr',\n",
       "  372: 'soon',\n",
       "  373: 'beautiful',\n",
       "  374: 'easy',\n",
       "  375: 'john',\n",
       "  376: 'happen',\n",
       "  377: 'married',\n",
       "  378: 'story',\n",
       "  379: 'question',\n",
       "  380: 'move',\n",
       "  381: 'turn',\n",
       "  382: 'phone',\n",
       "  383: 'asked',\n",
       "  384: 'sit',\n",
       "  385: 'under',\n",
       "  386: 'everybody',\n",
       "  387: 'year',\n",
       "  388: 'whatever',\n",
       "  389: 'having',\n",
       "  390: 'pay',\n",
       "  391: 'ass',\n",
       "  392: 'least',\n",
       "  393: 'hate',\n",
       "  394: 'kids',\n",
       "  395: 'far',\n",
       "  396: 'later',\n",
       "  397: 'quite',\n",
       "  398: 'family',\n",
       "  399: 'both',\n",
       "  400: 'six',\n",
       "  401: 'happy',\n",
       "  402: 'cut',\n",
       "  403: 'wish',\n",
       "  404: 'trouble',\n",
       "  405: 'gave',\n",
       "  406: 'anybody',\n",
       "  407: 'eat',\n",
       "  408: 'check',\n",
       "  409: 'taking',\n",
       "  410: 'mine',\n",
       "  411: 'times',\n",
       "  412: 'shot',\n",
       "  413: 'face',\n",
       "  414: 'door',\n",
       "  415: 'trust',\n",
       "  416: 'hit',\n",
       "  417: 'half',\n",
       "  418: 'thousand',\n",
       "  419: 'town',\n",
       "  420: 'funny',\n",
       "  421: 'yours',\n",
       "  422: 'police',\n",
       "  423: 'suppose',\n",
       "  424: 'ok',\n",
       "  425: 'change',\n",
       "  426: 'couple',\n",
       "  427: 'dollars',\n",
       "  428: 'captain',\n",
       "  429: 'bet',\n",
       "  430: 'chance',\n",
       "  431: 'end',\n",
       "  432: 'honey',\n",
       "  433: 'important',\n",
       "  434: 'different',\n",
       "  435: 'met',\n",
       "  436: 'second',\n",
       "  437: 'gun',\n",
       "  438: 'young',\n",
       "  439: 'fuckin',\n",
       "  440: 'answer',\n",
       "  441: 'excuse',\n",
       "  442: 'anymore',\n",
       "  443: 'walk',\n",
       "  444: 'each',\n",
       "  445: 'hours',\n",
       "  446: 'telling',\n",
       "  447: 'sometimes',\n",
       "  448: 'rest',\n",
       "  449: 'making',\n",
       "  450: 'bit',\n",
       "  451: 'everyone',\n",
       "  452: 'ah',\n",
       "  453: 'hand',\n",
       "  454: 'gets',\n",
       "  455: 'close',\n",
       "  456: 'party',\n",
       "  457: 'death',\n",
       "  458: 'truth',\n",
       "  459: 'set',\n",
       "  460: 'person',\n",
       "  461: 'either',\n",
       "  462: 'inside',\n",
       "  463: 'tried',\n",
       "  464: 'number',\n",
       "  465: 'bed',\n",
       "  466: 'sick',\n",
       "  467: 'heart',\n",
       "  468: 'sort',\n",
       "  469: 'christ',\n",
       "  470: 'game',\n",
       "  471: 'serious',\n",
       "  472: 'means',\n",
       "  473: 'reason',\n",
       "  474: 'waiting',\n",
       "  475: 'side',\n",
       "  476: 'office',\n",
       "  477: 'almost',\n",
       "  478: 'send',\n",
       "  479: 'buy',\n",
       "  480: 'eyes',\n",
       "  481: 'daddy',\n",
       "  482: 'drive',\n",
       "  483: 'though',\n",
       "  484: 'water',\n",
       "  485: 'war',\n",
       "  486: 'break',\n",
       "  487: 'stupid',\n",
       "  488: 'its',\n",
       "  489: 'pick',\n",
       "  490: 'comes',\n",
       "  491: 'died',\n",
       "  492: 'white',\n",
       "  493: 'goes',\n",
       "  494: 'stand',\n",
       "  495: 'dinner',\n",
       "  496: 'book',\n",
       "  497: 'shoot',\n",
       "  498: 'speak',\n",
       "  499: 'husband',\n",
       "  500: 'alive',\n",
       "  501: 'along',\n",
       "  502: 'ahead',\n",
       "  503: 'twenty',\n",
       "  504: 'women',\n",
       "  505: 'george',\n",
       "  506: 'running',\n",
       "  507: 'frank',\n",
       "  508: 'sounds',\n",
       "  509: 'lady',\n",
       "  510: 'fire',\n",
       "  511: 'ship',\n",
       "  512: 'asking',\n",
       "  513: 'dear',\n",
       "  514: 'behind',\n",
       "  515: 'seem',\n",
       "  516: 'hands',\n",
       "  517: 'goddamn',\n",
       "  518: 'high',\n",
       "  519: 'promise',\n",
       "  520: 'against',\n",
       "  521: 'body',\n",
       "  522: 'black',\n",
       "  523: 'harry',\n",
       "  524: 'blood',\n",
       "  525: 'perhaps',\n",
       "  526: 'scared',\n",
       "  527: 'hour',\n",
       "  528: 'dog',\n",
       "  529: 'bullshit',\n",
       "  530: 'months',\n",
       "  531: 'news',\n",
       "  532: 'safe',\n",
       "  533: 'line',\n",
       "  534: 'fun',\n",
       "  535: 'power',\n",
       "  536: 'feeling',\n",
       "  537: 'seems',\n",
       "  538: 'outside',\n",
       "  539: 'million',\n",
       "  540: 'hot',\n",
       "  541: 'figure',\n",
       "  542: 'lose',\n",
       "  543: 'full',\n",
       "  544: 'kidding',\n",
       "  545: 'brought',\n",
       "  546: 'sound',\n",
       "  547: 'somewhere',\n",
       "  548: 'weeks',\n",
       "  549: 'happens',\n",
       "  550: 'boys',\n",
       "  551: 'write',\n",
       "  552: 'city',\n",
       "  553: 'sense',\n",
       "  554: 'street',\n",
       "  555: 'glad',\n",
       "  556: 'bill',\n",
       "  557: 'cool',\n",
       "  558: 'fight',\n",
       "  559: 'sister',\n",
       "  560: 'sent',\n",
       "  561: 'started',\n",
       "  562: 'between',\n",
       "  563: 'fact',\n",
       "  564: 'girls',\n",
       "  565: 'goin',\n",
       "  566: 'living',\n",
       "  567: 'eight',\n",
       "  568: 'needs',\n",
       "  569: 'also',\n",
       "  570: 'shall',\n",
       "  571: 'movie',\n",
       "  572: 'save',\n",
       "  573: 'country',\n",
       "  574: 'light',\n",
       "  575: 'lucky',\n",
       "  576: 'free',\n",
       "  577: 'front',\n",
       "  578: 'calling',\n",
       "  579: 'leaving',\n",
       "  580: 'president',\n",
       "  581: \"c'mon\",\n",
       "  582: 'fifty',\n",
       "  583: 'lie',\n",
       "  584: 'sex',\n",
       "  585: 'wonderful',\n",
       "  586: 'tired',\n",
       "  587: 'himself',\n",
       "  588: 'moment',\n",
       "  589: 'plan',\n",
       "  590: 'possible',\n",
       "  591: 'fast',\n",
       "  592: 'picture',\n",
       "  593: 'questions',\n",
       "  594: 'mary',\n",
       "  595: 'coffee',\n",
       "  596: 'certainly',\n",
       "  597: 'luck',\n",
       "  598: 'daughter',\n",
       "  599: 'cold',\n",
       "  600: 'children',\n",
       "  601: 'till',\n",
       "  602: 'beat',\n",
       "  603: 'rather',\n",
       "  604: 'able',\n",
       "  605: 'bitch',\n",
       "  606: 'sam',\n",
       "  607: 'poor',\n",
       "  608: 'expect',\n",
       "  609: 'hair',\n",
       "  610: 'special',\n",
       "  611: 'mister',\n",
       "  612: 'pull',\n",
       "  613: 'child',\n",
       "  614: 'york',\n",
       "  615: 'touch',\n",
       "  616: 'playing',\n",
       "  617: 'thinks',\n",
       "  618: 'learn',\n",
       "  619: 'accident',\n",
       "  620: 'follow',\n",
       "  621: 'control',\n",
       "  622: 'worth',\n",
       "  623: 'hotel',\n",
       "  624: 'date',\n",
       "  625: 'ride',\n",
       "  626: 'food',\n",
       "  627: 'parents',\n",
       "  628: 'miles',\n",
       "  629: 'hospital',\n",
       "  630: 'goodbye',\n",
       "  631: 'looked',\n",
       "  632: 'absolutely',\n",
       "  633: 'company',\n",
       "  634: 'lives',\n",
       "  635: 'piece',\n",
       "  636: 'small',\n",
       "  637: 'perfect',\n",
       "  638: 'dream',\n",
       "  639: 'air',\n",
       "  640: 'catch',\n",
       "  641: 'words',\n",
       "  642: 'straight',\n",
       "  643: 'king',\n",
       "  644: 'cause',\n",
       "  645: 'order',\n",
       "  646: 'none',\n",
       "  647: 'uhhuh',\n",
       "  648: 'taken',\n",
       "  649: 'explain',\n",
       "  650: 'meeting',\n",
       "  651: 'interested',\n",
       "  652: 'doin',\n",
       "  653: 'works',\n",
       "  654: 'seven',\n",
       "  655: 'worse',\n",
       "  656: 'others',\n",
       "  657: 'worked',\n",
       "  658: 'tom',\n",
       "  659: 'outta',\n",
       "  660: 'mouth',\n",
       "  661: 'general',\n",
       "  662: 'charlie',\n",
       "  663: 'wonder',\n",
       "  664: 'act',\n",
       "  665: 'swear',\n",
       "  666: 'worried',\n",
       "  667: 'hang',\n",
       "  668: 'walter',\n",
       "  669: 'yesterday',\n",
       "  670: 'major',\n",
       "  671: 'talked',\n",
       "  672: 'known',\n",
       "  673: 'buddy',\n",
       "  674: 'david',\n",
       "  675: 'throw',\n",
       "  676: 'takes',\n",
       "  677: 'bob',\n",
       "  678: 'drop',\n",
       "  679: 'quit',\n",
       "  680: 'eh',\n",
       "  681: 'clear',\n",
       "  682: 'paul',\n",
       "  683: 'boss',\n",
       "  684: 'state',\n",
       "  685: 'handle',\n",
       "  686: 'wear',\n",
       "  687: 'sweet',\n",
       "  688: 'top',\n",
       "  689: 'ben',\n",
       "  690: 'clothes',\n",
       "  691: 'report',\n",
       "  692: 'hurry',\n",
       "  693: 'besides',\n",
       "  694: 'past',\n",
       "  695: 'joe',\n",
       "  696: 'michael',\n",
       "  697: 'unless',\n",
       "  698: 'nine',\n",
       "  699: 'human',\n",
       "  700: 'seeing',\n",
       "  701: 'except',\n",
       "  702: 'busy',\n",
       "  703: 'anywhere',\n",
       "  704: 'mad',\n",
       "  705: 'thirty',\n",
       "  706: 'sign',\n",
       "  707: 'careful',\n",
       "  708: 'dark',\n",
       "  709: 'fault',\n",
       "  710: 'lying',\n",
       "  711: 'music',\n",
       "  712: 'red',\n",
       "  713: 'clean',\n",
       "  714: 'finish',\n",
       "  715: 'meant',\n",
       "  716: 'mistake',\n",
       "  717: 'feet',\n",
       "  718: 'secret',\n",
       "  719: 'apartment',\n",
       "  720: 'difference',\n",
       "  721: 'cops',\n",
       "  722: 'less',\n",
       "  723: 'blow',\n",
       "  724: 'road',\n",
       "  725: 'choice',\n",
       "  726: 'strange',\n",
       "  727: 'nothin',\n",
       "  728: 'tv',\n",
       "  729: 'early',\n",
       "  730: 'marry',\n",
       "  731: 'darling',\n",
       "  732: 'terrible',\n",
       "  733: 'moving',\n",
       "  734: 'jimmy',\n",
       "  735: 'cop',\n",
       "  736: 'smart',\n",
       "  737: 'dude',\n",
       "  738: 'liked',\n",
       "  739: 'um',\n",
       "  740: 'nick',\n",
       "  741: 'lord',\n",
       "  742: 'peter',\n",
       "  743: 'lunch',\n",
       "  744: 'earth',\n",
       "  745: 'figured',\n",
       "  746: 'paid',\n",
       "  747: 'information',\n",
       "  748: 'fair',\n",
       "  749: 'fall',\n",
       "  750: 'bank',\n",
       "  751: 'class',\n",
       "  752: 'bucks',\n",
       "  753: \"ma'am\",\n",
       "  754: 'tough',\n",
       "  755: 'month',\n",
       "  756: 'changed',\n",
       "  757: 'interesting',\n",
       "  758: 'giving',\n",
       "  759: 'trip',\n",
       "  760: 'plane',\n",
       "  761: 'ray',\n",
       "  762: 'simple',\n",
       "  763: 'near',\n",
       "  764: 'broke',\n",
       "  765: 'quiet',\n",
       "  766: 'fucked',\n",
       "  767: 'sake',\n",
       "  768: 'jim',\n",
       "  769: 'favor',\n",
       "  770: 'fifteen',\n",
       "  771: 'felt',\n",
       "  772: 'evening',\n",
       "  773: 'relax',\n",
       "  774: 'ought',\n",
       "  775: 'store',\n",
       "  776: 'private',\n",
       "  777: 'ones',\n",
       "  778: 'caught',\n",
       "  779: 'building',\n",
       "  780: 'weird',\n",
       "  781: 'dick',\n",
       "  782: 'boat',\n",
       "  783: 'dance',\n",
       "  784: 'max',\n",
       "  785: 'personal',\n",
       "  786: 'happening',\n",
       "  787: 'calls',\n",
       "  788: 'wow',\n",
       "  789: 'watching',\n",
       "  790: 'twelve',\n",
       "  791: 'honest',\n",
       "  792: 'rich',\n",
       "  793: 'loved',\n",
       "  794: 'kinda',\n",
       "  795: 'win',\n",
       "  796: 'chief',\n",
       "  797: 'nervous',\n",
       "  798: 'rose',\n",
       "  799: 'forgot',\n",
       "  800: 'murder',\n",
       "  801: 'blue',\n",
       "  802: 'paper',\n",
       "  803: 'law',\n",
       "  804: 'longer',\n",
       "  805: 'eddie',\n",
       "  806: 'ice',\n",
       "  807: 'problems',\n",
       "  808: 'finished',\n",
       "  809: 'message',\n",
       "  810: 'dangerous',\n",
       "  811: 'system',\n",
       "  812: 'james',\n",
       "  813: 'likes',\n",
       "  814: 'officer',\n",
       "  815: 'asshole',\n",
       "  816: 'bastard',\n",
       "  817: 'turned',\n",
       "  818: \"o'clock\",\n",
       "  819: 'american',\n",
       "  820: 'johnny',\n",
       "  821: 'count',\n",
       "  822: 'jake',\n",
       "  823: 'fool',\n",
       "  824: 'welcome',\n",
       "  825: 'eye',\n",
       "  826: 'kiss',\n",
       "  827: 'missed',\n",
       "  828: 'stick',\n",
       "  829: 'mama',\n",
       "  830: 'army',\n",
       "  831: 'bother',\n",
       "  832: 'floor',\n",
       "  833: 'books',\n",
       "  834: 'owe',\n",
       "  835: 'sell',\n",
       "  836: 'staying',\n",
       "  837: 'record',\n",
       "  838: 'uncle',\n",
       "  839: 'killer',\n",
       "  840: 'agent',\n",
       "  841: 'involved',\n",
       "  842: 'upset',\n",
       "  843: 'hungry',\n",
       "  844: 'cash',\n",
       "  845: 'honor',\n",
       "  846: 'offer',\n",
       "  847: 'middle',\n",
       "  848: 'key',\n",
       "  849: 'english',\n",
       "  850: 'drunk',\n",
       "  851: 'imagine',\n",
       "  852: 'killing',\n",
       "  853: 'ha',\n",
       "  854: 'radio',\n",
       "  855: 'afternoon',\n",
       "  856: 'born',\n",
       "  857: 'completely',\n",
       "  858: 'given',\n",
       "  859: 'wake',\n",
       "  860: 'pleasure',\n",
       "  861: 'pictures',\n",
       "  862: 'tape',\n",
       "  863: 'list',\n",
       "  864: 'picked',\n",
       "  865: 'christmas',\n",
       "  866: 'become',\n",
       "  867: 'hardly',\n",
       "  868: 'calm',\n",
       "  869: 'future',\n",
       "  870: 'wearing',\n",
       "  871: 'brain',\n",
       "  872: 'totally',\n",
       "  873: 'security',\n",
       "  874: 'kept',\n",
       "  875: 'letter',\n",
       "  876: 'neither',\n",
       "  877: 'suit',\n",
       "  878: 'needed',\n",
       "  879: 'ring',\n",
       "  880: 'appreciate',\n",
       "  881: 'lawyer',\n",
       "  882: 'lots',\n",
       "  883: 'truck',\n",
       "  884: 'ma',\n",
       "  885: 'realize',\n",
       "  886: 'surprise',\n",
       "  887: 'dress',\n",
       "  888: 'doubt',\n",
       "  889: 'bought',\n",
       "  890: 'bag',\n",
       "  891: 'voice',\n",
       "  892: 'window',\n",
       "  893: 'somethin',\n",
       "  894: 'quick',\n",
       "  895: 'listening',\n",
       "  896: 'grand',\n",
       "  897: 'missing',\n",
       "  898: 'service',\n",
       "  899: 'table',\n",
       "  900: 'pain',\n",
       "  901: 'train',\n",
       "  902: 'fix',\n",
       "  903: 'sitting',\n",
       "  904: 'situation',\n",
       "  905: 'slow',\n",
       "  906: 'beer',\n",
       "  907: 'certain',\n",
       "  908: 'joke',\n",
       "  909: 'forgive',\n",
       "  910: 'club',\n",
       "  911: 'dying',\n",
       "  912: 'stopped',\n",
       "  913: 'college',\n",
       "  914: 'spend',\n",
       "  915: 'history',\n",
       "  916: 'jail',\n",
       "  917: 'named',\n",
       "  918: 'respect',\n",
       "  919: 'birthday',\n",
       "  920: 'seconds',\n",
       "  921: 'attack',\n",
       "  922: 'wrote',\n",
       "  923: 'la',\n",
       "  924: 'yah',\n",
       "  925: 'short',\n",
       "  926: 'station',\n",
       "  927: 'join',\n",
       "  928: 'team',\n",
       "  929: 'driving',\n",
       "  930: 'impossible',\n",
       "  931: 'cover',\n",
       "  932: 'enjoy',\n",
       "  933: 'pardon',\n",
       "  934: 'card',\n",
       "  935: 'charge',\n",
       "  936: 'court',\n",
       "  937: 'mike',\n",
       "  938: 'space',\n",
       "  939: 'fly',\n",
       "  940: 'decided',\n",
       "  941: 'island',\n",
       "  942: 'girlfriend',\n",
       "  943: 'carry',\n",
       "  944: 'third',\n",
       "  945: 'pass',\n",
       "  946: 'land',\n",
       "  947: 'french',\n",
       "  948: 'lieutenant',\n",
       "  949: 'gives',\n",
       "  950: 'prove',\n",
       "  951: 'movies',\n",
       "  952: 'age',\n",
       "  953: 'ran',\n",
       "  954: 'detective',\n",
       "  955: 'instead',\n",
       "  956: 'awful',\n",
       "  957: 'mark',\n",
       "  958: 'evil',\n",
       "  959: 'asleep',\n",
       "  960: 'smell',\n",
       "  961: 'reach',\n",
       "  962: 'present',\n",
       "  963: 'return',\n",
       "  964: 'talkin',\n",
       "  965: 'silly',\n",
       "  966: 'fat',\n",
       "  967: 'eve',\n",
       "  968: 'crime',\n",
       "  969: 'finally',\n",
       "  970: 'cost',\n",
       "  971: 'wedding',\n",
       "  972: 'deep',\n",
       "  973: 'usually',\n",
       "  974: 'bar',\n",
       "  975: 'saved',\n",
       "  976: 'church',\n",
       "  977: 'putting',\n",
       "  978: 'across',\n",
       "  979: 'gotten',\n",
       "  980: 'machine',\n",
       "  981: 'idiot',\n",
       "  982: 'plans',\n",
       "  983: 'reading',\n",
       "  984: 'prison',\n",
       "  985: 'computer',\n",
       "  986: 'fish',\n",
       "  987: 'papers',\n",
       "  988: 'park',\n",
       "  989: 'jerry',\n",
       "  990: 'holy',\n",
       "  991: 'moved',\n",
       "  992: 'address',\n",
       "  993: 'promised',\n",
       "  994: 'plenty',\n",
       "  995: 'twice',\n",
       "  996: 'dreams',\n",
       "  997: 'wondering',\n",
       "  998: 'star',\n",
       "  999: 'department',\n",
       "  ...})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', '<UNK>', '<UNK>', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', '<UNK>', 'public', 'break', 'up', 'on', 'the', '<UNK>', 'again']\n",
      "['well', 'i', 'thought', 'we', 'would', 'start', 'with', '<UNK>', 'if', 'that', 'okay', 'with', 'you']\n",
      "\n",
      "['well', 'i', 'thought', 'we', 'would', 'start', 'with', '<UNK>', 'if', 'that', 'okay', 'with', 'you']\n",
      "['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please']\n",
      "\n",
      "['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please']\n",
      "['okay', 'then', 'how', 'about', 'we', 'try', 'out', 'some', 'french', '<UNK>', 'saturday', 'night']\n",
      "\n",
      "['you', 'are', 'asking', 'me', 'out', 'that', 'so', 'cute', 'what', 'your', 'name', 'again']\n",
      "['forget', 'it']\n",
      "\n",
      "['no', 'no', 'it', 'my', 'fault', 'we', 'did', 'not', 'have', 'a', 'proper', 'introduction']\n",
      "['cameron']\n",
      "\n",
      "['cameron']\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "['seems', 'like', 'she', 'could', 'get', 'a', 'date', 'easy', 'enough']\n",
      "\n",
      "['why']\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "['that', 'a', 'shame']\n",
      "\n",
      "['gosh', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend']\n",
      "['let', 'me', 'see', 'what', 'i', 'can', 'do']\n",
      "\n",
      "['<UNK>', 'ma', '<UNK>', 'this', 'is', 'my', 'head']\n",
      "['right', 'see', 'you', 'are', 'ready', 'for', 'the', '<UNK>']\n",
      "\n",
      "['that', 'because', 'it', 'such', 'a', 'nice', 'one']\n",
      "['forget', 'french']\n",
      "\n",
      "['how', 'is', 'our', 'little', 'find', 'the', '<UNK>', 'a', 'date', 'plan', '<UNK>']\n",
      "['well', 'there', 'someone', 'i', 'think', 'might', 'be']\n",
      "\n",
      "['there']\n",
      "['where']\n",
      "\n",
      "['you', 'got', 'something', 'on', 'your', 'mind']\n",
      "['i', 'counted', 'on', 'you', 'to', 'help', 'my', 'cause', 'you', 'and', 'that', '<UNK>', 'are', 'obviously', 'failing', 'are', 'not', 'we', 'ever', 'going', 'on', 'our', 'date']\n",
      "\n",
      "['you', 'have', 'my', 'word', 'as', 'a', 'gentleman']\n",
      "['you', 'are', 'sweet']\n",
      "\n",
      "['how', 'do', 'you', 'get', 'your', 'hair', 'to', 'look', 'like', 'that']\n",
      "['<UNK>', 'deep', '<UNK>', 'every', 'two', 'days', 'and', 'i', 'never', 'ever', 'use', 'a', '<UNK>', 'without', 'the', '<UNK>', '<UNK>']\n",
      "\n",
      "['sure', 'have']\n",
      "['i', 'really', 'really', 'really', 'wan', 'na', 'go', 'but', 'i', 'ca', 'not', 'not', 'unless', 'my', 'sister', 'goes']\n",
      "\n",
      "['i', 'really', 'really', 'really', 'wan', 'na', 'go', 'but', 'i', 'ca', 'not', 'not', 'unless', 'my', 'sister', 'goes']\n",
      "['i', 'workin', 'on', 'it', 'but', 'she', 'does', 'not', 'seem', 'to', 'be', 'goin', 'for', 'him']\n",
      "\n",
      "['she', 'not', 'a']\n",
      "['lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<UNK>', '<UNK>', 'in', 'one', 'of', 'her', '<UNK>', 'so', 'i', 'pretty', 'sure', 'she', 'not', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "['lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<UNK>', '<UNK>', 'in', 'one', 'of', 'her', '<UNK>', 'so', 'i', 'pretty', 'sure', 'she', 'not', '<UNK>', '<UNK>', '<UNK>']\n",
      "['so', 'that', 'the', 'kind', 'of', 'guy', 'she', 'likes', 'pretty', 'ones']\n",
      "\n",
      "['so', 'that', 'the', 'kind', 'of', 'guy', 'she', 'likes', 'pretty', 'ones']\n",
      "['who', 'knows', 'all', 'i', 'have', 'ever', 'heard', 'her', 'say', 'is', 'that', 'she', 'would', 'dip', 'before', 'dating', 'a', 'guy', 'that', 'smokes']\n",
      "\n",
      "['hi']\n",
      "['looks', 'like', 'things', 'worked', 'out', 'tonight', 'huh']\n",
      "\n",
      "['you', 'know', '<UNK>']\n",
      "['i', 'believe', 'we', 'share', 'an', 'art', '<UNK>']\n",
      "\n",
      "['have', 'fun', 'tonight']\n",
      "['tons']\n",
      "\n",
      "['i', 'looked', 'for', 'you', 'back', 'at', 'the', 'party', 'but', 'you', 'always', 'seemed', 'to', 'be', 'occupied']\n",
      "['i', 'was']\n",
      "\n",
      "['i', 'was']\n",
      "['you', 'never', 'wanted', 'to', 'go', 'out', 'with', 'did', 'you']\n",
      "\n",
      "['well', 'no']\n",
      "['then', 'that', 'all', 'you', 'had', 'to', 'say']\n",
      "\n",
      "['then', 'that', 'all', 'you', 'had', 'to', 'say']\n",
      "['but']\n",
      "\n",
      "['but']\n",
      "['you', 'always', 'been', 'this', 'selfish']\n",
      "\n",
      "['then', '<UNK>', 'says', 'if', 'you', 'go', 'any', 'lighter', 'you', 'are', 'gon', 'na', 'look', 'like', 'an', 'extra', 'on']\n",
      "['no']\n",
      "\n",
      "['do', 'you', 'listen', 'to', 'this', 'crap']\n",
      "['what', 'crap']\n",
      "\n",
      "['what', 'crap']\n",
      "['me', 'this', '<UNK>', 'blonde', '<UNK>', 'i', 'like', 'boring', 'myself']\n",
      "\n",
      "['me', 'this', '<UNK>', 'blonde', '<UNK>', 'i', 'like', 'boring', 'myself']\n",
      "['thank', 'god', 'if', 'i', 'had', 'to', 'hear', 'one', 'more', 'story', 'about', 'your', '<UNK>']\n",
      "\n",
      "['i', 'figured', 'you', 'would', 'get', 'to', 'the', 'good', 'stuff', 'eventually']\n",
      "['what', 'good', 'stuff']\n",
      "\n",
      "['what', 'good', 'stuff']\n",
      "['the', 'real', 'you']\n",
      "\n",
      "['the', 'real', 'you']\n",
      "['like', 'my', 'fear', 'of', 'wearing', '<UNK>']\n",
      "\n",
      "['i', 'kidding', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '<UNK>', 'and', 'you', 'do', 'not', 'know', 'how', 'to', 'quit']\n",
      "['no']\n",
      "\n",
      "['no']\n",
      "['okay', 'you', 'are', 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie']\n",
      "\n",
      "['wow']\n",
      "['let', 'go']\n",
      "\n",
      "['she', 'okay']\n",
      "['i', 'hope', 'so']\n",
      "\n",
      "['they', 'do', 'to']\n",
      "['they', 'do', 'not']\n",
      "\n",
      "['did', 'you', 'change', 'your', 'hair']\n",
      "['no']\n",
      "\n",
      "['no']\n",
      "['you', 'might', 'wan', 'na', 'think', 'about', 'it']\n",
      "\n",
      "['where', 'did', 'he', 'go', 'he', 'was', 'just', 'here']\n",
      "['who']\n",
      "\n",
      "['who']\n",
      "['joey']\n",
      "\n",
      "['great']\n",
      "['would', 'you', 'mind', 'getting', 'me', 'a', 'drink', 'cameron']\n",
      "\n",
      "['he', 'practically', 'proposed', 'when', 'he', 'found', 'out', 'we', 'had', 'the', 'same', '<UNK>', 'i', 'mean', 'dr', '<UNK>', 'is', 'great', 'an', 'all', 'but', 'he', 'not', 'exactly', 'relevant', 'party', 'conversation']\n",
      "['is', 'he', '<UNK>', 'or', 'dry']\n",
      "\n",
      "['is', 'he', '<UNK>', 'or', 'dry']\n",
      "['combination', 'i', 'do', 'not', 'know', 'i', 'thought', 'he', 'would', 'be', 'different', 'more', 'of', 'a', 'gentleman']\n",
      "\n",
      "['bianca', 'i', 'do', 'not', 'think', 'the', '<UNK>', 'of', 'dating', 'joey', '<UNK>', 'are', 'going', 'to', 'include', '<UNK>', 'and', '<UNK>']\n",
      "['sometimes', 'i', 'wonder', 'if', 'the', 'guys', 'we', 'are', 'supposed', 'to', 'want', 'to', 'go', 'out', 'with', 'are', 'the', 'ones', 'we', 'actually', 'want', 'to', 'go', 'out', 'with', 'you', 'know']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(questions_text[i])\n",
    "    print(answers_text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(sequence, int2vocab):\n",
    "    return [int2vocab[index] for index in sequence if index != METATOKEN_INDEX]\n",
    "\n",
    "def text_to_int(sequence, vocab2int):\n",
    "    return [vocab2int.get(token, vocab2int[UNK]) for token in sequence if token not in codes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_corpus=[]\n",
    "combined_corpus.extend(questions_text)\n",
    "combined_corpus.extend(answers_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393982"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can',\n",
       "  'we',\n",
       "  'make',\n",
       "  'this',\n",
       "  'quick',\n",
       "  '<UNK>',\n",
       "  '<UNK>',\n",
       "  'and',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'are',\n",
       "  'having',\n",
       "  'an',\n",
       "  'incredibly',\n",
       "  '<UNK>',\n",
       "  'public',\n",
       "  'break',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  '<UNK>',\n",
       "  'again'],\n",
       " ['well',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'we',\n",
       "  'would',\n",
       "  'start',\n",
       "  'with',\n",
       "  '<UNK>',\n",
       "  'if',\n",
       "  'that',\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you'],\n",
       " ['not', 'the', 'hacking', 'and', '<UNK>', 'and', '<UNK>', 'part', 'please'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'asking',\n",
       "  'me',\n",
       "  'out',\n",
       "  'that',\n",
       "  'so',\n",
       "  'cute',\n",
       "  'what',\n",
       "  'your',\n",
       "  'name',\n",
       "  'again'],\n",
       " ['no',\n",
       "  'no',\n",
       "  'it',\n",
       "  'my',\n",
       "  'fault',\n",
       "  'we',\n",
       "  'did',\n",
       "  'not',\n",
       "  'have',\n",
       "  'a',\n",
       "  'proper',\n",
       "  'introduction']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 1024\n",
    "model = Word2Vec(sentences=combined_corpus, size=embedding_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['well'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'i',\n",
       " '<UNK>',\n",
       " 'the',\n",
       " 'not',\n",
       " 'to',\n",
       " 'it',\n",
       " 'a',\n",
       " 'do',\n",
       " 'that',\n",
       " 'what',\n",
       " 'are',\n",
       " 'and',\n",
       " 'of',\n",
       " 'have',\n",
       " 'me',\n",
       " 'is',\n",
       " 'in',\n",
       " 'we',\n",
       " 'he',\n",
       " 'this',\n",
       " 'know',\n",
       " 'no',\n",
       " 'for',\n",
       " 'your',\n",
       " 'will',\n",
       " 'was',\n",
       " 'my',\n",
       " 'be',\n",
       " 'on',\n",
       " 'just',\n",
       " 'did',\n",
       " 'with',\n",
       " 'but',\n",
       " 'would',\n",
       " 'they',\n",
       " 'like',\n",
       " 'about',\n",
       " 'there',\n",
       " 'all',\n",
       " 'get',\n",
       " 'here',\n",
       " 'got',\n",
       " 'so',\n",
       " 'how',\n",
       " 'she',\n",
       " 'out',\n",
       " 'if',\n",
       " 'him',\n",
       " 'want',\n",
       " 'can',\n",
       " 'think',\n",
       " 'up',\n",
       " 'well',\n",
       " 'right',\n",
       " 'why',\n",
       " 'go',\n",
       " 'at',\n",
       " 'one',\n",
       " 'yes',\n",
       " 'now',\n",
       " 'oh',\n",
       " 'yeah',\n",
       " 'going',\n",
       " 'her',\n",
       " 'who',\n",
       " 'see',\n",
       " 'where',\n",
       " 'good',\n",
       " 'tell',\n",
       " 'could',\n",
       " 'come',\n",
       " 'ca',\n",
       " 'were',\n",
       " 'as',\n",
       " 'an',\n",
       " 'u',\n",
       " 'when',\n",
       " 'from',\n",
       " 'say',\n",
       " 'been',\n",
       " 'time',\n",
       " 'his',\n",
       " 'some',\n",
       " 'let',\n",
       " 'back',\n",
       " 'or',\n",
       " 'look',\n",
       " 'then',\n",
       " 'them',\n",
       " 'something',\n",
       " 'mean',\n",
       " 'take',\n",
       " 'us',\n",
       " 'man',\n",
       " 'never',\n",
       " 'had',\n",
       " 'okay',\n",
       " 'does',\n",
       " 'too',\n",
       " 'sure',\n",
       " 'na',\n",
       " 'really',\n",
       " 'way',\n",
       " 'make',\n",
       " 'should',\n",
       " 'said',\n",
       " 'any',\n",
       " 'down',\n",
       " 'more',\n",
       " 'little',\n",
       " 'need',\n",
       " 'maybe',\n",
       " 'gon',\n",
       " 'very',\n",
       " 'over',\n",
       " 'off',\n",
       " 'thing',\n",
       " 'only',\n",
       " 'mr',\n",
       " 'sorry',\n",
       " 'much',\n",
       " 'has',\n",
       " 'am',\n",
       " 'people',\n",
       " 'anything',\n",
       " 'by',\n",
       " 'two',\n",
       " 'our',\n",
       " 'sir',\n",
       " 'talk',\n",
       " 'nothing',\n",
       " 'give',\n",
       " 'doing',\n",
       " 'thought',\n",
       " 'call',\n",
       " 'love',\n",
       " 'because',\n",
       " 'told',\n",
       " 'still',\n",
       " 'must',\n",
       " 'work',\n",
       " 'better',\n",
       " 'find',\n",
       " 'ever',\n",
       " 'before',\n",
       " 'please',\n",
       " 'night',\n",
       " 'even',\n",
       " 'hey',\n",
       " 'money',\n",
       " 'help',\n",
       " 'long',\n",
       " 'wo',\n",
       " 'around',\n",
       " 'god',\n",
       " 'name',\n",
       " 'last',\n",
       " 'first',\n",
       " 'guy',\n",
       " 'believe',\n",
       " 'into',\n",
       " 'other',\n",
       " 'these',\n",
       " 'things',\n",
       " 'always',\n",
       " 'than',\n",
       " 'life',\n",
       " 'those',\n",
       " 'hell',\n",
       " 'shit',\n",
       " 'again',\n",
       " 'away',\n",
       " 'put',\n",
       " 'old',\n",
       " 'home',\n",
       " 'after',\n",
       " 'thank',\n",
       " 'great',\n",
       " 'place',\n",
       " 'keep',\n",
       " 'course',\n",
       " 'everything',\n",
       " 'guess',\n",
       " 'new',\n",
       " 'feel',\n",
       " 'fuck',\n",
       " 'talking',\n",
       " 'day',\n",
       " 'fine',\n",
       " 'bad',\n",
       " 'wait',\n",
       " 'kind',\n",
       " 'years',\n",
       " 'big',\n",
       " 'remember',\n",
       " 'uh',\n",
       " 'dead',\n",
       " 'leave',\n",
       " 'wrong',\n",
       " 'kill',\n",
       " 'huh',\n",
       " 'ask',\n",
       " 'else',\n",
       " 'nice',\n",
       " 'ai',\n",
       " 'might',\n",
       " 'hear',\n",
       " 'happened',\n",
       " 'girl',\n",
       " 'understand',\n",
       " 'lot',\n",
       " 'stop',\n",
       " 'three',\n",
       " 'ta',\n",
       " 'thanks',\n",
       " 'mind',\n",
       " 'enough',\n",
       " 'wanted',\n",
       " 'father',\n",
       " 'yourself',\n",
       " 'made',\n",
       " 'their',\n",
       " 'listen',\n",
       " 'real',\n",
       " 'someone',\n",
       " 'stay',\n",
       " 'done',\n",
       " 'getting',\n",
       " 'fucking',\n",
       " 'mother',\n",
       " 'another',\n",
       " 'house',\n",
       " 'car',\n",
       " 'try',\n",
       " 'tonight',\n",
       " 'left',\n",
       " 'heard',\n",
       " 'saw',\n",
       " 'every',\n",
       " 'care',\n",
       " 'coming',\n",
       " 'job',\n",
       " 'trying',\n",
       " 'friend',\n",
       " 'through',\n",
       " 'came',\n",
       " 'seen',\n",
       " 'boy',\n",
       " 'looking',\n",
       " 'pretty',\n",
       " 'knew',\n",
       " 'may',\n",
       " 'room',\n",
       " 'being',\n",
       " 'ya',\n",
       " 'own',\n",
       " 'miss',\n",
       " 'same',\n",
       " 'went',\n",
       " 'guys',\n",
       " 'tomorrow',\n",
       " 'idea',\n",
       " 'show',\n",
       " 'world',\n",
       " 'killed',\n",
       " 'business',\n",
       " 'matter',\n",
       " 'live',\n",
       " 'many',\n",
       " 'morning',\n",
       " 'best',\n",
       " 'yet',\n",
       " 'called',\n",
       " 'dad',\n",
       " 'already',\n",
       " 'next',\n",
       " 'five',\n",
       " 'used',\n",
       " 'hello',\n",
       " 'whole',\n",
       " 'found',\n",
       " 'today',\n",
       " 'baby',\n",
       " 'says',\n",
       " 'which',\n",
       " 'stuff',\n",
       " 'run',\n",
       " 'wife',\n",
       " 'use',\n",
       " 'play',\n",
       " 'meet',\n",
       " 'saying',\n",
       " 'woman',\n",
       " 'few',\n",
       " 'probably',\n",
       " 'without',\n",
       " 'minute',\n",
       " 'myself',\n",
       " 'start',\n",
       " 'alone',\n",
       " 'hi',\n",
       " 'ago',\n",
       " 'once',\n",
       " 'somebody',\n",
       " 'together',\n",
       " 'crazy',\n",
       " 'hundred',\n",
       " 'problem',\n",
       " 'exactly',\n",
       " 'afraid',\n",
       " 'school',\n",
       " 'while',\n",
       " 'days',\n",
       " 'son',\n",
       " 'four',\n",
       " 'jesus',\n",
       " 'nobody',\n",
       " 'men',\n",
       " 'damn',\n",
       " 'gone',\n",
       " 'worry',\n",
       " 'head',\n",
       " 'hard',\n",
       " 'forget',\n",
       " 'looks',\n",
       " 'friends',\n",
       " 'hope',\n",
       " 'took',\n",
       " 'mrs',\n",
       " 'since',\n",
       " 'wants',\n",
       " 'until',\n",
       " 'ten',\n",
       " 'mom',\n",
       " 'kid',\n",
       " 'anyway',\n",
       " 'doctor',\n",
       " 'most',\n",
       " 'anyone',\n",
       " 'such',\n",
       " 'alright',\n",
       " 'wan',\n",
       " 'minutes',\n",
       " 'shut',\n",
       " 'read',\n",
       " 'supposed',\n",
       " 'bring',\n",
       " 'deal',\n",
       " 'true',\n",
       " 'late',\n",
       " 'case',\n",
       " 'hold',\n",
       " 'die',\n",
       " 'point',\n",
       " 'actually',\n",
       " 'word',\n",
       " 'watch',\n",
       " 'hurt',\n",
       " 'drink',\n",
       " 'ready',\n",
       " 'sleep',\n",
       " 'knows',\n",
       " 'working',\n",
       " 'makes',\n",
       " 'brother',\n",
       " 'jack',\n",
       " 'thinking',\n",
       " 'lost',\n",
       " 'week',\n",
       " 'part',\n",
       " 'open',\n",
       " 'dr',\n",
       " 'soon',\n",
       " 'beautiful',\n",
       " 'easy',\n",
       " 'john',\n",
       " 'happen',\n",
       " 'married',\n",
       " 'story',\n",
       " 'question',\n",
       " 'move',\n",
       " 'turn',\n",
       " 'phone',\n",
       " 'asked',\n",
       " 'sit',\n",
       " 'under',\n",
       " 'everybody',\n",
       " 'year',\n",
       " 'whatever',\n",
       " 'having',\n",
       " 'pay',\n",
       " 'ass',\n",
       " 'least',\n",
       " 'hate',\n",
       " 'kids',\n",
       " 'far',\n",
       " 'later',\n",
       " 'quite',\n",
       " 'family',\n",
       " 'both',\n",
       " 'six',\n",
       " 'happy',\n",
       " 'cut',\n",
       " 'wish',\n",
       " 'trouble',\n",
       " 'gave',\n",
       " 'anybody',\n",
       " 'eat',\n",
       " 'check',\n",
       " 'taking',\n",
       " 'mine',\n",
       " 'times',\n",
       " 'shot',\n",
       " 'face',\n",
       " 'door',\n",
       " 'trust',\n",
       " 'hit',\n",
       " 'half',\n",
       " 'thousand',\n",
       " 'town',\n",
       " 'funny',\n",
       " 'yours',\n",
       " 'police',\n",
       " 'suppose',\n",
       " 'ok',\n",
       " 'change',\n",
       " 'couple',\n",
       " 'dollars',\n",
       " 'captain',\n",
       " 'bet',\n",
       " 'chance',\n",
       " 'end',\n",
       " 'honey',\n",
       " 'important',\n",
       " 'different',\n",
       " 'met',\n",
       " 'second',\n",
       " 'gun',\n",
       " 'young',\n",
       " 'fuckin',\n",
       " 'answer',\n",
       " 'excuse',\n",
       " 'anymore',\n",
       " 'walk',\n",
       " 'each',\n",
       " 'hours',\n",
       " 'telling',\n",
       " 'sometimes',\n",
       " 'rest',\n",
       " 'making',\n",
       " 'bit',\n",
       " 'everyone',\n",
       " 'ah',\n",
       " 'hand',\n",
       " 'gets',\n",
       " 'close',\n",
       " 'party',\n",
       " 'death',\n",
       " 'truth',\n",
       " 'set',\n",
       " 'person',\n",
       " 'either',\n",
       " 'inside',\n",
       " 'tried',\n",
       " 'number',\n",
       " 'bed',\n",
       " 'sick',\n",
       " 'heart',\n",
       " 'sort',\n",
       " 'christ',\n",
       " 'game',\n",
       " 'serious',\n",
       " 'means',\n",
       " 'reason',\n",
       " 'waiting',\n",
       " 'side',\n",
       " 'office',\n",
       " 'almost',\n",
       " 'send',\n",
       " 'buy',\n",
       " 'eyes',\n",
       " 'daddy',\n",
       " 'drive',\n",
       " 'though',\n",
       " 'water',\n",
       " 'war',\n",
       " 'break',\n",
       " 'stupid',\n",
       " 'its',\n",
       " 'pick',\n",
       " 'comes',\n",
       " 'died',\n",
       " 'white',\n",
       " 'goes',\n",
       " 'stand',\n",
       " 'dinner',\n",
       " 'book',\n",
       " 'shoot',\n",
       " 'speak',\n",
       " 'husband',\n",
       " 'alive',\n",
       " 'along',\n",
       " 'ahead',\n",
       " 'twenty',\n",
       " 'women',\n",
       " 'george',\n",
       " 'running',\n",
       " 'frank',\n",
       " 'sounds',\n",
       " 'lady',\n",
       " 'fire',\n",
       " 'ship',\n",
       " 'asking',\n",
       " 'dear',\n",
       " 'behind',\n",
       " 'seem',\n",
       " 'hands',\n",
       " 'goddamn',\n",
       " 'high',\n",
       " 'promise',\n",
       " 'against',\n",
       " 'body',\n",
       " 'black',\n",
       " 'harry',\n",
       " 'blood',\n",
       " 'perhaps',\n",
       " 'scared',\n",
       " 'hour',\n",
       " 'dog',\n",
       " 'bullshit',\n",
       " 'months',\n",
       " 'news',\n",
       " 'safe',\n",
       " 'line',\n",
       " 'fun',\n",
       " 'power',\n",
       " 'feeling',\n",
       " 'seems',\n",
       " 'outside',\n",
       " 'million',\n",
       " 'hot',\n",
       " 'figure',\n",
       " 'lose',\n",
       " 'full',\n",
       " 'kidding',\n",
       " 'brought',\n",
       " 'sound',\n",
       " 'somewhere',\n",
       " 'weeks',\n",
       " 'happens',\n",
       " 'boys',\n",
       " 'write',\n",
       " 'city',\n",
       " 'sense',\n",
       " 'street',\n",
       " 'glad',\n",
       " 'bill',\n",
       " 'cool',\n",
       " 'fight',\n",
       " 'sister',\n",
       " 'sent',\n",
       " 'started',\n",
       " 'between',\n",
       " 'fact',\n",
       " 'girls',\n",
       " 'goin',\n",
       " 'living',\n",
       " 'eight',\n",
       " 'needs',\n",
       " 'also',\n",
       " 'shall',\n",
       " 'movie',\n",
       " 'save',\n",
       " 'country',\n",
       " 'light',\n",
       " 'lucky',\n",
       " 'free',\n",
       " 'front',\n",
       " 'calling',\n",
       " 'leaving',\n",
       " 'president',\n",
       " \"c'mon\",\n",
       " 'fifty',\n",
       " 'lie',\n",
       " 'sex',\n",
       " 'wonderful',\n",
       " 'tired',\n",
       " 'himself',\n",
       " 'moment',\n",
       " 'plan',\n",
       " 'possible',\n",
       " 'fast',\n",
       " 'picture',\n",
       " 'questions',\n",
       " 'mary',\n",
       " 'coffee',\n",
       " 'certainly',\n",
       " 'luck',\n",
       " 'daughter',\n",
       " 'cold',\n",
       " 'children',\n",
       " 'till',\n",
       " 'beat',\n",
       " 'rather',\n",
       " 'able',\n",
       " 'bitch',\n",
       " 'sam',\n",
       " 'poor',\n",
       " 'expect',\n",
       " 'hair',\n",
       " 'special',\n",
       " 'mister',\n",
       " 'pull',\n",
       " 'child',\n",
       " 'york',\n",
       " 'touch',\n",
       " 'playing',\n",
       " 'thinks',\n",
       " 'learn',\n",
       " 'accident',\n",
       " 'follow',\n",
       " 'control',\n",
       " 'worth',\n",
       " 'hotel',\n",
       " 'date',\n",
       " 'ride',\n",
       " 'food',\n",
       " 'parents',\n",
       " 'miles',\n",
       " 'hospital',\n",
       " 'goodbye',\n",
       " 'looked',\n",
       " 'absolutely',\n",
       " 'company',\n",
       " 'lives',\n",
       " 'piece',\n",
       " 'small',\n",
       " 'perfect',\n",
       " 'dream',\n",
       " 'air',\n",
       " 'catch',\n",
       " 'words',\n",
       " 'straight',\n",
       " 'king',\n",
       " 'cause',\n",
       " 'order',\n",
       " 'none',\n",
       " 'uhhuh',\n",
       " 'taken',\n",
       " 'explain',\n",
       " 'meeting',\n",
       " 'interested',\n",
       " 'doin',\n",
       " 'works',\n",
       " 'seven',\n",
       " 'worse',\n",
       " 'others',\n",
       " 'worked',\n",
       " 'tom',\n",
       " 'outta',\n",
       " 'mouth',\n",
       " 'general',\n",
       " 'charlie',\n",
       " 'wonder',\n",
       " 'act',\n",
       " 'swear',\n",
       " 'worried',\n",
       " 'hang',\n",
       " 'walter',\n",
       " 'yesterday',\n",
       " 'major',\n",
       " 'talked',\n",
       " 'known',\n",
       " 'buddy',\n",
       " 'david',\n",
       " 'throw',\n",
       " 'takes',\n",
       " 'bob',\n",
       " 'drop',\n",
       " 'quit',\n",
       " 'eh',\n",
       " 'clear',\n",
       " 'paul',\n",
       " 'boss',\n",
       " 'state',\n",
       " 'handle',\n",
       " 'wear',\n",
       " 'sweet',\n",
       " 'top',\n",
       " 'ben',\n",
       " 'clothes',\n",
       " 'report',\n",
       " 'hurry',\n",
       " 'besides',\n",
       " 'past',\n",
       " 'joe',\n",
       " 'michael',\n",
       " 'unless',\n",
       " 'nine',\n",
       " 'human',\n",
       " 'seeing',\n",
       " 'except',\n",
       " 'busy',\n",
       " 'anywhere',\n",
       " 'mad',\n",
       " 'thirty',\n",
       " 'sign',\n",
       " 'careful',\n",
       " 'dark',\n",
       " 'fault',\n",
       " 'lying',\n",
       " 'music',\n",
       " 'red',\n",
       " 'clean',\n",
       " 'finish',\n",
       " 'meant',\n",
       " 'mistake',\n",
       " 'feet',\n",
       " 'secret',\n",
       " 'apartment',\n",
       " 'difference',\n",
       " 'cops',\n",
       " 'less',\n",
       " 'blow',\n",
       " 'road',\n",
       " 'choice',\n",
       " 'strange',\n",
       " 'nothin',\n",
       " 'tv',\n",
       " 'early',\n",
       " 'marry',\n",
       " 'darling',\n",
       " 'terrible',\n",
       " 'moving',\n",
       " 'jimmy',\n",
       " 'cop',\n",
       " 'smart',\n",
       " 'dude',\n",
       " 'liked',\n",
       " 'um',\n",
       " 'nick',\n",
       " 'lord',\n",
       " 'peter',\n",
       " 'lunch',\n",
       " 'earth',\n",
       " 'figured',\n",
       " 'paid',\n",
       " 'information',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'bank',\n",
       " 'class',\n",
       " 'bucks',\n",
       " \"ma'am\",\n",
       " 'tough',\n",
       " 'month',\n",
       " 'changed',\n",
       " 'interesting',\n",
       " 'giving',\n",
       " 'trip',\n",
       " 'plane',\n",
       " 'ray',\n",
       " 'simple',\n",
       " 'near',\n",
       " 'broke',\n",
       " 'quiet',\n",
       " 'fucked',\n",
       " 'sake',\n",
       " 'jim',\n",
       " 'favor',\n",
       " 'fifteen',\n",
       " 'felt',\n",
       " 'evening',\n",
       " 'relax',\n",
       " 'ought',\n",
       " 'store',\n",
       " 'private',\n",
       " 'ones',\n",
       " 'caught',\n",
       " 'building',\n",
       " 'weird',\n",
       " 'dick',\n",
       " 'boat',\n",
       " 'dance',\n",
       " 'max',\n",
       " 'personal',\n",
       " 'happening',\n",
       " 'calls',\n",
       " 'wow',\n",
       " 'watching',\n",
       " 'twelve',\n",
       " 'honest',\n",
       " 'rich',\n",
       " 'loved',\n",
       " 'kinda',\n",
       " 'win',\n",
       " 'chief',\n",
       " 'nervous',\n",
       " 'rose',\n",
       " 'forgot',\n",
       " 'murder',\n",
       " 'blue',\n",
       " 'paper',\n",
       " 'law',\n",
       " 'longer',\n",
       " 'eddie',\n",
       " 'ice',\n",
       " 'problems',\n",
       " 'finished',\n",
       " 'message',\n",
       " 'dangerous',\n",
       " 'system',\n",
       " 'james',\n",
       " 'likes',\n",
       " 'officer',\n",
       " 'asshole',\n",
       " 'bastard',\n",
       " 'turned',\n",
       " \"o'clock\",\n",
       " 'american',\n",
       " 'johnny',\n",
       " 'count',\n",
       " 'jake',\n",
       " 'fool',\n",
       " 'welcome',\n",
       " 'eye',\n",
       " 'kiss',\n",
       " 'missed',\n",
       " 'stick',\n",
       " 'mama',\n",
       " 'army',\n",
       " 'bother',\n",
       " 'floor',\n",
       " 'books',\n",
       " 'owe',\n",
       " 'sell',\n",
       " 'staying',\n",
       " 'record',\n",
       " 'uncle',\n",
       " 'killer',\n",
       " 'agent',\n",
       " 'involved',\n",
       " 'upset',\n",
       " 'hungry',\n",
       " 'cash',\n",
       " 'honor',\n",
       " 'offer',\n",
       " 'middle',\n",
       " 'key',\n",
       " 'english',\n",
       " 'drunk',\n",
       " 'imagine',\n",
       " 'killing',\n",
       " 'ha',\n",
       " 'radio',\n",
       " 'afternoon',\n",
       " 'born',\n",
       " 'completely',\n",
       " 'given',\n",
       " 'wake',\n",
       " 'pleasure',\n",
       " 'pictures',\n",
       " 'tape',\n",
       " 'list',\n",
       " 'picked',\n",
       " 'christmas',\n",
       " 'become',\n",
       " 'hardly',\n",
       " 'calm',\n",
       " 'future',\n",
       " 'wearing',\n",
       " 'brain',\n",
       " 'totally',\n",
       " 'security',\n",
       " 'kept',\n",
       " 'letter',\n",
       " 'neither',\n",
       " 'suit',\n",
       " 'needed',\n",
       " 'ring',\n",
       " 'appreciate',\n",
       " 'lawyer',\n",
       " 'lots',\n",
       " 'truck',\n",
       " 'ma',\n",
       " 'realize',\n",
       " 'surprise',\n",
       " 'dress',\n",
       " 'doubt',\n",
       " 'bought',\n",
       " 'bag',\n",
       " 'voice',\n",
       " 'window',\n",
       " 'somethin',\n",
       " 'quick',\n",
       " 'listening',\n",
       " 'grand',\n",
       " 'missing',\n",
       " 'service',\n",
       " 'table',\n",
       " 'pain',\n",
       " 'train',\n",
       " 'fix',\n",
       " 'sitting',\n",
       " 'situation',\n",
       " 'slow',\n",
       " 'beer',\n",
       " 'certain',\n",
       " 'joke',\n",
       " 'forgive',\n",
       " 'club',\n",
       " 'dying',\n",
       " 'stopped',\n",
       " 'college',\n",
       " 'spend',\n",
       " 'history',\n",
       " 'jail',\n",
       " 'named',\n",
       " 'respect',\n",
       " 'birthday',\n",
       " 'seconds',\n",
       " 'attack',\n",
       " 'wrote',\n",
       " 'la',\n",
       " 'yah',\n",
       " 'short',\n",
       " 'station',\n",
       " 'join',\n",
       " 'team',\n",
       " 'driving',\n",
       " 'impossible',\n",
       " 'cover',\n",
       " 'enjoy',\n",
       " 'pardon',\n",
       " 'card',\n",
       " 'charge',\n",
       " 'court',\n",
       " 'mike',\n",
       " 'space',\n",
       " 'fly',\n",
       " 'decided',\n",
       " 'island',\n",
       " 'girlfriend',\n",
       " 'carry',\n",
       " 'third',\n",
       " 'pass',\n",
       " 'land',\n",
       " 'french',\n",
       " 'lieutenant',\n",
       " 'gives',\n",
       " 'prove',\n",
       " 'movies',\n",
       " 'age',\n",
       " 'ran',\n",
       " 'detective',\n",
       " 'instead',\n",
       " 'awful',\n",
       " 'mark',\n",
       " 'evil',\n",
       " 'asleep',\n",
       " 'smell',\n",
       " 'reach',\n",
       " 'present',\n",
       " 'return',\n",
       " 'talkin',\n",
       " 'silly',\n",
       " 'fat',\n",
       " 'eve',\n",
       " 'crime',\n",
       " 'finally',\n",
       " 'cost',\n",
       " 'wedding',\n",
       " 'deep',\n",
       " 'usually',\n",
       " 'bar',\n",
       " 'saved',\n",
       " 'church',\n",
       " 'putting',\n",
       " 'across',\n",
       " 'gotten',\n",
       " 'machine',\n",
       " 'idiot',\n",
       " 'plans',\n",
       " 'reading',\n",
       " 'prison',\n",
       " 'computer',\n",
       " 'fish',\n",
       " 'papers',\n",
       " 'park',\n",
       " 'jerry',\n",
       " 'holy',\n",
       " 'moved',\n",
       " 'address',\n",
       " 'promised',\n",
       " 'plenty',\n",
       " 'twice',\n",
       " 'dreams',\n",
       " 'wondering',\n",
       " 'star',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.zeros((len(model.wv.vocab),1024))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "        word_vecs[vocab2int[word]] = model[word]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary lengths\n",
      "8101\n",
      "8101\n",
      "8101\n",
      "8101\n",
      "8101\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary lengths\")\n",
    "print(len(word_vecs))\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs.npy',word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Word2Affect Vector - VAD </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_vad=pd.read_excel('Warriner, Kuperman, Brysbaert - 2013 BRM-ANEW expanded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>D.Rat.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.64</td>\n",
       "      <td>21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>2.63</td>\n",
       "      <td>7</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.02</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>11</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  A.SD.Sum  \\\n",
       "1     aardvark        6.26      2.21         19        2.41      1.40   \n",
       "2      abalone        5.30      1.59         20        2.65      1.90   \n",
       "3      abandon        2.84      1.54         19        3.73      2.43   \n",
       "4  abandonment        2.63      1.74         19        4.95      2.64   \n",
       "5        abbey        5.85      1.69         20        2.20      1.70   \n",
       "\n",
       "   A.Rat.Sum  D.Mean.Sum  D.SD.Sum  D.Rat.Sum   ...     A.Rat.L  A.Mean.H  \\\n",
       "1         22        4.27      1.75         15   ...          11      2.55   \n",
       "2         20        4.95      1.79         22   ...          12      2.38   \n",
       "3         22        3.32      2.50         22   ...          11      3.82   \n",
       "4         21        2.64      1.81         28   ...          14      5.29   \n",
       "5         20        5.00      2.02         25   ...           9      2.55   \n",
       "\n",
       "   A.SD.H  A.Rat.H  D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "1    1.29       11      4.12    1.64        8      4.43    1.99        7  \n",
       "2    1.92        8      5.55    2.21       11      4.36    1.03       11  \n",
       "3    2.14       11      2.77    2.09       13      4.11    2.93        9  \n",
       "4    2.63        7      2.31    1.45       16      3.08    2.19       12  \n",
       "5    1.92       11      4.83    2.18       18      5.43    1.62        7  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vad.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wordvecs=[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    list_wordvecs.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'principal',\n",
       " 'shitty',\n",
       " 'society',\n",
       " 'championship',\n",
       " 'whip',\n",
       " 'contractor',\n",
       " 'rumor',\n",
       " 'cancel',\n",
       " 'loaf',\n",
       " 'affect',\n",
       " 'soda',\n",
       " 'discover',\n",
       " 'prominent',\n",
       " 'quarantine',\n",
       " 'recommend',\n",
       " 'acid',\n",
       " 'teeth',\n",
       " 'adopt',\n",
       " 'wonder',\n",
       " 'rich',\n",
       " 'cargo',\n",
       " 'hustle',\n",
       " 'speech',\n",
       " 'recovery',\n",
       " 'fuss',\n",
       " 'bully',\n",
       " 'corporal',\n",
       " 'nickel',\n",
       " 'flare',\n",
       " 'delay',\n",
       " 'deaf',\n",
       " 'reckless',\n",
       " 'nix',\n",
       " 'counsel',\n",
       " 'shower',\n",
       " 'sphere',\n",
       " 'outrageous',\n",
       " 'giant',\n",
       " 'delicious',\n",
       " 'fairy',\n",
       " 'joy',\n",
       " 'soviet',\n",
       " 'orange',\n",
       " 'drum',\n",
       " 'tooth',\n",
       " 'perfect',\n",
       " 'entertain',\n",
       " 'pub',\n",
       " 'drain',\n",
       " 'imagine',\n",
       " 'helmet',\n",
       " 'science',\n",
       " 'crooked',\n",
       " 'lung',\n",
       " 'counter',\n",
       " 'virus',\n",
       " 'step',\n",
       " 'apology',\n",
       " 'agency',\n",
       " 'pervert',\n",
       " 'confuse',\n",
       " 'vow',\n",
       " 'nun',\n",
       " 'sleeper',\n",
       " 'anchor',\n",
       " 'payment',\n",
       " 'fresh',\n",
       " 'explosive',\n",
       " 'tender',\n",
       " 'poison',\n",
       " 'attraction',\n",
       " 'attract',\n",
       " 'identify',\n",
       " 'fuzzy',\n",
       " 'scared',\n",
       " 'martini',\n",
       " 'liberty',\n",
       " 'statement',\n",
       " 'refill',\n",
       " 'prove',\n",
       " 'scumbag',\n",
       " 'junior',\n",
       " 'grid',\n",
       " 'physical',\n",
       " 'withdraw',\n",
       " 'scotch',\n",
       " 'robbery',\n",
       " 'dinner',\n",
       " 'sentimental',\n",
       " 'cause',\n",
       " 'stew',\n",
       " 'ear',\n",
       " 'grief',\n",
       " 'space',\n",
       " 'combination',\n",
       " 'commerce',\n",
       " 'probe',\n",
       " 'warp',\n",
       " 'pig',\n",
       " 'undead',\n",
       " 'cigar',\n",
       " 'drag',\n",
       " 'mace',\n",
       " 'resume',\n",
       " 'fascist',\n",
       " 'magazine',\n",
       " 'freak',\n",
       " 'freezing',\n",
       " 'clothing',\n",
       " 'birdie',\n",
       " 'tow',\n",
       " 'advanced',\n",
       " 'winter',\n",
       " 'volume',\n",
       " 'test',\n",
       " 'loved',\n",
       " 'net',\n",
       " 'presence',\n",
       " 'choose',\n",
       " 'maroon',\n",
       " 'shame',\n",
       " 'reaction',\n",
       " 'liar',\n",
       " 'overcome',\n",
       " 'miss',\n",
       " 'stupid',\n",
       " 'professional',\n",
       " 'ghost',\n",
       " 'retrieve',\n",
       " 'dim',\n",
       " 'ropes',\n",
       " 'parasite',\n",
       " 'memorial',\n",
       " 'relevant',\n",
       " 'say',\n",
       " 'philosophy',\n",
       " 'sheet',\n",
       " 'earth',\n",
       " 'person',\n",
       " 'lack',\n",
       " 'electricity',\n",
       " 'possible',\n",
       " 'sushi',\n",
       " 'gutter',\n",
       " 'chill',\n",
       " 'disco',\n",
       " 'phrase',\n",
       " 'pride',\n",
       " 'crush',\n",
       " 'trust',\n",
       " 'cocktail',\n",
       " 'cast',\n",
       " 'pinot',\n",
       " 'handful',\n",
       " 'large',\n",
       " 'build',\n",
       " 'temporary',\n",
       " 'hurricane',\n",
       " 'hide',\n",
       " 'sake',\n",
       " 'puzzle',\n",
       " 'area',\n",
       " 'assistant',\n",
       " 'confession',\n",
       " 'heartbeat',\n",
       " 'sign',\n",
       " 'tone',\n",
       " 'parade',\n",
       " 'brat',\n",
       " 'dull',\n",
       " 'transportation',\n",
       " 'faithful',\n",
       " 'situation',\n",
       " 'ketchup',\n",
       " 'short',\n",
       " 'order',\n",
       " 'collar',\n",
       " 'incredible',\n",
       " 'crop',\n",
       " 'past',\n",
       " 'skinny',\n",
       " 'nostril',\n",
       " 'shrimp',\n",
       " 'pop',\n",
       " 'improve',\n",
       " 'emotional',\n",
       " 'moral',\n",
       " 'presume',\n",
       " 'psychology',\n",
       " 'dentist',\n",
       " 'funeral',\n",
       " 'apply',\n",
       " 'teenager',\n",
       " 'beware',\n",
       " 'must',\n",
       " 'concentrate',\n",
       " 'visual',\n",
       " 'lifestyle',\n",
       " 'restore',\n",
       " 'civilization',\n",
       " 'fright',\n",
       " 'dive',\n",
       " 'marshal',\n",
       " 'chaos',\n",
       " 'militia',\n",
       " 'ethics',\n",
       " 'creature',\n",
       " 'root',\n",
       " 'angle',\n",
       " 'mean',\n",
       " 'famous',\n",
       " 'color',\n",
       " 'continue',\n",
       " 'jaw',\n",
       " 'sound',\n",
       " 'humanity',\n",
       " 'put',\n",
       " 'happen',\n",
       " 'death',\n",
       " 'bet',\n",
       " 'animal',\n",
       " 'foot',\n",
       " 'wit',\n",
       " 'deliver',\n",
       " 'operate',\n",
       " 'ticket',\n",
       " 'meaning',\n",
       " 'sure',\n",
       " 'stop',\n",
       " 'admission',\n",
       " 'taxi',\n",
       " 'camper',\n",
       " 'blame',\n",
       " 'borrow',\n",
       " 'shy',\n",
       " 'hay',\n",
       " 'card',\n",
       " 'spark',\n",
       " 'minimal',\n",
       " 'caviar',\n",
       " 'water',\n",
       " 'reach',\n",
       " 'colon',\n",
       " 'fucking',\n",
       " 'pack',\n",
       " 'optimistic',\n",
       " 'exercise',\n",
       " 'shine',\n",
       " 'conspiracy',\n",
       " 'tissue',\n",
       " 'apologize',\n",
       " 'armed',\n",
       " 'virtue',\n",
       " 'management',\n",
       " 'bitter',\n",
       " 'smear',\n",
       " 'basketball',\n",
       " 'recording',\n",
       " 'room',\n",
       " 'airlock',\n",
       " 'bounty',\n",
       " 'infection',\n",
       " 'friend',\n",
       " 'raft',\n",
       " 'thumb',\n",
       " 'consideration',\n",
       " 'lipstick',\n",
       " 'baby',\n",
       " 'whistle',\n",
       " 'lock',\n",
       " 'bat',\n",
       " 'thoughtful',\n",
       " 'university',\n",
       " 'lion',\n",
       " 'vodka',\n",
       " 'exotic',\n",
       " 'lecture',\n",
       " 'crude',\n",
       " 'insignificant',\n",
       " 'urgent',\n",
       " 'hound',\n",
       " 'home',\n",
       " 'panel',\n",
       " 'spaghetti',\n",
       " 'godfather',\n",
       " 'chair',\n",
       " 'sheep',\n",
       " 'thought',\n",
       " 'bout',\n",
       " 'nexus',\n",
       " 'mature',\n",
       " 'bill',\n",
       " 'trash',\n",
       " 'description',\n",
       " 'depression',\n",
       " 'belief',\n",
       " 'fact',\n",
       " 'voyage',\n",
       " 'tough',\n",
       " 'background',\n",
       " 'vessel',\n",
       " 'watch',\n",
       " 'slight',\n",
       " 'representative',\n",
       " 'use',\n",
       " 'caffeine',\n",
       " 'corps',\n",
       " 'primitive',\n",
       " 'ungrateful',\n",
       " 'type',\n",
       " 'eye',\n",
       " 'comfortable',\n",
       " 'finder',\n",
       " 'acknowledge',\n",
       " 'bottle',\n",
       " 'report',\n",
       " 'attack',\n",
       " 'truck',\n",
       " 'eager',\n",
       " 'safety',\n",
       " 'allow',\n",
       " 'thigh',\n",
       " 'feel',\n",
       " 'lean',\n",
       " 'crowded',\n",
       " 'fret',\n",
       " 'vengeance',\n",
       " 'source',\n",
       " 'gross',\n",
       " 'sad',\n",
       " 'seem',\n",
       " 'messy',\n",
       " 'cabinet',\n",
       " 'channel',\n",
       " 'scrambled',\n",
       " 'tribe',\n",
       " 'shit',\n",
       " 'load',\n",
       " 'minister',\n",
       " 'valentine',\n",
       " 'revenge',\n",
       " 'reverend',\n",
       " 'anonymous',\n",
       " 'board',\n",
       " 'soul',\n",
       " 'wash',\n",
       " 'smack',\n",
       " 'neck',\n",
       " 'reserved',\n",
       " 'unnecessary',\n",
       " 'rehearsal',\n",
       " 'considerable',\n",
       " 'cover',\n",
       " 'peculiar',\n",
       " 'chocolate',\n",
       " 'stunt',\n",
       " 'nation',\n",
       " 'pardon',\n",
       " 'odd',\n",
       " 'happiness',\n",
       " 'site',\n",
       " 'soap',\n",
       " 'ambassador',\n",
       " 'shipment',\n",
       " 'shining',\n",
       " 'hardware',\n",
       " 'plumber',\n",
       " 'pad',\n",
       " 'apple',\n",
       " 'murderer',\n",
       " 'activity',\n",
       " 'beard',\n",
       " 'basket',\n",
       " 'winch',\n",
       " 'pry',\n",
       " 'advice',\n",
       " 'available',\n",
       " 'passport',\n",
       " 'probation',\n",
       " 'flame',\n",
       " 'burn',\n",
       " 'wrist',\n",
       " 'dummy',\n",
       " 'sauce',\n",
       " 'technology',\n",
       " 'madman',\n",
       " 'definite',\n",
       " 'feed',\n",
       " 'rusty',\n",
       " 'idiotic',\n",
       " 'nickname',\n",
       " 'evolution',\n",
       " 'decision',\n",
       " 'mill',\n",
       " 'start',\n",
       " 'prayer',\n",
       " 'chip',\n",
       " 'chemical',\n",
       " 'agreement',\n",
       " 'offend',\n",
       " 'friendship',\n",
       " 'coach',\n",
       " 'complain',\n",
       " 'anesthetic',\n",
       " 'prince',\n",
       " 'bandit',\n",
       " 'honor',\n",
       " 'sheriff',\n",
       " 'effective',\n",
       " 'sit',\n",
       " 'twelve',\n",
       " 'roll',\n",
       " 'nap',\n",
       " 'ad',\n",
       " 'quiet',\n",
       " 'discuss',\n",
       " 'raw',\n",
       " 'rely',\n",
       " 'impressive',\n",
       " 'potential',\n",
       " 'stealing',\n",
       " 'film',\n",
       " 'examine',\n",
       " 'chuck',\n",
       " 'snoop',\n",
       " 'see',\n",
       " 'skip',\n",
       " 'pickup',\n",
       " 'center',\n",
       " 'lip',\n",
       " 'favor',\n",
       " 'super',\n",
       " 'log',\n",
       " 'tongue',\n",
       " 'scout',\n",
       " 'vehicle',\n",
       " 'surgical',\n",
       " 'hopeless',\n",
       " 'corn',\n",
       " 'touch',\n",
       " 'pointless',\n",
       " 'surprise',\n",
       " 'trip',\n",
       " 'squid',\n",
       " 'industry',\n",
       " 'kiss',\n",
       " 'dealer',\n",
       " 'transfer',\n",
       " 'awkward',\n",
       " 'thing',\n",
       " 'address',\n",
       " 'household',\n",
       " 'part',\n",
       " 'maid',\n",
       " 'thief',\n",
       " 'dark',\n",
       " 'blacksmith',\n",
       " 'blunt',\n",
       " 'social',\n",
       " 'landlord',\n",
       " 'twenty',\n",
       " 'mankind',\n",
       " 'backyard',\n",
       " 'house',\n",
       " 'discussion',\n",
       " 'safe',\n",
       " 'instant',\n",
       " 'canary',\n",
       " 'triumph',\n",
       " 'blue',\n",
       " 'cure',\n",
       " 'system',\n",
       " 'splendid',\n",
       " 'emerald',\n",
       " 'instinct',\n",
       " 'threaten',\n",
       " 'jail',\n",
       " 'vomit',\n",
       " 'publishing',\n",
       " 'eleven',\n",
       " 'bread',\n",
       " 'wide',\n",
       " 'kiddo',\n",
       " 'operator',\n",
       " 'brandy',\n",
       " 'affection',\n",
       " 'pal',\n",
       " 'fire',\n",
       " 'beach',\n",
       " 'basic',\n",
       " 'nature',\n",
       " 'highway',\n",
       " 'convention',\n",
       " 'psychic',\n",
       " 'crown',\n",
       " 'dock',\n",
       " 'create',\n",
       " 'invite',\n",
       " 'fifty',\n",
       " 'pulse',\n",
       " 'containment',\n",
       " 'marine',\n",
       " 'shocking',\n",
       " 'experiment',\n",
       " 'jacket',\n",
       " 'congratulations',\n",
       " 'pill',\n",
       " 'headquarters',\n",
       " 'enter',\n",
       " 'dump',\n",
       " 'lovely',\n",
       " 'soil',\n",
       " 'sum',\n",
       " 'wait',\n",
       " 'era',\n",
       " 'responsibility',\n",
       " 'land',\n",
       " 'project',\n",
       " 'knee',\n",
       " 'private',\n",
       " 'corrupt',\n",
       " 'night',\n",
       " 'oxygen',\n",
       " 'killer',\n",
       " 'interrogation',\n",
       " 'sherry',\n",
       " 'drop',\n",
       " 'guidance',\n",
       " 'convenient',\n",
       " 'staff',\n",
       " 'global',\n",
       " 'android',\n",
       " 'old',\n",
       " 'dessert',\n",
       " 'female',\n",
       " 'eternity',\n",
       " 'patrol',\n",
       " 'pencil',\n",
       " 'precise',\n",
       " 'sick',\n",
       " 'occasional',\n",
       " 'union',\n",
       " 'run',\n",
       " 'separate',\n",
       " 'bath',\n",
       " 'iron',\n",
       " 'string',\n",
       " 'tragedy',\n",
       " 'tall',\n",
       " 'team',\n",
       " 'academy',\n",
       " 'crap',\n",
       " 'willing',\n",
       " 'prepare',\n",
       " 'blast',\n",
       " 'try',\n",
       " 'ergo',\n",
       " 'task',\n",
       " 'examination',\n",
       " 'massive',\n",
       " 'copper',\n",
       " 'twin',\n",
       " 'concentration',\n",
       " 'adult',\n",
       " 'decency',\n",
       " 'commitment',\n",
       " 'block',\n",
       " 'ridiculous',\n",
       " 'shuffleboard',\n",
       " 'tower',\n",
       " 'dwarf',\n",
       " 'bargain',\n",
       " 'republican',\n",
       " 'typewriter',\n",
       " 'heaven',\n",
       " 'reverse',\n",
       " 'wrestling',\n",
       " 'dream',\n",
       " 'appointment',\n",
       " 'street',\n",
       " 'foul',\n",
       " 'interference',\n",
       " 'depressing',\n",
       " 'lime',\n",
       " 'slip',\n",
       " 'detect',\n",
       " 'fox',\n",
       " 'alarm',\n",
       " 'pool',\n",
       " 'narrow',\n",
       " 'arctic',\n",
       " 'sun',\n",
       " 'option',\n",
       " 'dust',\n",
       " 'mass',\n",
       " 'fortress',\n",
       " 'banana',\n",
       " 'certified',\n",
       " 'writer',\n",
       " 'romance',\n",
       " 'coast',\n",
       " 'pen',\n",
       " 'license',\n",
       " 'solution',\n",
       " 'rid',\n",
       " 'attorney',\n",
       " 'physics',\n",
       " 'gardener',\n",
       " 'cop',\n",
       " 'conference',\n",
       " 'brain',\n",
       " 'difference',\n",
       " 'presidential',\n",
       " 'leap',\n",
       " 'orbit',\n",
       " 'hood',\n",
       " 'sail',\n",
       " 'excellent',\n",
       " 'adventure',\n",
       " 'potatoes',\n",
       " 'arrogant',\n",
       " 'odds',\n",
       " 'puke',\n",
       " 'percent',\n",
       " 'mention',\n",
       " 'buster',\n",
       " 'picture',\n",
       " 'dickhead',\n",
       " 'midnight',\n",
       " 'mud',\n",
       " 'hump',\n",
       " 'sailor',\n",
       " 'wretched',\n",
       " 'pity',\n",
       " 'bell',\n",
       " 'stab',\n",
       " 'cup',\n",
       " 'swine',\n",
       " 'correct',\n",
       " 'maverick',\n",
       " 'internal',\n",
       " 'crew',\n",
       " 'cab',\n",
       " 'wipe',\n",
       " 'ambitious',\n",
       " 'proceed',\n",
       " 'suffer',\n",
       " 'alcoholic',\n",
       " 'literature',\n",
       " 'shooter',\n",
       " 'meat',\n",
       " 'slice',\n",
       " 'web',\n",
       " 'corner',\n",
       " 'click',\n",
       " 'bluff',\n",
       " 'gut',\n",
       " 'pussy',\n",
       " 'circuit',\n",
       " 'power',\n",
       " 'driver',\n",
       " 'pressure',\n",
       " 'bachelor',\n",
       " 'ought',\n",
       " 'pull',\n",
       " 'relation',\n",
       " 'objection',\n",
       " 'press',\n",
       " 'tan',\n",
       " 'mind',\n",
       " 'health',\n",
       " 'crime',\n",
       " 'document',\n",
       " 'communicate',\n",
       " 'hundred',\n",
       " 'set',\n",
       " 'boring',\n",
       " 'sucker',\n",
       " 'swell',\n",
       " 'funding',\n",
       " 'casual',\n",
       " 'pretty',\n",
       " 'convenience',\n",
       " 'exciting',\n",
       " 'unlikely',\n",
       " 'majesty',\n",
       " 'gal',\n",
       " 'detonate',\n",
       " 'monkey',\n",
       " 'low',\n",
       " 'store',\n",
       " 'weekend',\n",
       " 'student',\n",
       " 'surprising',\n",
       " 'grail',\n",
       " 'representation',\n",
       " 'fine',\n",
       " 'patience',\n",
       " 'consider',\n",
       " 'village',\n",
       " 'strip',\n",
       " 'harvest',\n",
       " 'towel',\n",
       " 'crystal',\n",
       " 'innocence',\n",
       " 'sister',\n",
       " 'amusing',\n",
       " 'owner',\n",
       " 'pit',\n",
       " 'shithead',\n",
       " 'manual',\n",
       " 'capable',\n",
       " 'white',\n",
       " 'sympathetic',\n",
       " 'guitar',\n",
       " 'specialist',\n",
       " 'ass',\n",
       " 'stepmother',\n",
       " 'pint',\n",
       " 'club',\n",
       " 'accident',\n",
       " 'command',\n",
       " 'peanut',\n",
       " 'look',\n",
       " 'tomb',\n",
       " 'debt',\n",
       " 'rooms',\n",
       " 'unfinished',\n",
       " 'scream',\n",
       " 'manage',\n",
       " 'memo',\n",
       " 'starve',\n",
       " 'alley',\n",
       " 'stubborn',\n",
       " 'paperwork',\n",
       " 'belong',\n",
       " 'jealous',\n",
       " 'settle',\n",
       " 'content',\n",
       " 'chapel',\n",
       " 'fountain',\n",
       " 'discretion',\n",
       " 'joint',\n",
       " 'beginning',\n",
       " 'rack',\n",
       " 'satisfaction',\n",
       " 'thunder',\n",
       " 'girl',\n",
       " 'maintain',\n",
       " 'mechanical',\n",
       " 'porch',\n",
       " 'collateral',\n",
       " 'newt',\n",
       " 'doll',\n",
       " 'relaxing',\n",
       " 'unique',\n",
       " 'pillow',\n",
       " 'ancient',\n",
       " 'rendezvous',\n",
       " 'crow',\n",
       " 'fuse',\n",
       " 'sober',\n",
       " 'storm',\n",
       " 'material',\n",
       " 'extreme',\n",
       " 'dick',\n",
       " 'new',\n",
       " 'squeeze',\n",
       " 'earn',\n",
       " 'complex',\n",
       " 'pigeon',\n",
       " 'surface',\n",
       " 'nigger',\n",
       " 'forgive',\n",
       " 'analysis',\n",
       " 'babe',\n",
       " 'remorse',\n",
       " 'supreme',\n",
       " 'appropriate',\n",
       " 'anxiety',\n",
       " 'arrangement',\n",
       " 'hospitality',\n",
       " 'sensitive',\n",
       " 'bottom',\n",
       " 'forget',\n",
       " 'aware',\n",
       " 'airline',\n",
       " 'paranoia',\n",
       " 'whisper',\n",
       " 'van',\n",
       " 'unhappy',\n",
       " 'curiosity',\n",
       " 'count',\n",
       " 'dope',\n",
       " 'psychiatrist',\n",
       " 'haircut',\n",
       " 'dance',\n",
       " 'bend',\n",
       " 'ceremony',\n",
       " 'mission',\n",
       " 'aid',\n",
       " 'disposal',\n",
       " 'love',\n",
       " 'training',\n",
       " 'butter',\n",
       " 'introduction',\n",
       " 'purchase',\n",
       " 'drive',\n",
       " 'authority',\n",
       " 'mess',\n",
       " 'piss',\n",
       " 'pain',\n",
       " 'religion',\n",
       " 'sunny',\n",
       " 'complaint',\n",
       " 'petty',\n",
       " 'doctor',\n",
       " 'die',\n",
       " 'racket',\n",
       " 'anarchy',\n",
       " 'extra',\n",
       " 'impulse',\n",
       " 'salvage',\n",
       " 'interesting',\n",
       " 'daytime',\n",
       " 'mare',\n",
       " 'deadly',\n",
       " 'bike',\n",
       " 'rescue',\n",
       " 'temple',\n",
       " 'electronic',\n",
       " 'service',\n",
       " 'think',\n",
       " 'holiday',\n",
       " 'gambler',\n",
       " 'cart',\n",
       " 'liable',\n",
       " 'restaurant',\n",
       " 'buff',\n",
       " 'zoo',\n",
       " 'confusion',\n",
       " 'supply',\n",
       " 'release',\n",
       " 'freedom',\n",
       " 'photograph',\n",
       " 'aircraft',\n",
       " 'mix',\n",
       " 'train',\n",
       " 'heck',\n",
       " 'milk',\n",
       " 'graduate',\n",
       " 'dining',\n",
       " 'wallpaper',\n",
       " 'pair',\n",
       " 'culture',\n",
       " 'pictures',\n",
       " 'amnesia',\n",
       " 'spin',\n",
       " 'fascinating',\n",
       " 'chairman',\n",
       " 'legit',\n",
       " 'heroic',\n",
       " 'realize',\n",
       " 'priest',\n",
       " 'effort',\n",
       " 'amendment',\n",
       " 'save',\n",
       " 'uniform',\n",
       " 'tiny',\n",
       " 'gray',\n",
       " 'action',\n",
       " 'coke',\n",
       " 'welsh',\n",
       " 'strategy',\n",
       " 'slob',\n",
       " 'escort',\n",
       " 'detail',\n",
       " 'plate',\n",
       " 'face',\n",
       " 'letter',\n",
       " 'fuck',\n",
       " 'history',\n",
       " 'touchy',\n",
       " 'excuse',\n",
       " 'promise',\n",
       " 'gag',\n",
       " 'lonely',\n",
       " 'chump',\n",
       " 'possibility',\n",
       " 'weigh',\n",
       " 'prison',\n",
       " 'circus',\n",
       " 'define',\n",
       " 'race',\n",
       " 'football',\n",
       " 'four',\n",
       " 'discount',\n",
       " 'carpenter',\n",
       " 'comedy',\n",
       " 'advise',\n",
       " 'conviction',\n",
       " 'compete',\n",
       " 'population',\n",
       " 'seventeen',\n",
       " 'medication',\n",
       " 'charge',\n",
       " 'cool',\n",
       " 'resent',\n",
       " 'brutal',\n",
       " 'irrelevant',\n",
       " 'afraid',\n",
       " 'cozy',\n",
       " 'junction',\n",
       " 'patient',\n",
       " 'ginger',\n",
       " 'computer',\n",
       " 'leave',\n",
       " 'stiff',\n",
       " 'mailbox',\n",
       " 'bow',\n",
       " 'road',\n",
       " 'move',\n",
       " 'believer',\n",
       " 'sabotage',\n",
       " 'error',\n",
       " 'aim',\n",
       " 'moron',\n",
       " 'transvestite',\n",
       " 'worthy',\n",
       " 'auto',\n",
       " 'testify',\n",
       " 'deposition',\n",
       " 'intelligent',\n",
       " 'man',\n",
       " 'station',\n",
       " 'pike',\n",
       " 'hook',\n",
       " 'princess',\n",
       " 'bowling',\n",
       " 'interest',\n",
       " 'beautiful',\n",
       " 'inappropriate',\n",
       " 'bare',\n",
       " 'parking',\n",
       " 'static',\n",
       " 'convince',\n",
       " 'deep',\n",
       " 'growth',\n",
       " 'head',\n",
       " 'destiny',\n",
       " 'tension',\n",
       " 'rush',\n",
       " 'employment',\n",
       " 'reconsider',\n",
       " 'sub',\n",
       " 'lord',\n",
       " 'enforcement',\n",
       " 'smooth',\n",
       " 'radiation',\n",
       " 'rational',\n",
       " 'hospital',\n",
       " 'pageant',\n",
       " 'incident',\n",
       " 'rope',\n",
       " 'lap',\n",
       " 'faggot',\n",
       " 'disaster',\n",
       " 'resist',\n",
       " 'claim',\n",
       " 'betray',\n",
       " 'research',\n",
       " 'muscle',\n",
       " 'theme',\n",
       " 'lie',\n",
       " 'misery',\n",
       " 'beauty',\n",
       " 'instrument',\n",
       " 'college',\n",
       " 'membership',\n",
       " 'whale',\n",
       " 'match',\n",
       " 'counterfeit',\n",
       " 'actress',\n",
       " 'admittance',\n",
       " 'shotgun',\n",
       " 'fabric',\n",
       " 'terminal',\n",
       " 'sketch',\n",
       " 'grave',\n",
       " 'middle',\n",
       " 'stir',\n",
       " 'beg',\n",
       " 'tradition',\n",
       " 'massage',\n",
       " 'expose',\n",
       " 'earl',\n",
       " 'mafia',\n",
       " 'practice',\n",
       " 'spread',\n",
       " 'hooker',\n",
       " ...}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list3 = set(list_wordvecs) & set(df_vad[\"Word\"])\n",
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4032"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vad = set(df_vad['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word_vecs_vad = np.zeros((len(model.wv.vocab),1027))\n",
    "count_vad=0\n",
    "count_neutral=0\n",
    "for i,word in enumerate(model.wv.index2word):   \n",
    "    if word in set(list_vad):\n",
    "        #print(word)\n",
    "        count_vad=count_vad+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=df_vad.loc[df_vad[\"Word\"] == word, 'V.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1025]=df_vad.loc[df_vad[\"Word\"] == word, 'A.Mean.Sum'].iloc[0]\n",
    "        word_vecs_vad[vocab2int[word]][1026]=df_vad.loc[df_vad[\"Word\"] == word, 'D.Mean.Sum'].iloc[0]\n",
    "        #print(word_vecs_vad[i])\n",
    "    else:\n",
    "        #print(\"out\")\n",
    "        count_neutral=count_neutral+1\n",
    "        word_vecs_vad[vocab2int[word]][0:1024] = model[word]\n",
    "        word_vecs_vad[vocab2int[word]][1024]=5\n",
    "        word_vecs_vad[vocab2int[word]][1025]=1\n",
    "        word_vecs_vad[vocab2int[word]][1026]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032\n",
      "4069\n"
     ]
    }
   ],
   "source": [
    "print(count_vad)\n",
    "print(count_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352    6.05\n",
       "Name: V.Mean.Sum, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val =df_vad[df_vad['Word'] == \"mailbox\"][\"V.Mean.Sum\"]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_VAD.npy',word_vecs_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - counterfitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_counterfit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_counterfit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.068298,  0.01397 , -0.02441 ,  0.054046,  0.033629, -0.01007 ,\n",
       "       -0.031107,  0.040267, -0.024809, -0.023129,  0.035075, -0.061981,\n",
       "        0.094889,  0.038437, -0.018818,  0.039997,  0.077868,  0.027647,\n",
       "       -0.0072  , -0.000698, -0.02708 ,  0.056055, -0.053605,  0.001178,\n",
       "        0.081652,  0.042793, -0.091373,  0.02682 , -0.010707, -0.034362,\n",
       "       -0.027781, -0.004621,  0.085808, -0.004652,  0.075245,  0.084456,\n",
       "       -0.050868, -0.044668, -0.008077,  0.009224,  0.096167,  0.001957,\n",
       "        0.03171 ,  0.026655, -0.043085,  0.038686,  0.014165,  0.06862 ,\n",
       "        0.050164, -0.005307,  0.145519,  0.081177, -0.056621,  0.014355,\n",
       "       -0.044101, -0.001805, -0.031732,  0.003166,  0.096613, -0.089282,\n",
       "       -0.090452,  0.046647, -0.198193, -0.04675 , -0.009436,  0.075172,\n",
       "       -0.052017, -0.028137, -0.078767,  0.027518,  0.006175, -0.017348,\n",
       "        0.179998, -0.017671, -0.091829, -0.055601,  0.006504,  0.079339,\n",
       "       -0.006983, -0.022379, -0.025881,  0.029811,  0.013226,  0.115053,\n",
       "        0.032207, -0.04349 , -0.051668,  0.027526, -0.046608,  0.115959,\n",
       "        0.057365,  0.090153, -0.064874, -0.027923,  0.029423, -0.092529,\n",
       "       -0.013401,  0.080153, -0.043774, -0.010028,  0.028602,  0.02152 ,\n",
       "       -0.044785, -0.006796, -0.034181, -0.010616, -0.034523, -0.019031,\n",
       "       -0.013247,  0.007484,  0.055436,  0.052278, -0.126453,  0.026781,\n",
       "        0.071985,  0.024007,  0.031252, -0.067289, -0.042609, -0.02263 ,\n",
       "        0.014178,  0.011003,  0.022296,  0.093423, -0.076267, -0.028193,\n",
       "        0.012271, -0.096434, -0.023858,  0.004527, -0.058173, -0.046978,\n",
       "       -0.051532, -0.016989, -0.061828, -0.07654 ,  0.106001,  0.10083 ,\n",
       "        0.023096,  0.046153, -0.052389,  0.065611,  0.096181, -0.014762,\n",
       "       -0.015398, -0.044217, -0.009704, -0.045321,  0.063446, -0.008449,\n",
       "       -0.03746 ,  0.159556, -0.063952,  0.10805 , -0.040134,  0.052322,\n",
       "       -0.005815, -0.000742, -0.026841,  0.003803,  0.016281,  0.079311,\n",
       "        0.060642,  0.070299,  0.133207, -0.013334, -0.094392, -0.039325,\n",
       "       -0.021152, -0.036935, -0.092639,  0.068085,  0.053983, -0.035672,\n",
       "        0.004502,  0.129921,  0.030324, -0.064907, -0.01776 ,  0.032938,\n",
       "       -0.074088, -0.136206, -0.037866, -0.070047, -0.119126, -0.030005,\n",
       "       -0.081447,  0.015276,  0.01613 , -0.029709, -0.027911,  0.085703,\n",
       "        0.044309, -0.107859,  0.015405, -0.132433, -0.045625, -0.063961,\n",
       "       -0.040616, -0.106   ,  0.011839,  0.021633,  0.027181,  0.024199,\n",
       "       -0.02978 , -0.083825, -0.022189, -0.048264,  0.016572,  0.112918,\n",
       "       -0.025803,  0.053843, -0.041338,  0.051484, -0.003051, -0.04204 ,\n",
       "        0.113067,  0.036121, -0.020523,  0.023554,  0.002181, -0.057482,\n",
       "       -0.08471 , -0.044766,  0.006952,  0.005777, -0.049501, -0.044739,\n",
       "        0.020132,  0.041329, -0.012819, -0.077212,  0.096833, -0.022325,\n",
       "        0.015486,  0.007067, -0.14609 , -0.078302, -0.014663,  0.002906,\n",
       "        0.005719,  0.075205,  0.039659,  0.005798, -0.030796, -0.007436,\n",
       "        0.018907, -0.039181, -0.142236, -0.075287, -0.027013,  0.109294,\n",
       "        0.015772, -0.004132,  0.012398,  0.007734, -0.050636,  0.026038,\n",
       "       -0.004614, -0.101044, -0.010447, -0.026595, -0.031416,  0.007031,\n",
       "        0.043274,  0.009761, -0.00074 , -0.065493, -0.027458, -0.011578,\n",
       "       -0.043572, -0.03095 ,  0.049361,  0.024768,  0.019395, -0.025657,\n",
       "        0.024031, -0.126927, -0.081736,  0.076594,  0.066419,  0.054371,\n",
       "       -0.014845,  0.078717, -0.033811,  0.046969, -0.05406 , -0.01149 ,\n",
       "        0.028577,  0.070959, -0.00128 , -0.018592, -0.072032,  0.002022,\n",
       "        0.014795,  0.069068,  0.021019, -0.0405  , -0.017028, -0.064674,\n",
       "       -0.003139,  0.013415, -0.011306], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_counterfit_affect[\"well\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "word_vecs_counterfit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    if word in model_counterfit_affect.wv.vocab:\n",
    "        word_vecs_counterfit_affect[vocab2int[word]] = model_counterfit_affect[word]\n",
    "    else:\n",
    "        list_word_not_found.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_unknown = np.mean(word_vecs_counterfit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_unknown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_word_not_found:\n",
    "    word_vecs_counterfit_affect[i] = word_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8101, 303)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs_counterfit_affect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_counterfit_affect.npy',word_vecs_counterfit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Word2Vec - retrofitting + affect </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_retrofit_affect = gensim.models.KeyedVectors.load_word2vec_format('./w2v_retrofit_append_affect.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.23350e-02, -3.88500e-03, -6.49750e-02,  3.56760e-02,\n",
       "       -3.79590e-02, -2.26900e-03,  8.23700e-03,  4.43840e-02,\n",
       "        2.83620e-02, -4.37600e-02,  5.67000e-03, -9.95770e-02,\n",
       "        8.63900e-02,  4.60080e-02, -2.64400e-02,  4.40500e-03,\n",
       "        1.02670e-02,  5.38950e-02, -4.03780e-02,  9.87700e-03,\n",
       "       -1.51570e-02,  4.20510e-02, -3.27200e-02, -4.11550e-02,\n",
       "        1.38780e-01,  2.17640e-02, -8.32230e-02, -5.89670e-02,\n",
       "       -3.76710e-02, -2.49510e-02, -1.35740e-02,  4.56560e-02,\n",
       "        6.87190e-02, -4.15760e-02,  1.90190e-02,  7.76660e-02,\n",
       "       -3.91100e-02,  1.02170e-02, -2.43200e-03, -3.44060e-02,\n",
       "        1.62309e-01,  9.66700e-03,  4.85220e-02,  1.48780e-02,\n",
       "       -1.43820e-02,  6.88200e-03, -6.29780e-02,  5.76300e-03,\n",
       "        5.62570e-02, -1.36200e-02,  1.05286e-01,  4.24430e-02,\n",
       "       -2.99170e-02,  3.37360e-02,  1.19460e-02,  5.95990e-02,\n",
       "       -5.28090e-02,  3.39970e-02,  1.04224e-01, -3.99220e-02,\n",
       "       -8.26270e-02,  8.65720e-02, -1.31603e-01, -9.60920e-02,\n",
       "       -1.99100e-02,  3.15280e-02, -2.13130e-02,  1.05400e-03,\n",
       "       -1.21629e-01,  1.23040e-02,  1.10550e-02,  1.95210e-02,\n",
       "        1.77765e-01, -3.91810e-02, -6.95640e-02, -2.92770e-02,\n",
       "        4.99680e-02,  1.25507e-01,  4.85040e-02,  2.35210e-02,\n",
       "       -3.44020e-02,  3.82310e-02, -5.77900e-03,  5.75700e-02,\n",
       "        7.47600e-03, -1.29390e-02, -1.43235e-01,  4.85440e-02,\n",
       "       -6.69710e-02,  3.27610e-02,  8.71550e-02,  1.17898e-01,\n",
       "       -8.04350e-02,  2.03950e-02,  8.24950e-02, -1.81700e-02,\n",
       "       -5.05350e-02,  2.46020e-02, -4.66390e-02,  2.51410e-02,\n",
       "        2.78640e-02, -3.22780e-02, -6.77830e-02,  2.05000e-02,\n",
       "       -1.12660e-02, -2.68060e-02, -3.76340e-02, -2.05500e-02,\n",
       "        2.07940e-02,  5.14310e-02,  1.87820e-02,  4.83210e-02,\n",
       "       -6.98370e-02, -1.41580e-02,  3.17920e-02,  4.18620e-02,\n",
       "        5.51990e-02, -2.19910e-02, -1.80590e-02,  1.83780e-02,\n",
       "        1.44100e-03, -2.18150e-02, -6.56300e-03,  9.51770e-02,\n",
       "       -4.47260e-02, -1.49000e-04, -2.62760e-02, -2.97560e-02,\n",
       "        6.68700e-02, -2.43010e-02,  4.77600e-03, -4.95810e-02,\n",
       "       -6.06360e-02, -3.61200e-02, -1.39711e-01, -4.42110e-02,\n",
       "        1.15905e-01,  1.04244e-01,  4.80440e-02,  9.28500e-03,\n",
       "       -2.21940e-02,  9.51280e-02,  8.00830e-02, -2.18480e-02,\n",
       "        5.48820e-02, -1.95630e-02, -3.21440e-02,  1.17570e-02,\n",
       "       -2.71500e-03,  5.51410e-02, -3.72040e-02,  1.09867e-01,\n",
       "       -2.99810e-02,  5.38400e-02, -2.05020e-02,  1.78600e-02,\n",
       "       -1.68950e-02,  2.08630e-02, -1.08970e-02,  3.21570e-02,\n",
       "        8.78380e-02,  1.40484e-01,  1.88510e-02,  9.09000e-02,\n",
       "        1.22892e-01, -1.19820e-02, -8.70400e-02, -3.43210e-02,\n",
       "       -1.93800e-03, -4.41520e-02, -1.09983e-01,  5.42150e-02,\n",
       "        3.39810e-02, -3.60460e-02,  1.96360e-02,  8.10820e-02,\n",
       "       -2.06260e-02, -8.10390e-02,  3.73710e-02,  2.45080e-02,\n",
       "       -3.18570e-02, -9.11560e-02, -9.33580e-02, -7.67050e-02,\n",
       "       -5.87040e-02,  8.21220e-02, -3.80480e-02,  5.60550e-02,\n",
       "        2.75920e-02,  3.05020e-02,  1.28540e-02,  3.69340e-02,\n",
       "        8.27580e-02, -9.79310e-02, -1.68440e-02, -9.16430e-02,\n",
       "       -8.14810e-02, -6.61900e-03,  3.90090e-02, -6.81060e-02,\n",
       "        3.44750e-02,  6.53300e-03,  5.83300e-03,  7.30600e-03,\n",
       "        1.90760e-02, -9.64750e-02, -3.46510e-02, -4.62300e-02,\n",
       "        2.11670e-02,  4.07630e-02, -2.84750e-02,  2.31460e-02,\n",
       "       -4.21440e-02, -1.13120e-02, -2.53450e-02, -3.93650e-02,\n",
       "        1.02872e-01, -2.48100e-03, -3.99310e-02, -7.74400e-03,\n",
       "       -3.52600e-03, -6.49670e-02, -6.61850e-02, -8.23090e-02,\n",
       "        1.98780e-02,  5.93100e-03, -2.32790e-02, -9.60140e-02,\n",
       "        6.97640e-02, -2.87520e-02,  4.76300e-03, -1.22470e-01,\n",
       "        8.86810e-02, -5.91370e-02,  1.05750e-02,  2.48840e-02,\n",
       "       -1.28944e-01, -1.46230e-02,  1.73130e-02, -3.27380e-02,\n",
       "       -5.55830e-02,  7.94720e-02,  8.93170e-02, -4.37700e-03,\n",
       "       -6.04770e-02, -9.04600e-03, -1.26280e-02, -1.79360e-02,\n",
       "       -1.27710e-01, -9.60200e-03, -1.29010e-02,  7.23800e-02,\n",
       "        4.80580e-02, -8.95660e-02,  4.49370e-02, -5.80800e-03,\n",
       "       -3.60920e-02,  2.89270e-02, -2.50700e-03, -1.21058e-01,\n",
       "        3.88800e-02,  1.79070e-02, -2.59000e-02,  3.53600e-02,\n",
       "        2.20000e-05,  1.26016e-01, -8.27400e-03, -1.48720e-02,\n",
       "        1.09660e-02, -1.19170e-02, -1.63020e-02, -7.11760e-02,\n",
       "        5.31760e-02,  4.53180e-02, -4.78360e-02, -6.62720e-02,\n",
       "        5.78510e-02, -1.68187e-01, -6.94980e-02,  3.03440e-02,\n",
       "        9.42100e-02,  7.30580e-02, -1.13400e-03,  4.18600e-02,\n",
       "        4.88000e-03,  1.29690e-02, -5.07840e-02, -3.64300e-02,\n",
       "        3.89070e-02,  7.12970e-02, -1.76920e-02, -6.80100e-02,\n",
       "       -1.02963e-01,  2.39220e-02,  3.75600e-03,  8.64140e-02,\n",
       "        2.08100e-03, -1.24210e-02,  1.22830e-02, -1.07682e-01,\n",
       "       -3.16200e-03,  1.35120e-02, -1.13870e-02], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_retrofit_affect.wv[\"well\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsriniv6/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "word_vecs_retrofit_affect = np.zeros((len(model.wv.vocab),303))\n",
    "list_word_not_found_retro =[]\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "    if word in model_retrofit_affect.wv.vocab:\n",
    "        word_vecs_retrofit_affect[vocab2int[word]] = model_retrofit_affect[word]\n",
    "    else:\n",
    "        list_word_not_found_retro.append(vocab2int[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_word_not_found_retro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_unknown_retro = np.mean(word_vecs_retrofit_affect, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_word_not_found_retro:\n",
    "    word_vecs_retrofit_affect[i] = word_unknown_retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs_retrofit_affect.npy',word_vecs_retrofit_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add EOS tokens to target data now that the embeddings have been trained\n",
    "for i in range(len(answers_int)):\n",
    "    answers_text[i] += \" \" + EOS\n",
    "    answers_int[i].append(METATOKEN_INDEX)\n",
    "    \n",
    "    #answers_int[i].append(answers_vocab_to_int[EOS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196991\n",
      "196991\n",
      "\n",
      "['cameron']\n",
      "['the', 'thing', 'is', 'cameron', 'i', 'at', 'the', 'mercy', 'of', 'a', 'particularly', '<UNK>', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'ca', 'not', 'date', 'until', 'she', 'does']\n",
      "\n",
      "['why']\n",
      "['<UNK>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something']\n",
      "\n",
      "['there']\n",
      "['where']\n",
      "\n",
      "['yes', 'i', 'see', 'you', 'have', 'issued', 'each', 'of', 'them', 'with', 'a', 'martini', 'henry', '<UNK>', 'our', '<UNK>', 'for', 'native', '<UNK>', 'one', 'rifle', 'to', 'ten', 'men', 'and', 'only', 'five', 'rounds', 'per', 'rifle']\n",
      "['but', 'will', 'they', 'make', 'good', 'use', 'of', 'them']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort questions and answers by the length of questions.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "max_source_line_length = max( [len(sentence) for sentence in questions_int])\n",
    "max_targ_line_length = max([len(sentence) for sentence in answers_int])\n",
    "max_line_length = max(max_source_line_length, max_targ_line_length)\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for index, sequence in enumerate(questions_int):\n",
    "        if len(sequence) == length:\n",
    "            sorted_questions.append(questions_int[index])\n",
    "            sorted_answers.append(answers_int[index])\n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "indices = [0, 1, 2, len(sorted_questions) - 1]\n",
    "for i in indices:\n",
    "    print(int_to_text(sorted_questions[i], questions_int_to_vocab))\n",
    "    print(int_to_text(sorted_answers[i], answers_int_to_vocab))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: This really should be something like \"preprocess_targets\"\n",
    "def process_decoding_input(target_data, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat( [tf.fill([batch_size, 1], METATOKEN_INDEX), ending], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_cell(rnn_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "def multi_dropout_cell(rnn_size, keep_prob, num_layers):    \n",
    "    return tf.contrib.rnn.MultiRNNCell( [dropout_cell(rnn_size, keep_prob) for _ in range(num_layers)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Create the encoding layer\n",
    "    \n",
    "    Returns a tuple `(outputs, output_states)` where\n",
    "      outputs is a 2-tuple of vectors of dimensions [sequence_length, rnn_size] for the forward and backward passes\n",
    "      output_states is a 2-tupe of the final hidden states of the forward and backward passes\n",
    "    \n",
    "    \"\"\"\n",
    "    forward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    backward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw = forward_cell,\n",
    "                                                   cell_bw = backward_cell,\n",
    "                                                   sequence_length = sequence_lengths,\n",
    "                                                   inputs = rnn_inputs,\n",
    "                                                    dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(enc_state, enc_outputs, dec_embed_input, dec_embeddings, #Inputs\n",
    "                        attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                        keep_prob, beam_width, #Hypeparameters\n",
    "                        source_lengths, target_lengths, batch_size): \n",
    "   \n",
    "    with tf.variable_scope(\"decoding\", reuse=tf.AUTO_REUSE) as decoding_scope:\n",
    "        dec_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "        init_dec_state_size = batch_size\n",
    "        #TRAINING\n",
    "        train_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size, memory=enc_outputs,\n",
    "                                                         memory_sequence_length=source_lengths)\n",
    "        \n",
    "        train_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, train_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_lengths, time_major=False)\n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(train_cell, helper,\n",
    "                            train_cell.zero_state(init_dec_state_size, tf.float32)\n",
    "                                                        .clone(cell_state=enc_state),\n",
    "                            output_layer = output_layer)\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, scope=decoding_scope)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        #INFERENCE\n",
    "        #Tile inputs\n",
    "        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, beam_width)\n",
    "        enc_outputs = tf.contrib.seq2seq.tile_batch(enc_outputs, beam_width)\n",
    "        tiled_source_lengths = tf.contrib.seq2seq.tile_batch(source_lengths, beam_width)\n",
    "        init_dec_state_size *= beam_width\n",
    "        \n",
    "        infer_attn = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size,memory=enc_outputs,\n",
    "                                                          memory_sequence_length=tiled_source_lengths)\n",
    "        infer_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, infer_attn,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        \n",
    "        \n",
    "        start_tokens = tf.tile( [METATOKEN_INDEX], [batch_size]) #Not by batch_size*beam_width, strangely\n",
    "        end_token = METATOKEN_INDEX\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = infer_cell,\n",
    "            embedding = dec_embeddings,\n",
    "            start_tokens = start_tokens, \n",
    "            end_token = end_token,\n",
    "            beam_width = beam_width,\n",
    "            initial_state = infer_cell.zero_state(init_dec_state_size, tf.float32).clone(cell_state=enc_state),\n",
    "            output_layer = output_layer\n",
    "        )  \n",
    "        final_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, scope=decoding_scope,\n",
    "                                                                      maximum_iterations=50)\n",
    "        \n",
    "        ids = final_decoder_output.predicted_ids\n",
    "        beams = ids\n",
    "                \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(wordVecs,input_data, target_data, keep_prob, batch_size,\n",
    "                  source_lengths, target_lengths,\n",
    "                  vocab_size, enc_embedding_size, dec_embedding_size,\n",
    "                  attn_size, rnn_size, num_layers, beam_width):\n",
    "    \n",
    "\n",
    "    W = tf.Variable(wordVecs,trainable=False,name=\"W\")\n",
    "    enc_embed_input = tf.nn.embedding_lookup(W, input_data)\n",
    "    enc_outputs, enc_states = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, source_lengths)    \n",
    "    concatenated_enc_output = tf.concat(enc_outputs, -1)\n",
    "    init_dec_state = enc_states[0]    \n",
    "    \n",
    "    \n",
    "    dec_input = process_decoding_input(target_data, batch_size)\n",
    "    dec_embeddings = W \n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    print(dec_embed_input.shape)\n",
    "    output_layer = tf.layers.Dense(vocab_size,bias_initializer=tf.zeros_initializer(),activation=tf.nn.relu)\n",
    "    logits, beams = decoding_layer(init_dec_state,\n",
    "                            concatenated_enc_output,\n",
    "                            dec_embed_input,\n",
    "                            dec_embeddings,\n",
    "                            attn_size,\n",
    "                            rnn_size, \n",
    "                            num_layers,\n",
    "                            output_layer,\n",
    "                            keep_prob,\n",
    "                            beam_width,\n",
    "                            source_lengths,\n",
    "                            target_lengths, \n",
    "                            batch_size\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return logits, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_with_meta = 8102\n",
      "METATOKEN_INDEX = 8101\n",
      "wordVecsWithMeta.shape = (8102, 1027)\n",
      "wordVecsWithMeta[METATOKEN_INDEX] = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Settings used by Asghar et al.\n",
    "rnn_size = 1024\n",
    "num_layers = 1\n",
    "embedding_size = 1027\n",
    "encoding_embedding_size = 1027 ## if word2vec.  1027 if word2Affect vect - VAD.  .... 303 if word2vec,retrofit or counterfit\n",
    "decoding_embedding_size = 1027 ## if word2vec.  1027 if word2Affect vect - VAD.  .... 303 if word2vec,retrofit or counterfit\n",
    "\n",
    "flag_affect_functions = True # change this flag to false if affect functions are not used\n",
    "attention_size = 256\n",
    "\n",
    "#Training\n",
    "epochs = 50\n",
    "train_batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.00001\n",
    "\n",
    "keep_probability = 0.75\n",
    "vocab_size = len(answers_vocab_to_int)\n",
    "#Decoding\n",
    "beam_width = 10\n",
    "\n",
    "#Validation\n",
    "valid_batch_size = 128\n",
    "\n",
    "wordVecs = np.load('word_Vecs_VAD.npy').astype(np.float32)\n",
    "metatoken_embedding = np.zeros((1, embedding_size), dtype=wordVecs.dtype)\n",
    "wordVecsWithMeta = np.concatenate( (wordVecs, metatoken_embedding), axis=0 )\n",
    "vocab_size_with_meta = wordVecsWithMeta.shape[0]\n",
    "\n",
    "print(\"vocab_size_with_meta =\", vocab_size_with_meta)\n",
    "print(\"METATOKEN_INDEX =\", METATOKEN_INDEX)\n",
    "print(\"wordVecsWithMeta.shape =\", wordVecsWithMeta.shape)\n",
    "print(\"wordVecsWithMeta[METATOKEN_INDEX] =\", wordVecsWithMeta[METATOKEN_INDEX])\n",
    "\n",
    "#print(wordVecsWithMeta.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 1027)\n",
      "(?, ?, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#                                      batch_size, time\n",
    "input_data = tf.placeholder(tf.int32, [None,       None], name='input')\n",
    "targets = tf.placeholder(tf.int32,    [None,       None], name='targets')\n",
    "lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "#                                          batch_size\n",
    "source_lengths = tf.placeholder(tf.int32, [None], name=\"source_lengths\")\n",
    "target_lengths = tf.placeholder(tf.int32, [None], name=\"target_lengths\")\n",
    "batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, beams = \\\n",
    "seq2seq_model(wordVecsWithMeta,input_data, targets, keep_prob, batch_size,\n",
    "        source_lengths, target_lengths, \n",
    "        vocab_size_with_meta, encoding_embedding_size, decoding_embedding_size, attention_size, rnn_size, num_layers,\n",
    "        beam_width)\n",
    "\n",
    "# Find the shape of the input data for sequence_loss\n",
    "with tf.name_scope(\"optimization\"): \n",
    "    \n",
    "    if(flag_affect_functions):\n",
    "        #Embeddings\n",
    "        Weight = tf.Variable(wordVecsWithMeta,trainable=False,name=\"Weight\")\n",
    "        embed_vec  = tf.nn.embedding_lookup(Weight, input_data)\n",
    "        input_embed =embed_vec[:,:,1024:1027]\n",
    "\n",
    "\n",
    "\n",
    "        dec_res = process_decoding_input(targets, batch_size)\n",
    "        dec_embed = tf.nn.embedding_lookup(Weight, dec_res)\n",
    "        target_embed = dec_embed[:,:,1024:1027]\n",
    "        print(target_embed.shape)\n",
    "    xent = loss_functions.cross_entropy(train_logits, targets, target_lengths)\n",
    "    '''--param values are 0.5, 0.4 and 0.5 as per the hyper parameter tuning mentioned in paper for LDMIN,LDMAX,LDAC functions--\n",
    "    '''\n",
    "    cost = loss_functions.min_affective_dissonance(0.5,train_logits,targets,target_lengths,input_embed,target_embed)\n",
    "    #cost = loss_functions.max_affective_dissonance(0.4,train_logits,targets,target_lengths,input_embed,target_embed)\n",
    "    #cost = loss_functions.max_affective_content(0.5,train_logits,targets,target_lengths,target_embed)\n",
    "    \n",
    "    eval_mask = tf.sequence_mask(target_lengths, dtype=tf.float32)\n",
    "    perplexity = tf.contrib.seq2seq.sequence_loss(train_logits, targets, eval_mask,\n",
    "                                                softmax_loss_function=metrics.perplexity)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    pad_int = METATOKEN_INDEX\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_length - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        source_lengths = np.array( [len(sentence) for sentence in questions_batch] )\n",
    "        target_lengths = np.array( [len(sentence) for sentence in answers_batch])\n",
    "        \n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield source_lengths, target_lengths, pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shuffle(source_sequences, target_sequences):\n",
    "    if len(source_sequences) != len(target_sequences):\n",
    "        raise ValueError(\"Cannot shuffle parallel sets with different numbers of sequences\")\n",
    "    indices = np.random.permutation(len(source_sequences))\n",
    "    shuffled_source = [source_sequences[indices[i]] for i in range(len(indices))]\n",
    "    shuffled_target = [target_sequences[indices[i]] for i in range(len(indices))]\n",
    "    \n",
    "    return (shuffled_source, shuffled_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  show_response(question_int, beams, answer_int = None, best_only=True):\n",
    "    pad_q = METATOKEN_INDEX\n",
    "    print(\"Prompt\")\n",
    "    print(\"  Word Ids: {}\".format([i for i in question_int if i != pad_q]))\n",
    "    print(\"      Text: {}\".format(int_to_text(question_int, questions_int_to_vocab)))\n",
    "    \n",
    "    pad_a = METATOKEN_INDEX\n",
    "    if answer_int is not None:\n",
    "        print(\"Actual Answer\")\n",
    "        print(\"  Word Ids: {}\".format([i for i in answer_int if i != pad_a]))\n",
    "        print(\"      Text: {}\".format(int_to_text(answer_int, answers_int_to_vocab)))\n",
    "\n",
    "    limit = 1 if best_only else beam_width\n",
    "    for i in range(limit):\n",
    "        beam = beams[:, i]\n",
    "        print(\"\\nBeam Answer\", i)\n",
    "        print('  Word Ids: {}'.format([i for i in beam if i != pad_a]))\n",
    "        print('      Text: {}'.format(int_to_text(beam, answers_int_to_vocab)))\n",
    "        \n",
    "def check_response(session, question_int, answer_int=None, best_only=True):\n",
    "    \"\"\"\n",
    "    session - the TensorFlow session\n",
    "    question_int - a list of integers\n",
    "    answer - the actual, correct response (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    two_d_question_int = [question_int]\n",
    "    q_lengths = [len(question_int)]\n",
    "    \n",
    "    [beam_output] = session.run([beams], feed_dict = {input_data: np.array(two_d_question_int, dtype=np.float32),\n",
    "                                                      source_lengths: q_lengths,\n",
    "                                                      keep_prob: 1})\n",
    "    \n",
    "    show_response(question_int, beam_output[0], answer_int, best_only=best_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics_batch(xent, perp, bleu, wer, num_batches=None):\n",
    "    \"\"\"\n",
    "    xent - cross-entropy error summed across batches (unless num_batches is None)\n",
    "    perp - perplexity          summed across batches (unless num_batches is None)\n",
    "    bleu - BLEU                summed across batches (unless num_batches is None)\n",
    "    wer -  Word-Error Rate     summed across batches (unless num_batches is None)\n",
    "    num_batches - the number of batches (used to average each of the metrics)\n",
    "    \"\"\"\n",
    "    if num_batches:\n",
    "        xent /= num_batches\n",
    "        perp /= num_batches\n",
    "        bleu /= num_batches\n",
    "        wer /= num_batches\n",
    "\n",
    "    print(\"Metrics averaged by sentence\")\n",
    "    print(\"\\t  Cross-Entropy: {:>9.6f}\".format(xent))\n",
    "    print(\"\\t     Perplexity: {}\".format(perp))\n",
    "    print(\"\\t           BLEU: {}\".format(bleu))\n",
    "    print(\"\\tWord-Error Rate: {}\".format(wer))\n",
    "    \n",
    "def calc_metrics_beams(beams, prompt_int, answer_int ):\n",
    "    print(\"Sample output\")\n",
    "    show_response(prompt_int, beams, answer_int, best_only=False)\n",
    "    targ_text = [int_to_text(answer_int, answers_int_to_vocab)]\n",
    "    pred_text = [int_to_text(beams[:, 0], answers_int_to_vocab)]\n",
    "    sing_bleu = metrics.bleu(targ_text, pred_text)\n",
    "    sing_wer = metrics.batch_word_error_rate(targ_text, pred_text)\n",
    "    print(\"Metrics for best beam\")\n",
    "    print(\"\\tBLEU: {}\".format(sing_bleu))\n",
    "    print(\"\\t WER: {}\".format(sing_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167443\n",
      "29548\n"
     ]
    }
   ],
   "source": [
    "# Validate the training with 10% of the data\n",
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "\n",
    "# Split the questions and answers into training and validating data\n",
    "(shuffled_questions, shuffled_answers) = parallel_shuffle(sorted_questions, sorted_answers)\n",
    "\n",
    "train_questions = shuffled_questions[train_valid_split:]\n",
    "train_answers = shuffled_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = shuffled_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "\n",
    "#VALIDATION\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_questions))//train_batch_size//2)-1 #Check validation loss every half-epoch\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "#Minimum number of epochs before we start checking sample output with beam search\n",
    "min_epochs_before_validation = 0\n",
    "\n",
    "checkpoint = \"./checkpoints/best_model.ckpt\" \n",
    "\n",
    "early_stopping_metric = \"perplexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model parameters\n",
      "Using perplexity for early stopping after 5 stalled steps\n",
      "Shuffling training data . . .\n",
      "Epoch   1/50 Batch    0/1308 - Loss:  0.005484, Seconds: 1372.15\n",
      "Shuffling validation data . . .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized model parameters\")\n",
    "    print(\"Using {} for early stopping after {} stalled steps\".format(early_stopping_metric, stop))\n",
    "    for epoch_i in range(1, epochs+1):        \n",
    "        print(\"Shuffling training data . . .\")\n",
    "        (train_questions, train_answers) = parallel_shuffle(train_questions, train_answers)\n",
    "                \n",
    "        for batch_i, (q_lengths, a_lengths, questions_batch, answers_batch) in enumerate(\n",
    "                batch_data(train_questions, train_answers, train_batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run([train_op, cost],\n",
    "                {input_data: questions_batch, targets: answers_batch,\n",
    "                 source_lengths: q_lengths, target_lengths: a_lengths,\n",
    "                 lr: learning_rate, keep_prob: keep_probability})\n",
    "            total_train_loss += loss\n",
    "            batch_time = time.time() - start_time\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>9.6f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_questions) // train_batch_size, \n",
    "                              total_train_loss / display_step, \n",
    "                              batch_time*display_step),\n",
    "                         flush=True)\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if batch_i % validation_check == 0 and epoch_i >= min_epochs_before_validation:\n",
    "                print(\"Shuffling validation data . . .\")\n",
    "                (valid_questions, valid_answers) = parallel_shuffle(valid_questions, valid_answers)\n",
    "                total_xent = 0\n",
    "                total_perp = 0\n",
    "                total_bleu = 0\n",
    "                total_wer = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                start_time = time.time()        \n",
    "                for batch_ii, (q_lengths, a_lengths, questions_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_questions, valid_answers, valid_batch_size)):\n",
    "                        \n",
    "                    [valid_xent, valid_perp, beam_output] = sess.run( [xent, perplexity, beams],\n",
    "                        {input_data: questions_batch, targets: answers_batch,\n",
    "                        source_lengths: q_lengths, target_lengths: a_lengths, keep_prob: 1})\n",
    "                    total_xent += valid_xent\n",
    "                    total_perp += valid_perp\n",
    "                    #Text-based metrics\n",
    "                    best_beams = beam_output[:, :, 0]\n",
    "                    beam_text = [int_to_text(best_beams[i], answers_int_to_vocab)\n",
    "                                     for i in range(len(best_beams))]\n",
    "                    target_text = [int_to_text(answers_batch[i], answers_int_to_vocab)\n",
    "                                       for i in range(len(answers_batch))]\n",
    "                    total_bleu += metrics.bleu(target_text, beam_text)\n",
    "                    total_wer  += metrics.batch_word_error_rate(target_text, beam_text)\n",
    "                    num_batches += 1\n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                print(\"Processed validation set in {:>4.2f} seconds\".format(batch_time))\n",
    "                show_metrics_batch(total_xent, total_perp, total_bleu, total_wer, num_batches)\n",
    "                calc_metrics_beams(beam_output[-1, :, :], questions_batch[-1], answers_batch[-1])\n",
    "\n",
    "                # Reduce learning rate, but not below its minimum value\n",
    "                learning_rate *= learning_rate_decay\n",
    "                if learning_rate < min_learning_rate:\n",
    "                    learning_rate = min_learning_rate\n",
    "\n",
    "                avg_valid_loss = total_xent / num_batches\n",
    "                print(summary_valid_loss)\n",
    "                if (len(summary_valid_loss) > 0) and (avg_valid_loss >= min(summary_valid_loss)):\n",
    "                    print(\"No improvement for {}.\".format(early_stopping_metric))\n",
    "                    stop_early += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"New record for {}!\".format(early_stopping_metric)) \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "                summary_valid_loss.append(avg_valid_loss)\n",
    "\n",
    "                if stop_early == stop:\n",
    "                        break\n",
    "                        \n",
    "\n",
    "\n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping training after {} stalled steps\".format(stop))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int, int_to_vocab):\n",
    "    '''Prepare the question for the model'''\n",
    "    cleaned_question = Corpus.clean_sequence(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int[UNK]) for word in cleaned_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/best_model.ckpt\n",
      "Prompt\n",
      "  Word Ids: [278]\n",
      "      Text: ['hello']\n",
      "Actual Answer\n",
      "  Word Ids: [5, 3004]\n",
      "      Text: ['it', 'elvis']\n",
      "\n",
      "Beam Answer 0\n",
      "  Word Ids: []\n",
      "      Text: []\n",
      "\n",
      "Beam Answer 1\n",
      "  Word Ids: [1]\n",
      "      Text: ['i']\n",
      "\n",
      "Beam Answer 2\n",
      "  Word Ids: [1, 1]\n",
      "      Text: ['i', 'i']\n",
      "\n",
      "Beam Answer 3\n",
      "  Word Ids: [1, 1, 1]\n",
      "      Text: ['i', 'i', 'i']\n",
      "\n",
      "Beam Answer 4\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 5\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 6\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 1, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'i', 'a', 'a']\n",
      "\n",
      "Beam Answer 7\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Beam Answer 8\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 6, 6, 6, 1]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'i', 'a', 'a', 'a', 'i']\n",
      "\n",
      "Beam Answer 9\n",
      "  Word Ids: [1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 1]\n",
      "      Text: ['i', 'i', 'i', 'i', 'i', 'i', 'i', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'i']\n"
     ]
    }
   ],
   "source": [
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(sorted_questions))\n",
    "question_int = sorted_questions[random]\n",
    "answer_int = sorted_answers[random]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Run the model with the input question\n",
    "    saver.restore(sess, checkpoint)\n",
    "    check_response(sess, question_int, answer_int, best_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
