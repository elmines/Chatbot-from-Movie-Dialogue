{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a chatbot using conversations from Cornell University's [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). The main features of our model are LSTM cells, a bidirectional dynamic RNN, and decoders with attention. \n",
    "\n",
    "The conversations will be cleaned rather extensively to help the model to produce better responses. As part of the cleaning process, punctuation will be removed, rare words will be replaced with \"UNK\" (our \"unknown\" token), longer sentences will not be used, and all letters will be in the lowercase. \n",
    "\n",
    "With a larger amount of data, it would be more practical to keep features, such as punctuation. However, I am using FloydHub's GPU services and I don't want to get carried away with too training for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Local libraries\n",
    "import metrics\n",
    "import loss_functions\n",
    "import embedding_models\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = os.path.join(\"corpora\")\n",
    "var_names = [\"train_prompts\", \"train_answers\", \"valid_prompts\", \"valid_answers\", \"vocab\"]\n",
    "file_names = [os.path.join(corpus_dir, var_name + \".txt\") for var_name in var_names]\n",
    "\n",
    "for (file_name, var_name) in zip(file_names, var_names):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as r:\n",
    "        text = [ [token for token in line.strip().split(\" \")] for line in r.readlines()]\n",
    "        exec(\"{} = {}\".format(var_name, text))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2int = {pair[0]:int(pair[1]) for pair in vocab}\n",
    "int2vocab = {index:word for (word, index) in vocab2int.items()}\n",
    "(questions_vocab_to_int, questions_int_to_vocab) = (vocab2int, int2vocab)\n",
    "(prompts_vocab_to_int, prompts_int_to_vocab) = (vocab2int, int2vocab) #Alternative names to ease the transition\n",
    "\n",
    "\n",
    "(answers_vocab_to_int, answers_int_to_vocab) = (vocab2int, int2vocab)\n",
    "\n",
    "UNK = vocab[0][0]\n",
    "METATOKEN_INDEX = len(vocab2int)\n",
    "META = \"<META>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "GO = \"<GO>\"\n",
    "codes = [META, EOS, PAD, GO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(sequence, int2vocab):\n",
    "    return [int2vocab[index] for index in sequence if index != METATOKEN_INDEX]\n",
    "\n",
    "def text_to_int(sequence, vocab2int):\n",
    "    return [vocab2int.get(token, vocab2int[UNK]) for token in sequence if token not in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in train_prompts]\n",
    "train_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in train_answers]\n",
    "valid_prompts_int = [text_to_int(prompt, questions_vocab_to_int) for prompt in valid_prompts]\n",
    "valid_answers_int = [text_to_int(answer, answers_vocab_to_int) for answer in valid_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', ',', 'shell', ',', 'that', 'is', 'ridiculous', ',', 'where', 'are', 'my', 'needle', '<UNK>', '<UNK>', '?']\n",
      "[73, 2, 4774, 2, 13, 8, 1061, 2, 80, 14, 32, 4855, 0, 0, 5]\n",
      "['sha', 'la', 'la', 'la', 'that', 'is', 'not', 'no', 'crime', '.']\n",
      "[7232, 1486, 1486, 1486, 13, 8, 9, 30, 1037, 1]\n",
      "['uh-huh', '.']\n",
      "[724, 1]\n",
      "['somebody', 'could', 'not', 'wait', 'until', 'dark', '.']\n",
      "[316, 79, 9, 206, 345, 714, 1]\n",
      "['that', 'is', 'it', '.']\n",
      "[13, 8, 11, 1]\n",
      "['i', 'could', 'almost', 'forgive', 'you', 'if', 'what', 'you', 'did', 'to', 'steve', 'came', 'from', 'jealousy', 'and', 'love', '.']\n",
      "[4, 79, 505, 893, 3, 53, 16, 3, 39, 7, 1876, 246, 87, 9438, 15, 143, 1]\n",
      "['certainly', 'not', '!']\n",
      "[642, 9, 18]\n",
      "['oh', ',', 'do', 'you', 'like', 'it', '?', 'i', 'am', 'not', 'partial', 'to', '<UNK>', ',', 'but', 'this', 'is', 'excellent', '.']\n",
      "[73, 2, 12, 3, 41, 11, 5, 4, 23, 9, 5997, 7, 0, 2, 37, 26, 8, 1540, 1]\n",
      "['not', 'so', '.', 'easily', '.', 'why', 'are', 'not', 'you', '.', '<UNK>', '?', 'go', 'on', '.', 'we', 'are', 'almost', 'there', '.']\n",
      "[9, 51, 1, 2245, 1, 67, 14, 9, 3, 1, 0, 5, 66, 34, 1, 19, 14, 505, 44, 1]\n",
      "['shut', 'the', 'hell', 'up', '.']\n",
      "[365, 6, 185, 59, 1]\n",
      "['listen', ',', 'if', 'a', 'boy', 'wants', 'to', 'do', 'homework', 'with', 'you', ',', 'it', 'really', 'means', 'he', 'did', 'not', 'have', 'nerve', 'to', 'ask', 'you', 'out', 'on', 'a', 'date', ',', 'so', 'you', 'shall', 'get', 'your', 'homework', 'together', 'and', 'pretend', 'to', 'be', 'studying', ',', 'and', 'the', 'next', 'thing', 'you', 'know', 'you', 'are', 'ordering', 'pizza', 'and', 'talking', 'about', 'your', 'favorite', 'movie', 'stars', '.']\n",
      "[225, 2, 53, 10, 254, 327, 7, 12, 2766, 36, 3, 2, 11, 108, 475, 24, 39, 9, 20, 1853, 7, 207, 3, 50, 34, 10, 662, 2, 51, 3, 55, 46, 29, 2766, 309, 15, 1678, 7, 33, 3523, 2, 15, 6, 283, 125, 3, 28, 3, 14, 6098, 2789, 15, 197, 43, 29, 1169, 587, 1650, 1]\n",
      "['no', 'no', 'no', ',', 'no', 'no', '.']\n",
      "[30, 30, 30, 2, 30, 30, 1]\n",
      "['her', 'life', 'is', 'in', 'your', 'hands', '.']\n",
      "[69, 159, 8, 21, 29, 521, 1]\n",
      "['h', \"'m\", '?']\n",
      "[4586, 2202, 5]\n",
      "['why', 'did', 'not', 'they', 'say', 'that', 'before', 'you', 'started', '?']\n",
      "[67, 39, 9, 38, 91, 13, 149, 3, 534, 5]\n",
      "['i', 'did', 'not', 'do', 'anything', '!']\n",
      "[4, 39, 9, 12, 137, 18]\n",
      "['divorce', '?', 'an', 'alcoholic', 'relative', '?', 'tell', 'me', ',', 'did', 'chip', 'torture', 'animals', 'when', 'he', 'was', 'young', '?']\n",
      "[1594, 5, 86, 5978, 4562, 5, 83, 22, 2, 39, 2714, 5269, 1582, 84, 24, 31, 427, 5]\n",
      "['who', 'are', 'you', 'talking', 'to', '?']\n",
      "[72, 14, 3, 197, 7, 5]\n",
      "['jeanne', ',', 'you', 'have', 'been', 'badly', 'wounded', '.']\n",
      "[2631, 2, 3, 20, 89, 2253, 3100, 1]\n",
      "['bible', 'truth', '.']\n",
      "[2302, 459, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_prompts[i])\n",
    "    print(train_prompts_int[i])\n",
    "for i in range(10):\n",
    "    print(train_answers[i])\n",
    "    print(train_answers_int[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417470"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus = train_prompts + train_answers + valid_prompts + valid_answers\n",
    "len(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['oh',\n",
       "  ',',\n",
       "  'shell',\n",
       "  ',',\n",
       "  'that',\n",
       "  'is',\n",
       "  'ridiculous',\n",
       "  ',',\n",
       "  'where',\n",
       "  'are',\n",
       "  'my',\n",
       "  'needle',\n",
       "  '<UNK>',\n",
       "  '<UNK>',\n",
       "  '?'],\n",
       " ['sha', 'la', 'la', 'la', 'that', 'is', 'not', 'no', 'crime', '.'],\n",
       " ['uh-huh', '.'],\n",
       " ['somebody', 'could', 'not', 'wait', 'until', 'dark', '.'],\n",
       " ['that', 'is', 'it', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 1024\n",
    "model = Word2Vec(sentences=combined_corpus, size=embedding_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['well'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.zeros((len(model.wv.vocab),1024))\n",
    "for i,word in enumerate(model.wv.index2word):\n",
    "        word_vecs[vocab2int[word]] = model[word]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary lengths\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n",
      "12001\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary lengths\")\n",
    "print(len(word_vecs))\n",
    "print(len(prompts_vocab_to_int))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(prompts_int_to_vocab))\n",
    "print(len(answers_int_to_vocab))\n",
    "print(len(word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_Vecs.npy',word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affective Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6848/12001 words assigned corresponsing VAD values.\n",
      "5153/12001 words assigned the neutral VAD vector.\n"
     ]
    }
   ],
   "source": [
    "#Choose from embedding_models.[appended_vad, counterfitted, retrofitted]\n",
    "embedding_model_path = embedding_models.appended_vad(model, vocab2int, regenerate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['listen', ',', 'if', 'a', 'boy', 'wants', 'to', 'do', 'homework', 'with', 'you', ',', 'it', 'really', 'means', 'he', 'did', 'not', 'have', 'nerve', 'to', 'ask', 'you', 'out', 'on', 'a', 'date', ',', 'so', 'you', 'shall', 'get', 'your', 'homework', 'together', 'and', 'pretend', 'to', 'be', 'studying', ',', 'and', 'the', 'next', 'thing', 'you', 'know', 'you', 'are', 'ordering', 'pizza', 'and', 'talking', 'about', 'your', 'favorite', 'movie', 'stars', '.', '<EOS>'], ['no', 'no', 'no', ',', 'no', 'no', '.', '<EOS>'], ['her', 'life', 'is', 'in', 'your', 'hands', '.', '<EOS>'], ['h', \"'m\", '?', '<EOS>'], ['why', 'did', 'not', 'they', 'say', 'that', 'before', 'you', 'started', '?', '<EOS>']]\n",
      "[[225, 2, 53, 10, 254, 327, 7, 12, 2766, 36, 3, 2, 11, 108, 475, 24, 39, 9, 20, 1853, 7, 207, 3, 50, 34, 10, 662, 2, 51, 3, 55, 46, 29, 2766, 309, 15, 1678, 7, 33, 3523, 2, 15, 6, 283, 125, 3, 28, 3, 14, 6098, 2789, 15, 197, 43, 29, 1169, 587, 1650, 1, 12001], [30, 30, 30, 2, 30, 30, 1, 12001], [69, 159, 8, 21, 29, 521, 1, 12001], [4586, 2202, 5, 12001], [67, 39, 9, 38, 91, 13, 149, 3, 534, 5, 12001]]\n"
     ]
    }
   ],
   "source": [
    "#Add EOS tokens to target data now that the embeddings have been trained\n",
    "def append_eos(answers_text, answers_int):\n",
    "    appended_text = [sequence + [EOS] for sequence in answers_text]\n",
    "    appended_ints = [sequence + [METATOKEN_INDEX] for sequence in answers_int]\n",
    "    return (appended_text, appended_ints)\n",
    "\n",
    "(train_answers, train_answers_int) = append_eos(train_answers, train_answers_int)\n",
    "(valid_answers, valid_answers_int) = append_eos(valid_answers, valid_answers_int)\n",
    "\n",
    "print(train_answers[:5])\n",
    "print(train_answers_int[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat( [tf.fill([batch_size, 1], METATOKEN_INDEX), ending], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_cell(rnn_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "def multi_dropout_cell(rnn_size, keep_prob, num_layers):    \n",
    "    return tf.contrib.rnn.MultiRNNCell( [dropout_cell(rnn_size, keep_prob) for _ in range(num_layers)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_lengths):\n",
    "    \"\"\"\n",
    "    Create the encoding layer\n",
    "    \n",
    "    Returns a tuple `(outputs, output_states)` where\n",
    "      outputs is a 2-tuple of vectors of dimensions [sequence_length, rnn_size] for the forward and backward passes\n",
    "      output_states is a 2-tupe of the final hidden states of the forward and backward passes\n",
    "    \n",
    "    \"\"\"\n",
    "    forward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    backward_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw = forward_cell,\n",
    "                                                   cell_bw = backward_cell,\n",
    "                                                   sequence_length = sequence_lengths,\n",
    "                                                   inputs = rnn_inputs,\n",
    "                                                    dtype=tf.float32)\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(enc_state, enc_outputs, dec_embed_input, dec_embeddings, #Inputs\n",
    "                        attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                        keep_prob,  #Hypeparameters\n",
    "                        source_lengths, target_lengths, batch_size): \n",
    "   \n",
    "    with tf.variable_scope(\"decoding\") as scope:\n",
    "        dec_cell = multi_dropout_cell(rnn_size, keep_prob, num_layers)\n",
    "        init_dec_state_size = batch_size\n",
    "        attn_mech = tf.contrib.seq2seq.BahdanauAttention(num_units=attn_size, memory=enc_outputs,\n",
    "                                                         memory_sequence_length=source_lengths)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech,\n",
    "                                                    attention_layer_size=dec_cell.output_size)\n",
    "        init_dec_state = attn_cell.zero_state(init_dec_state_size, tf.float32).clone(cell_state=enc_state)\n",
    "        \n",
    "        decoder_gen = lambda helper: tf.contrib.seq2seq.BasicDecoder(attn_cell, helper, init_dec_state,\n",
    "                                        output_layer = output_layer)\n",
    "        \n",
    "        #TRAINING\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_lengths)\n",
    "        train_decoder = decoder_gen(train_helper)\n",
    "        train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, scope=scope)\n",
    "        train_logits = train_outputs.rnn_output\n",
    "\n",
    "        #INFERENCE\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                                start_tokens = tf.tile([METATOKEN_INDEX],\n",
    "                                                                                       [batch_size]),\n",
    "                                                                 end_token = METATOKEN_INDEX)\n",
    "        infer_decoder = decoder_gen(infer_helper)\n",
    "        infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(infer_decoder, scope=scope, maximum_iterations=tf.round(tf.reduce_max(source_lengths) * 2))\n",
    "        infer_ids = infer_outputs.sample_id\n",
    "                \n",
    "    return train_logits, infer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(enc_embed_input, dec_embed_input, dec_embeddings, #Inputs\n",
    "                  source_lengths, target_lengths, batch_size, #Dimensions\n",
    "                  attn_size, rnn_size, num_layers, output_layer, #Architecture\n",
    "                  keep_prob): #Hyperparameters\n",
    "    \n",
    "    enc_outputs, enc_states = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, source_lengths)    \n",
    "    concatenated_enc_output = tf.concat(enc_outputs, -1)\n",
    "    init_dec_state = enc_states[0]    \n",
    "    \n",
    "    \n",
    "    train_logits, infer_ids = decoding_layer(init_dec_state,\n",
    "                            concatenated_enc_output,\n",
    "                            dec_embed_input,\n",
    "                            dec_embeddings,\n",
    "                            attn_size,\n",
    "                            rnn_size, \n",
    "                            num_layers,\n",
    "                            output_layer,\n",
    "                            keep_prob,\n",
    "                            source_lengths,\n",
    "                            target_lengths, \n",
    "                            batch_size\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    return train_logits, infer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_with_meta = 12002\n",
      "METATOKEN_INDEX = 12001\n",
      "wordVecsWithMeta.shape = (12002, 1027)\n",
      "wordVecsWithMeta[METATOKEN_INDEX] = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#FLAGS\n",
    "flag_affect_function = True\n",
    "flag_vad_values = True # Set to true if using VAD values appended onto existing embeddings\n",
    "affect_function = \"max_affective_content\" #Other valid values \"max_affective_dissonance\", \"min_affective_dissonance\"\n",
    "\n",
    "#Settings used by Asghar et al.\n",
    "rnn_size = 1024\n",
    "num_layers = 1\n",
    "attention_size = 256\n",
    "epochs_before_affective_loss = 40\n",
    "epochs = 50\n",
    "train_batch_size = 64\n",
    "lambda_param_max_affective_content = 0.5\n",
    "lambda_param_min_affective_dissonance=0.5\n",
    "lambda_param_max_affective_dissonance = 0.4\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.0001\n",
    "keep_probability = 0.75\n",
    "\n",
    "#Validation\n",
    "valid_batch_size = 64\n",
    "\n",
    "\n",
    "wordVecs = np.load(embedding_model_path).astype(np.float32)\n",
    "\n",
    "embedding_size = wordVecs.shape[1] #Dynamically determine embedding size from loaded embedding file\n",
    "\n",
    "metatoken_embedding = np.zeros((1, embedding_size), dtype=wordVecs.dtype)\n",
    "wordVecsWithMeta = np.concatenate( (wordVecs, metatoken_embedding), axis=0 )\n",
    "vocab_size_with_meta = wordVecsWithMeta.shape[0]\n",
    "\n",
    "print(\"vocab_size_with_meta =\", vocab_size_with_meta)\n",
    "print(\"METATOKEN_INDEX =\", METATOKEN_INDEX)\n",
    "print(\"wordVecsWithMeta.shape =\", wordVecsWithMeta.shape)\n",
    "print(\"wordVecsWithMeta[METATOKEN_INDEX] =\", wordVecsWithMeta[METATOKEN_INDEX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#                                      batch_size, sequence_length\n",
    "input_data = tf.placeholder(tf.int32, [None,       None], name='input')\n",
    "targets = tf.placeholder(tf.int32,    [None,       None], name='targets')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "#Determines whether we use a normal loss function or an affective loss function\n",
    "train_affect = tf.placeholder(tf.bool, shape=(), name=\"train_affect\")\n",
    "\n",
    "\n",
    "#                                          batch_size\n",
    "source_lengths = tf.placeholder(tf.int32, [None], name=\"source_lengths\")\n",
    "target_lengths = tf.placeholder(tf.int32, [None], name=\"target_lengths\")\n",
    "batch_size = tf.shape(input_data)[0]\n",
    "\n",
    "full_embeddings = tf.Variable(wordVecsWithMeta,trainable=False,name=\"Weight\")\n",
    "enc_embed_input = tf.nn.embedding_lookup(full_embeddings, input_data)\n",
    "dec_embed_input = tf.nn.embedding_lookup(full_embeddings, process_decoding_input(targets, batch_size))\n",
    "\n",
    "output_layer = tf.layers.Dense(vocab_size_with_meta,bias_initializer=tf.zeros_initializer(),activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, infer_ids = \\\n",
    "seq2seq_model(enc_embed_input, dec_embed_input, full_embeddings,\n",
    "        source_lengths, target_lengths, batch_size, \n",
    "        attention_size, rnn_size, num_layers, output_layer,\n",
    "        keep_prob)\n",
    "\n",
    "\n",
    "# Find the shape of the input data for sequence_loss\n",
    "with tf.name_scope(\"optimization\"): \n",
    "    \n",
    "    mask = tf.sequence_mask(target_lengths, dtype=tf.float32)\n",
    "    xent = loss_functions.cross_entropy(train_logits, targets, mask)\n",
    "    perplexity = tf.contrib.seq2seq.sequence_loss(train_logits, targets, mask, metrics.perplexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    vad_values = full_embeddings[:, -3:]\n",
    "    input_vad_values =enc_embed_input[:,:,1024:1027]\n",
    "\n",
    "    if flag_affect_function:\n",
    "        if affect_function == \"max_affective_content\":\n",
    "            assert flag_vad_values\n",
    "            emot_embeddings = full_embeddings[:, -3:]\n",
    "            neutral_vector = tf.constant([5.0, 1.0, 5.0], dtype=tf.float32)\n",
    "            affective_loss = loss_functions.max_affective_content(lambda_param_max_affective_content,\n",
    "                                                                train_logits, targets,\n",
    "                                                                emot_embeddings,neutral_vector, mask)\n",
    "        else:\n",
    "            if flag_vad_values:\n",
    "                emot_embeddings = full_embeddings[:, -3]\n",
    "                enc_emot_embed_input = enc_embed_input[:, -3]\n",
    "            else:\n",
    "                emot_embeddings = full_embeddings\n",
    "                enc_emot_embed_input = enc_embed_input\n",
    "            \n",
    "            if affect_function == \"max_affective_dissonance\":\n",
    "                affective_loss = loss_functions.max_affective_dissonance(lambda_param_max_affective_dissonance,\n",
    "                                                                train_logits, targets, \n",
    "                                                                 enc_emot_embed_input,emot_embeddings, mask)\n",
    "            else:\n",
    "                affective_loss = loss_functions.min_affective_dissonance(lambda_param_min_affective_dissonance,\n",
    "                                                                train_logits, targets, \n",
    "                                                                 enc_emot_embed_input,emot_embeddings, mask)\n",
    "    else:\n",
    "        affective_loss = xent #Just so we have a term for below \n",
    "    \n",
    "    train_cost = tf.cond(train_affect, true_fn=lambda: affective_loss, false_fn=lambda: xent)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(train_cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response(prompt_int, prediction, answer_int = None):\n",
    "    pad_q = METATOKEN_INDEX\n",
    "    print(\"Prompt\")\n",
    "    print(\"  Word Ids: {}\".format([i for i in prompt_int if i != pad_q]))\n",
    "    print(\"      Text: {}\".format(int_to_text(prompt_int, prompts_int_to_vocab)))\n",
    "    \n",
    "    pad_a = METATOKEN_INDEX\n",
    "    if answer_int is not None:\n",
    "        print(\"Target Answer\")\n",
    "        print(\"  Word Ids: {}\".format([i for i in answer_int if i != pad_a]))\n",
    "        print(\"      Text: {}\".format(int_to_text(answer_int, answers_int_to_vocab)))\n",
    "\n",
    "    print(\"\\nPrediction\")\n",
    "    print('  Word Ids: {}'.format([i for i in prediction if i != pad_a]))\n",
    "    print('      Text: {}'.format(int_to_text(prediction, answers_int_to_vocab)))\n",
    "        \n",
    "def check_response(session, prompt_int, answer_int=None):\n",
    "    \"\"\"\n",
    "    session - the TensorFlow session\n",
    "    question_int - a list of integers\n",
    "    answer - the actual, correct response (if available)\n",
    "    \"\"\"\n",
    "    \n",
    "    two_d_prompt_int = [prompt_int]\n",
    "    p_lengths = [len(prompt_int)]\n",
    "    \n",
    "    [infer_ids_output] = session.run([infer_ids], feed_dict = {input_data: np.array(two_d_prompt_int, dtype=np.float32),\n",
    "                                                      source_lengths: p_lengths,\n",
    "                                                      keep_prob: 1})\n",
    "    \n",
    "    show_response(prompt_int, infer_ids_output[0], answer_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    pad_int = METATOKEN_INDEX\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_length - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        \n",
    "        source_lengths = np.array( [len(sentence) for sentence in questions_batch] )\n",
    "        target_lengths = np.array( [len(sentence) for sentence in answers_batch])\n",
    "        \n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield source_lengths, target_lengths, pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shuffle(source_sequences, target_sequences):\n",
    "    if len(source_sequences) != len(target_sequences):\n",
    "        raise ValueError(\"Cannot shuffle parallel sets with different numbers of sequences\")\n",
    "    indices = np.random.permutation(len(source_sequences))\n",
    "    shuffled_source = [source_sequences[indices[i]] for i in range(len(indices))]\n",
    "    shuffled_target = [target_sequences[indices[i]] for i in range(len(indices))]\n",
    "    \n",
    "    return (shuffled_source, shuffled_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "\n",
    "#VALIDATION\n",
    "validation_check = ((len(train_prompts))//train_batch_size//2)-1 #Check validation loss every half-epoch\n",
    "#Minimum number of epochs before we start checking sample output\n",
    "min_epochs_before_validation = 2\n",
    "\n",
    "#Used to make uniquely directories, not to identify when a model is saved\n",
    "time_string = time.strftime(\"%b%d_%H:%M:%S\")\n",
    "\n",
    "checkpoint_dir = os.path.join(\"checkpoints\", time_string)\n",
    "if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n",
    "checkpoint_best = str(checkpoint_dir) + \"/\" + \"best_model.ckpt\" \n",
    "checkpoint_latest = str(checkpoint_dir) + \"/\" + \"latest_model.ckpt\"\n",
    "\n",
    "log_dir = os.path.join(\"logging\", time_string)\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "train_log = os.path.join(log_dir, \"train.csv\")\n",
    "valid_log = os.path.join(log_dir, \"valid.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_entries(csv_path, *fields, header = False):\n",
    "    if len(fields[0]) < 1: return\n",
    "    mode = \"w\" if header else \"a\"\n",
    "    with open(csv_path, mode, encoding=\"utf-8\") as log:\n",
    "        lines = []\n",
    "        num_lines = len(fields[0])\n",
    "        lines = \"\\n\".join(\",\".join([str(field[i]) for field in fields]) \n",
    "                          for i in range(num_lines)\n",
    "        )\n",
    "        log.write(lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_fields(log_fields):\n",
    "    for field in log_fields:\n",
    "        field.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty training log logging/Jun26_13:26:03/train.csv, validation log logging/Jun26_13:26:03/valid.csv\n",
      "Initialized model parameters, wrote initial model to checkpoints/Jun26_13:26:03/latest_model.ckpt\n",
      "Beginning training with cross-entropy loss.\n",
      "Shuffling training data . . .\n",
      "Epoch   1/50 Batch    0/2556 - Loss-per-Token:  9.396128, Seconds: 1.44\n"
     ]
    }
   ],
   "source": [
    "train_epoch_nos = []\n",
    "train_batch_tokens = [] #Number of tokens in a batch\n",
    "train_batch_losses = [] #Per-token loss for a batch\n",
    "train_log_fields = [train_epoch_nos, train_batch_tokens, train_batch_losses]\n",
    "\n",
    "valid_epoch_nos = []\n",
    "valid_check_nos = []\n",
    "valid_batch_tokens = []\n",
    "valid_batch_losses = []\n",
    "valid_log_fields = [valid_epoch_nos, valid_check_nos, valid_batch_tokens, valid_batch_losses]\n",
    "\n",
    "train_start_time = None #For logging time for sets of batches\n",
    "log_entries(train_log, [\"epoch\"], [\"num_tokens\"], [\"loss_per_token\"], header=True)\n",
    "log_entries(valid_log, [\"epoch\"], [\"check\"], [\"num_tokens\"], [\"perplexity_per_token\"], header=True)\n",
    "print(\"Initialized empty training log {}, validation log {}\".format(train_log, valid_log))\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "use_affect_func = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().save(sess, checkpoint_latest)\n",
    "    print(\"Initialized model parameters, wrote initial model to {}\".format(checkpoint_latest))\n",
    "    if not use_affect_func: print(\"Beginning training with cross-entropy loss.\")\n",
    "    else:                   print(\"Beginning training with {}\".format(affect_function))\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        if not use_affect_func and epoch_i > epochs_before_affective_loss:\n",
    "            print(\"Switching from cross-entropy loss to {}\".format(affect_function))\n",
    "            use_affect_func = True        \n",
    "        \n",
    "        print(\"Shuffling training data . . .\")\n",
    "        (train_prompts_int, train_answers_int) = parallel_shuffle(train_prompts_int, train_answers_int)\n",
    "        \n",
    "        valid_check_no = 1\n",
    "        \n",
    "        \n",
    "        for batch_i, (p_lengths, a_lengths, prompts_batch, answers_batch) in enumerate(\n",
    "                batch_data(train_prompts_int, train_answers_int, train_batch_size)):\n",
    "            if train_start_time is None: train_start_time = time.time()\n",
    "            \n",
    "            #VALIDATION CHECK\n",
    "            if batch_i % validation_check == 0 and epoch_i > min_epochs_before_validation:\n",
    "                print(\"Shuffling validation data . . .\")\n",
    "                (valid_prompts_int, valid_answers_int) = parallel_shuffle(valid_prompts_int, valid_answers_int)\n",
    "                \n",
    "                clear_fields(valid_log_fields)\n",
    "\n",
    "                \n",
    "                valid_start_time = time.time()\n",
    "                for batch_ii, (p_lengths, a_lengths, prompts_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_prompts_int, valid_answers_int, valid_batch_size)):\n",
    "\n",
    "                    [valid_loss] = sess.run([perplexity],\n",
    "                        {input_data: prompts_batch, targets: answers_batch,\n",
    "                        source_lengths: p_lengths, target_lengths: a_lengths, keep_prob: 1})\n",
    "                    valid_epoch_nos.append(epoch_i)\n",
    "                    valid_check_nos.append(valid_check_no)\n",
    "                    valid_batch_tokens.append(sum(a_lengths))\n",
    "                    valid_batch_losses.append(valid_loss)\n",
    "\n",
    "                \n",
    "                valid_check_no += 1\n",
    "                duration = time.time() - valid_start_time\n",
    "                avg_valid_loss = sum(loss*tokens \n",
    "                        for (loss, tokens) in zip(valid_batch_losses, valid_batch_tokens)) / sum(valid_batch_tokens)\n",
    "                \n",
    "                log_entries(valid_log, *(valid_log_fields))\n",
    "                clear_fields(valid_log_fields)\n",
    "                print(\"Processed validation set in {:>4.2f} seconds\".format(duration))\n",
    "                print(\"Average perplexity per token = {}\".format(avg_valid_loss))\n",
    "                if avg_valid_loss >= best_valid_loss:\n",
    "                    print(\"No improvement for validation loss.\")\n",
    "                else:\n",
    "                    best_valid_loss = avg_valid_loss\n",
    "                    print(\"New record for validation loss!\")\n",
    "                    print(\"Saving best model to {}\".format(checkpoint_best))\n",
    "                    tf.train.Saver().save(sess, checkpoint_best)\n",
    "                check_response(sess, prompts_batch[-1], answers_batch[-1])\n",
    "                \n",
    "                train_start_time = time.time()\n",
    "            \n",
    "            #TRAINING\n",
    "            _, loss = sess.run([train_op, train_cost],\n",
    "                {input_data: prompts_batch, targets: answers_batch,\n",
    "                 source_lengths: p_lengths, target_lengths: a_lengths,\n",
    "                 keep_prob: keep_probability,\n",
    "                 train_affect: use_affect_func})\n",
    "            train_epoch_nos.append(epoch_i)\n",
    "            train_batch_losses.append(loss)\n",
    "            train_batch_tokens.append(sum(a_lengths))\n",
    "            \n",
    "            if batch_i % display_step == 0:\n",
    "                duration = time.time() - train_start_time\n",
    "                avg_train_loss = sum(loss*tokens \n",
    "                        for (loss, tokens) in zip(train_batch_losses, train_batch_tokens)) / sum(train_batch_tokens)\n",
    "                    \n",
    "                log_entries(train_log, *(train_log_fields))\n",
    "                clear_fields(train_log_fields)\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss-per-Token: {:>9.6f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i, epochs, batch_i, len(train_prompts_int) // train_batch_size, \n",
    "                              avg_train_loss, duration),\n",
    "                         flush=True)\n",
    "                train_start_time = time.time()\n",
    "\n",
    "        print(\"{} epochs completed, saving model to {}\".format(epoch_i, checkpoint_latest))\n",
    "        tf.train.Saver().save(sess, checkpoint_latest)\n",
    "        log_entries(train_log, *(train_log_fields))\n",
    "        clear_fields(train_log_fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int, int_to_vocab):\n",
    "    '''Prepare the question for the model'''\n",
    "    cleaned_question = Corpus.clean_sequence(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int[UNK]) for word in cleaned_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(train_prompts_int))\n",
    "prompt_int = train_prompts_int[random]\n",
    "answer_int = train_answers_int[random]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Run the model with the input question\n",
    "    saver.restore(sess, checkpoint)\n",
    "    check_response(sess, prompt_int, answer_int, best_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
